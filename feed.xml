<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joohunhyun.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joohunhyun.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-17T03:48:09+00:00</updated><id>https://joohunhyun.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">(ENG) MLP, DNN</title><link href="https://joohunhyun.github.io/blog/2024/skip-safety-skip/" rel="alternate" type="text/html" title="(ENG) MLP, DNN"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/skip-safety-skip</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/skip-safety-skip/"><![CDATA[<ul> <li>Perceptron -&gt; delta rule</li> <li>MLP (2~3 layers) -&gt; backpropagation</li> <li>DNN (~10 layers) -&gt; using ReLU instead of sigmoid</li> </ul> <h4 id="mlps-problem">MLPâ€™s problem</h4> <ul> <li>Major issue of designing MLP is how</li> <li>As the number of hidden layers increase, the sigmoid functionâ€™s relatively small gradient repeatedly gets multiplied. -&gt; this makes the gradient close to 0, and the weights are not updated (<strong>vanishing gradient problem</strong>)</li> </ul> <p><strong>Solution to this problem</strong></p> <ol> <li>Using a ReLU Function</li> </ol> <p>The Rectified Linear Unit (ReLU) function is defined as:</p> \[f(x)=max(0,x)\] <p>ReLU has a gradient of 1 1 for positive inputs, which prevents the gradient from shrinking excessively as it is propagated through the network.</p> <p>Advantages of ReLU:</p> <ul> <li>Avoids vanishing gradients: The gradient remains constant for positive inputs, ensuring that weights continue to be updated.</li> <li>Computational efficiency: ReLU is simpler to compute than sigmoid or tanh.</li> <li>Sparsity: It introduces sparsity in activations (many outputs are zero), which can improve generalization.</li> </ul> <p>However, ReLU can suffer from the dying ReLU problem, where neurons become inactive (outputting zero) due to large negative gradients. Variants like Leaky ReLU and Parametric ReLU address this issue.</p> <ol> <li>Xavier Initialization</li> </ol> <p>Proper weight initialization is crucial for mitigating vanishing or exploding gradients. The Xavier initialization (or Glorot initialization) ensures that the variance of activations and gradients is maintained across layers.</p> <p>Benefits:</p> <ul> <li>Prevents vanishing/exploding gradients by keeping the variance of inputs and outputs consistent across layers.</li> <li>Helps the network converge faster.</li> </ul> <ol> <li>Batch Normalization</li> </ol> <p>Batch Normalization (BatchNorm) normalizes the inputs to each layer by adjusting the mean and variance of the activations during training.</p> <p>Benefits</p> <ul> <li>Improves gradient flow: Normalization reduces internal covariate shift, allowing deeper networks to train effectively.</li> <li>Stabilizes learning: It reduces sensitivity to initialization and learning rate.</li> <li>Acts as regularization: BatchNorm has a slight regularization effect, reducing the need for dropout in some cases.</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Perceptron -&gt; delta rule MLP (2~3 layers) -&gt; backpropagation DNN (~10 layers) -&gt; using ReLU instead of sigmoid]]></summary></entry><entry><title type="html">(ENG) SVM, SVC</title><link href="https://joohunhyun.github.io/blog/2024/machine-learning-copy/" rel="alternate" type="text/html" title="(ENG) SVM, SVC"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/machine-learning%20copy</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/machine-learning-copy/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="classification-models">Classification Models</h4> <p>Definition : ë‘ê°œì˜ class A and Bë¥¼ feature spaceì—ì„œ ë¶„ë¦¬í•˜ëŠ” ëª¨ë¸</p> <ul> <li>Decision Trees</li> <li>Probabilistic models</li> <li>Linear models</li> <li>Non-linear models</li> </ul> <p>In the case of non-linear data, there are different methods to model this.</p> <ol> <li>MLP : achieve non-linearlity by combining perceptrons(linear models)</li> <li>SVM : supervised machine learning algorithm used for classification and regression tasks.</li> <li>SVC :</li> </ol> <h5 id="svm">SVM</h5> <p>Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. The key idea is to find the best hyperplane that separates data points into different classes. In an ideal case, this hyperplane maximizes the margin between data points of different classes.</p> <ul> <li>Core Concepts: <ul> <li>The hyperplane is the decision boundary.</li> <li>SVM focuses on data points (support vectors) near the decision boundary that influence its position.</li> <li>The algorithm uses kernels to transform data into higher-dimensional spaces when it is not linearly separable in the original space.</li> </ul> </li> <li>Applications: Image classification, text categorization, and other binary/multi-class problems.</li> <li>Pros: <ul> <li>Effective for high-dimensional spaces.</li> <li>Works well with a clear margin of separation.</li> </ul> </li> <li>Cons: <ul> <li>Computationally expensive for large datasets.</li> <li>Sensitive to the choice of kernel and hyperparameters.</li> </ul> </li> </ul> <h6 id="rbf-kernel">RBF Kernel</h6> <p>The RBF kernel, also known as the Gaussian kernel, is one of the most commonly used kernels in SVM. It is a non-linear kernel that maps data points into a higher-dimensional space where a linear hyperplane can effectively separate them.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: It can model complex relationships in the data.</li> <li>Local influence: Points that are closer to each other have a higher similarity, and their influence decreases exponentially with distance</li> <li> <p>Scalability: Effective for problems where the decision boundary is not linear.</p> </li> <li>Pros: <ul> <li>Handles non-linearly separable data well.</li> <li>Requires fewer hyperparameters compared to other kernels.</li> </ul> </li> <li>Cons: <ul> <li>Î³ must be carefully tuned; poor tuning can lead to overfitting or underfitting.</li> </ul> </li> </ul> <h6 id="polynomial-kernel">Polynomial Kernel</h6> <p>The Polynomial kernel is another non-linear kernel used in SVM. It computes the similarity between two data points as a polynomial function of their inner product in the input space.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: Maps the input space to a higher-dimensional space defined by polynomial terms.</li> <li>Flexibility: The degree \(d\) controls the complexity of the decision boundary.</li> <li>Global influence: Unlike the RBF kernel, it considers global features of the data.</li> <li>Pros: <ul> <li>Can model non-linear relationships with adjustable complexity via \(d\)</li> <li>Effective for datasets where classes are distinguishable by polynomial decision boundaries.</li> </ul> </li> <li>Cons: <ul> <li>More sensitive to feature scaling than the RBF kernel.</li> <li>Higher degrees can lead to overfitting.</li> </ul> </li> </ul> <h5 id="svc">SVC</h5> <ul> <li> <p>SVC stands for Support Vector Classifier and is the implementation of SVM for classification tasks in libraries like scikit-learn.</p> </li> <li>Key Differences Between SVM and SVC: SVM is the broader concept that encompasses both classification and regression tasks (e.g., SVM for regression is called SVR).</li> <li>SVC is specifically the classification implementation of SVM in scikit-learn.</li> <li>Features of SVC: <ul> <li>It supports linear and non-linear kernels like polynomial, RBF (Radial Basis Function), and sigmoid.</li> <li>Provides hyperparameters like C (regularization strength) and gamma (kernel coefficient) for tuning the model.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(ENG) Feature Selection, Extraction, and Ensamble Methods</title><link href="https://joohunhyun.github.io/blog/2024/machine-learning/" rel="alternate" type="text/html" title="(ENG) Feature Selection, Extraction, and Ensamble Methods"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/machine-learning</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/machine-learning/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="feature-subset-selection">Feature Subset Selection</h4> <p>Definition : Feature subset selection finds a subset of original features without new features</p> <h5 id="1-filter-method">1. Filter Method</h5> <ul> <li>Evaluates features with performance measures based on information gain or distance between each attribute in the class</li> <li>ì¥ì  : computationally siple and fast</li> <li>ë‹¨ì  : slow searching</li> <li>í•´ê²°ì±… : use a single-attribute evaluator with ranking</li> </ul> <h5 id="2-wrapper-method">2. Wrapper Method</h5> <ul> <li>Searches for best combination of features among <strong>all</strong> posttible combination</li> <li>ì¥ì  : generally better performance, simple and direct</li> <li>ë‹¨ì  : \(O(n^2)\) time complexity - slow</li> <li>í•´ê²°ì±… <ul> <li>eliminate irrelevant features</li> <li>eliminate redundant attributes</li> </ul> </li> </ul> <h5 id="3-embedded-method">3. Embedded method</h5> <ul> <li>Uses ML models for classification and then an obtimal subset of features/ranking of feature is built by the classifier algorithm</li> </ul> <h5 id="search-methods">Search Methods</h5> <ul> <li>Weka ì˜ˆì œì—ì„œëŠ” classfier(wrapper, etc)ì™€ search method ë‘˜ ë‹¤ ì •ì˜í•´ì•¼í•œë‹¤.</li> </ul> <h5 id="different-search-methods">Different Search Methods</h5> <ol> <li>Exhaustive : \(2^n\) subsets</li> <li>Backwards</li> <li>Forwards</li> <li>Bidirectional</li> </ol> <h2><br/></h2> <h4 id="feature-extraction-aka-reducing-dimentionality">Feature Extraction (AKA: Reducing Dimentionality)</h4> <p>Definition : extracts new set of features from the original features</p> <h5 id="pca">PCA</h5> <ul> <li>Definition : unsupervised approach to examine relations among a set of variables</li> <li>When to use this? <ul> <li>PCA is useful in reducing the dimentionality of 3-D, 4-D scatterplots (ì™œ ì‚¬ìš©í•˜ëƒ : visually difficult to interpret data points in high-dimensional space)</li> </ul> </li> <li>Ex : Compressing MNIST- dataset using PCA (to reduce dimension)</li> </ul> <h2 id="-1"><br/></h2> <h4 id="ensamble-method">Ensamble Method</h4> <ul> <li>Definition : ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ë“¤ì„ ì¡°í•©í•´ì„œ ë”ìš± ì •í™•í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ë°©ë²• (ìœ„ì›íšŒ ìš´ì˜ê³¼ ë¹„ìŠ·)</li> </ul> <p>Types of Ensamble Methods:</p> <ol> <li>Boosting</li> <li>Bagging</li> <li>Stacking</li> </ol> <h5 id="1-boosting-ada-boosting">1. Boosting :Ada-Boosting</h5> <h5 id="2-baggingstands-for-boostrap-aggrevating">2. Bagging(stands for: Boostrap Aggrevating)</h5> <p>Baggingì€ ì•™ìƒë¸” ê¸°ë²•ë“¤ ê°€ìš´ë° ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì´ë‹¤(taking a vote, or weighted vote). Decision Treeì˜ ê²½ìš°, training setì— ë”°ë¼ì„œ ê·¸ êµ¬ì¡°ê°€ ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°”ë€ë‹¤. ì´ëŸ° ê²½ìš°ì—ëŠ” 1) training setì„ ì—¬ëŸ¬ ê°œ ë§Œë“¤ê³  2) ê°ê° ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ 3) votingì„ í•œë‹¤. In numerical prediction, taking the average of all predictions.</p> <p>Breiman(1996) noticed that an ensamble of trees improved when the trees differed significantly from each other, today called <strong>Random Forest</strong>. Bagging was able to create a diverse ensamble of classifiers by introducing randomness into the learning algâ€™s input, which resulted in <strong>better results</strong>. In a DT, picking the best option can be randomized by picking one of the N best options at random instead of a signle winner (ex: top 3 options, top 5 options)</p> <p><strong>Methods of picking different â€œwinnersâ€ in bagging</strong></p> <ol> <li>elite : picking the best one</li> <li>randomization : picking 1 randomly from top 5 candidates</li> <li>stochastic : top5 ì¤‘ì—ì„œ ë£°ë ›ìœ¼ë¡œ ê³ ë¥´ëŠ” ë°©ë²• (weighted)</li> </ol> <h5 id="3-stacking">3. Stacking</h5> <p>Stacking is used on models built by different learning algorithsm. For example, stacking can be used when you want to form a classifier for a given dataset with A) deicision tree inducer B) naive bayes learnner and C) instance based learning scheme.</p> <p>Stacking involves a <strong>metalearner</strong> that replaces the voting procedure of boosing and bagging. Metalearner is used to discover how to best combine the output of the base learners to determine which classifiers are the reliable ones.</p> <ul> <li>base learners : level-0 models</li> <li>meta learner : level-1 model</li> <li>predictions from base models are inputs to the meta learner. (funnelì²˜ëŸ¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‚´ë ¤ì˜¨ë‹¤)</li> </ul> <p>TL;DR : Stacking combines predictions of base learners using metalearner(NOT voting)</p>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(KOR) ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë°°ì†í•˜ëŠ” ë°©ë²•</title><link href="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/" rel="alternate" type="text/html" title="(KOR) ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë°°ì†í•˜ëŠ” ë°©ë²•"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/skip-safety-skip%20copy</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/"><![CDATA[<p>202412ì›” ê¸°ì¤€, í•´ë‹¹ ë°©ë²•ì´ ê°€ì¥ í¸í•˜ë‹¤. ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ëª¨ë‘ ë§‰í˜”ë‹¤. ì¤‘ê°„ì— â€œê¹œì§ í€´ì¦ˆâ€ë„ ìˆìœ¼ë‹ˆ ìœ ì˜í•´ì•¼í•œë‹¤.</p> <p><strong>í¬ë¡¬</strong>ì—ì„œ ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë™ì˜ìƒ ì¬ìƒ í›„ ê°œë°œìëª¨ë“œ ì½˜ì†” <code class="language-plaintext highlighter-rouge">F12</code>ì— í•´ë‹¹ ì½”ë“œ ë¶™ì—¬ë„£ê¸°:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>document.querySelector('video').playbackRate = 15;
</code></pre></div></div>]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[202412ì›” ê¸°ì¤€, í•´ë‹¹ ë°©ë²•ì´ ê°€ì¥ í¸í•˜ë‹¤. ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ëª¨ë‘ ë§‰í˜”ë‹¤. ì¤‘ê°„ì— â€œê¹œì§ í€´ì¦ˆâ€ë„ ìˆìœ¼ë‹ˆ ìœ ì˜í•´ì•¼í•œë‹¤.]]></summary></entry><entry><title type="html">(ENG) VsCode Shortcut Keys(MacOS)</title><link href="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/" rel="alternate" type="text/html" title="(ENG) VsCode Shortcut Keys(MacOS)"/><published>2024-12-13T00:00:00+00:00</published><updated>2024-12-13T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/"><![CDATA[<p>ğŸ’¡ìµìˆ™í•´ì§„ ë‹¨ì¶•í‚¤ëŠ” ë¬¸ì„œì—ì„œ ì‚­ì œ</p> <p>ğŸ’¡ì›ë³¸ì€ .md íŒŒì¼ì— ì¡´ì•ˆ</p> <hr/> <p><strong>Table of contents</strong></p> <ul> <li><a href="#vscode-shortcuts">VScode Shortcuts</a></li> <li><a href="#vscode-git-status">VSCode git status</a></li> </ul> <p><br/></p> <h4 id="vscode-shortcuts">VScode Shortcuts</h4> <table> <thead> <tr> <th>Action</th> <th>Shortcut</th> </tr> </thead> <tbody> <tr> <td><strong>General</strong></td> <td>Â </td> </tr> <tr> <td>Open Command Palette</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + P</code></td> </tr> <tr> <td>Open Settings</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + ,</code></td> </tr> <tr> <td>Open Keyboard Shortcuts</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + K âŒ˜ + S</code></td> </tr> <tr> <td>Open Extensions View</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + X</code></td> </tr> <tr> <td>Show Integrated Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + \</code></td> </tr> <tr> <td>Close Window</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + W</code></td> </tr> <tr> <td>Quit VSCode</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + Q</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>GitHub Copilot</strong></td> <td>Â </td> </tr> <tr> <td>Accept Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Tab</code></td> </tr> <tr> <td>Next Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + ]</code></td> </tr> <tr> <td>Previous Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + [</code></td> </tr> <tr> <td>Show Suggestions</td> <td><code class="language-plaintext highlighter-rouge">Ctrl + Enter</code></td> </tr> <tr> <td>Approve Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Cmd + â†</code> or <code class="language-plaintext highlighter-rouge">Cmd + â†’</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>File Management</strong></td> <td>Â </td> </tr> <tr> <td>Open File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + O</code></td> </tr> <tr> <td>Save File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + S</code></td> </tr> <tr> <td>Save All Files</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + âŒ˜ + S</code></td> </tr> <tr> <td>Close Editor</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + W</code></td> </tr> <tr> <td>Reopen Closed Editor</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + T</code></td> </tr> <tr> <td>New File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + N</code></td> </tr> <tr> <td>Open Recent Files</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + R</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Editing</strong></td> <td>Â </td> </tr> <tr> <td>Cut Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + X</code></td> </tr> <tr> <td>Copy Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + C</code></td> </tr> <tr> <td>Paste</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + V</code></td> </tr> <tr> <td>Delete Line</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + K</code></td> </tr> <tr> <td>Duplicate Line</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†“</code> or <code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†‘</code></td> </tr> <tr> <td>Move Line Up/Down</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + â†‘</code> or <code class="language-plaintext highlighter-rouge">âŒ¥ + â†“</code></td> </tr> <tr> <td>Indent Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + ]</code></td> </tr> <tr> <td>Outdent Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + [</code></td> </tr> <tr> <td>Comment Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + /</code></td> </tr> <tr> <td>Add Multi-Cursor</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + Click</code></td> </tr> <tr> <td>Select All Occurrences</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + â‡§ + L</code></td> </tr> <tr> <td>Expand Selection</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†’</code></td> </tr> <tr> <td>Shrink Selection</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†</code></td> </tr> <tr> <td>Format Document</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + F</code></td> </tr> <tr> <td>Go to Matching Bracket</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + \</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Navigation</strong></td> <td>Â </td> </tr> <tr> <td>Go to File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + P</code></td> </tr> <tr> <td>Go to Line</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + G</code></td> </tr> <tr> <td>Go to Definition</td> <td><code class="language-plaintext highlighter-rouge">F12</code></td> </tr> <tr> <td>Go to Implementation</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + F12</code></td> </tr> <tr> <td>Show References</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F12</code></td> </tr> <tr> <td>Navigate Back</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + -</code></td> </tr> <tr> <td>Navigate Forward</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + â‡§ + -</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Search and Replace</strong></td> <td>Â </td> </tr> <tr> <td>Find</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + F</code></td> </tr> <tr> <td>Find in Files</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + F</code></td> </tr> <tr> <td>Replace</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + âŒ¥ + F</code></td> </tr> <tr> <td>Replace in Files</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + H</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Debugging</strong></td> <td>Â </td> </tr> <tr> <td>Start/Continue Debugging</td> <td><code class="language-plaintext highlighter-rouge">F5</code></td> </tr> <tr> <td>Step Over</td> <td><code class="language-plaintext highlighter-rouge">F10</code></td> </tr> <tr> <td>Step Into</td> <td><code class="language-plaintext highlighter-rouge">F11</code></td> </tr> <tr> <td>Step Out</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F11</code></td> </tr> <tr> <td>Restart Debugging</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + F5</code></td> </tr> <tr> <td>Stop Debugging</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F5</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Terminal</strong></td> <td>Â </td> </tr> <tr> <td>Create New Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + â‡§ + </code>`</td> </tr> <tr> <td>Split Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + \</code>`</td> </tr> <tr> <td>Kill Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + K</code> (inside terminal)</td> </tr> <tr> <td>Navigate Terminals</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + â†</code> or <code class="language-plaintext highlighter-rouge">âŒƒ + â†’</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Version Control</strong></td> <td>Â </td> </tr> <tr> <td>Open Source Control</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + G</code></td> </tr> <tr> <td>Commit Changes</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + Enter</code> (in source control)</td> </tr> </tbody> </table> <p><br/></p> <h4 id="vscode-git-status">VSCode git status</h4> <table> <thead> <tr> <th>Code</th> <th>Status</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>Added</td> <td>This is a new file that has been added to the repository</td> </tr> <tr> <td>M</td> <td>Modified</td> <td>An existing file has been changed</td> </tr> <tr> <td>D</td> <td>Deleted</td> <td>A file has been deleted</td> </tr> <tr> <td>U</td> <td>Untracked</td> <td>The file is new or has been changed but has not been added to the repository yet</td> </tr> <tr> <td>C</td> <td>Conflict</td> <td>There is a conflict in the file</td> </tr> <tr> <td>R</td> <td>Renamed</td> <td>The file has been renamed</td> </tr> <tr> <td>S</td> <td>Submodule</td> <td>In repository exists another subrepository</td> </tr> <tr> <td>T</td> <td>Typechange</td> <td>The file changed from symlink to regular file, or vice versa</td> </tr> </tbody> </table> ]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[ğŸ’¡ìµìˆ™í•´ì§„ ë‹¨ì¶•í‚¤ëŠ” ë¬¸ì„œì—ì„œ ì‚­ì œ]]></summary></entry><entry><title type="html">(ENG) Brief History of AI</title><link href="https://joohunhyun.github.io/blog/2024/history-of-ML/" rel="alternate" type="text/html" title="(ENG) Brief History of AI"/><published>2024-12-13T00:00:00+00:00</published><updated>2024-12-13T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/history-of-ML</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/history-of-ML/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#1940s1950s---early-foundations">1940sâ€“1950s - Early Foundations</a></li> <li><a href="#1950s1960s---discovery-of-neural-networks">1950sâ€“1960s - Discovery of Neural Networks</a></li> <li><a href="#1970s1980s---ais-first-winterì¸ê³µì§€ëŠ¥ì˜-ì²«-ê²¨ìš¸--backpropagationì—­ì „íŒŒê¸°ë²•">1970sâ€“1980s - AIâ€™s First Winter(ì¸ê³µì§€ëŠ¥ì˜ ì²« ê²¨ìš¸) \&amp; Backpropagation(ì—­ì „íŒŒê¸°ë²•)</a></li> <li><a href="#1980s1990s---sequence-processing-and-memory">1980sâ€“1990s - Sequence Processing and Memory</a></li> <li><a href="#1997---long-short-term-memorylstm-hochreiter--schmidhuber">1997 - Long Short-Term Memory(LSTM) (Hochreiter \&amp; Schmidhuber)</a></li> <li><a href="#1980s1990s---visual-recognition-and-spatial-data">1980sâ€“1990s - Visual Recognition and Spatial Data</a></li> <li><a href="#2012---deep-learning-revolution">2012 - Deep Learning Revolution</a></li> <li><a href="#2014---generative-adversarial-networksgan-goodfellow">2014 - Generative Adversarial Networks(GAN) (Goodfellow)</a></li> <li><a href="#2017---attention-mechanisms-and-transformers-vaswani-et-al">2017 - Attention Mechanisms and Transformers (Vaswani et al.)</a></li> <li><a href="#2020s---multi-modal-ai-and-real-time-applications">2020s - Multi-Modal AI and Real-Time Applications</a></li> <li><a href="#tldr">TL;DR</a></li> </ul> <hr/> <h4 id="1940s1950s---early-foundations">1940sâ€“1950s - Early Foundations</h4> <p>Discovery: Boolean logic and the Turing Test</p> <p>Problem: How can we formalize reasoning and test machine intelligence?</p> <p>Solution: Alan Turing introduced the concept of a machine capable of computation (Turing Machine) and proposed the Turing Test as a way to determine if a machine exhibits intelligent behavior.</p> <p><br/></p> <h4 id="1950s1960s---discovery-of-neural-networks">1950sâ€“1960s - Discovery of Neural Networks</h4> <p>Discovery: Perceptron (1958, Frank Rosenblatt)</p> <p>Problem: How can we mimic the human brainâ€™s ability to learn patterns?</p> <p>Solution: The perceptron, a simple single-layer neural network, was developed to classify data linearly by adjusting weights using feedback. However, it was unable to solve non-linear problems, such as XOR.</p> <p><br/></p> <h4 id="1970s1980s---ais-first-winterì¸ê³µì§€ëŠ¥ì˜-ì²«-ê²¨ìš¸--backpropagationì—­ì „íŒŒê¸°ë²•">1970sâ€“1980s - AIâ€™s First Winter(ì¸ê³µì§€ëŠ¥ì˜ ì²« ê²¨ìš¸) &amp; Backpropagation(ì—­ì „íŒŒê¸°ë²•)</h4> <p>Discovery: Backpropagation (1986, Rumelhart, Hinton, Williams)</p> <p>Problem: How can multi-layer neural networks be efficiently trained?</p> <p>Solution: Backpropagation introduced a systematic way to compute gradients and update weights in deep networks using the chain rule, which is the cornerstone of deep learning.This allowed to solve non-linear problems.Applications suggested during that period included image recognition and character classification but were limited by computational power and data availability.</p> <p><br/></p> <h4 id="1980s1990s---sequence-processing-and-memory">1980sâ€“1990s - Sequence Processing and Memory</h4> <p>Discovery: Recurrent Neural Networks(RNN) (1986, Rumelhart &amp; McClelland)</p> <p>Problem: How can sequential data be modeled and retain context from prior inputs?</p> <p>Solution: RNNs introduced a feedback loop where outputs from previous steps are fed back as inputs, allowing networks to maintain a â€œmemoryâ€ over time.</p> <p>Limitations: RNNs faced vanishing gradient issues, making them ineffective for long sequences -&gt; LSTM was introduced to resolve this issue.</p> <p><br/></p> <h4 id="1997---long-short-term-memorylstm-hochreiter--schmidhuber">1997 - Long Short-Term Memory(LSTM) (Hochreiter &amp; Schmidhuber)</h4> <p>Problem: How can we learn and retain long-term dependencies in sequences</p> <p>Solution: LSTMs introduced gated cells to control the flow of information, solving vanishing gradient problems. Applications include speech recognition and language translation.</p> <p><br/></p> <h4 id="1980s1990s---visual-recognition-and-spatial-data">1980sâ€“1990s - Visual Recognition and Spatial Data</h4> <p>Discovery: Convolutional Neural Networks (CNNs, 1989, LeCun)</p> <p>Problem: How to efficiently process and recognize spatially structured data like images.</p> <p>Solution: CNNs use convolutional layers to detect patterns such as edges and textures by learning spatial hierarchies. Pooling layers reduce dimensionality while preserving critical information.</p> <p>Applications: Handwritten digit recognition (e.g., MNIST dataset) and later extended to more complex tasks like object detection.</p> <p><br/></p> <h4 id="2012---deep-learning-revolution">2012 - Deep Learning Revolution</h4> <p>Discovery: Deep CNNs and AlexNet (2012, Krizhevsky, Sutskever, Hinton)</p> <p>Problem: How to achieve state-of-the-art(SOTA) accuracy in image recognition.</p> <p>Solution: AlexNet leveraged deeper architectures, ReLU activations, and GPUs for training, achieving a breakthrough in the ImageNet competition. (Previous SOTA classification : 74% -&gt; Alexnet <code class="language-plaintext highlighter-rouge">84.69%</code>)</p> <p>Applications: Facial recognition, autonomous vehicles, and medical imaging.</p> <p><br/></p> <h4 id="2014---generative-adversarial-networksgan-goodfellow">2014 - Generative Adversarial Networks(GAN) (Goodfellow)</h4> <p>Problem: How can we generate new output data that resembles an input dataset?</p> <p>Solution: GANs use a generator and discriminator in a competitive framework, enabling applications like image synthesis and style transfer.</p> <p><br/></p> <h4 id="2017---attention-mechanisms-and-transformers-vaswani-et-al">2017 - Attention Mechanisms and Transformers (Vaswani et al.)</h4> <p>Problem: How can we handle long-term dependencies and parallelize(ë³‘ë ¬í™”) sequence processing?</p> <p>Solution: The attention mechanism learns relationships between all inputs simultaneously, eliminating sequential constraints. Transformers like BERT and GPT revolutionized natural language processing (NLP).</p> <p>Applications: Machine translation, summarization, and chatbots.</p> <p><br/></p> <h4 id="2020s---multi-modal-ai-and-real-time-applications">2020s - Multi-Modal AI and Real-Time Applications</h4> <p>Discovery: Multi-Modal Models (e.g., CLIP, DALL-E)</p> <p>Problem: How can we unify processing across different modalities like text, images, and audio?</p> <p>Solution: Pretraining large-scale models that align multiple data types, enabling applications in creative AI and real-time analysis.</p> <p><br/></p> <h4 id="tldr">TL;DR</h4> <ul> <li>Backpropagation solved problem of non-linearity.</li> <li>RNNs/LSTMs enabled handling sequential data.</li> <li>CNNs revolutionized image processing.</li> <li>Transformers led to breakthroughs in NLP and multi-modal AI.</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML,"/><category term="DL"/><summary type="html"><![CDATA[Table of Contents 1940sâ€“1950s - Early Foundations 1950sâ€“1960s - Discovery of Neural Networks 1970sâ€“1980s - AIâ€™s First Winter(ì¸ê³µì§€ëŠ¥ì˜ ì²« ê²¨ìš¸) \&amp; Backpropagation(ì—­ì „íŒŒê¸°ë²•) 1980sâ€“1990s - Sequence Processing and Memory 1997 - Long Short-Term Memory(LSTM) (Hochreiter \&amp; Schmidhuber) 1980sâ€“1990s - Visual Recognition and Spatial Data 2012 - Deep Learning Revolution 2014 - Generative Adversarial Networks(GAN) (Goodfellow) 2017 - Attention Mechanisms and Transformers (Vaswani et al.) 2020s - Multi-Modal AI and Real-Time Applications TL;DR]]></summary></entry><entry><title type="html">(ENG) Backpropagation</title><link href="https://joohunhyun.github.io/blog/2024/backpropagation/" rel="alternate" type="text/html" title="(ENG) Backpropagation"/><published>2024-12-12T00:00:00+00:00</published><updated>2024-12-12T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/backpropagation</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/backpropagation/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#before-delving-in">Before delving in</a> <ul> <li><a href="#fitting-the-curve">Fitting the curve</a></li> <li><a href="#what-is-the-best-curve">What is the â€œbest curveâ€?</a></li> <li><a href="#minimizing-the-loss-function">Minimizing the loss function</a> <ul> <li><a href="#limitation-of-the-perceptron">Limitation of the Perceptron</a></li> <li><a href="#how-backpropagation-solved-this-issue">How Backpropagation solved this issue?</a></li> </ul> </li> <li><a href="#chain-rule-behind-backpropagation">Chain Rule behind Backpropagation</a> <ul> <li><a href="#composition-of-a-nn">Composition of a NN</a></li> <li><a href="#chain-rule">Chain Rule</a></li> </ul> </li> </ul> </li> <li><a href="#references">References</a></li> </ul> <p><br/></p> <h3 id="before-delving-in">Before delving in</h3> <h4 id="fitting-the-curve">Fitting the curve</h4> <h4 id="what-is-the-best-curve">What is the â€œbest curveâ€?</h4> <p>Fitting data points across a plane using a curve is the most important question.</p> <p>So what is the â€œbest curveâ€?</p> <p>Loss function : Measure of total squared distance between the points and the curve</p> <p>ğŸ§  It is referred to as a <strong>function</strong> because it has multiple parameters \(\eta = (k_0 ... k_5)\)</p> <p>The function yields a sigle value, where low value = good fit</p> <h4 id="minimizing-the-loss-function">Minimizing the loss function</h4> <p>How can we find the best configuration of \((k_0 ... k_5)\), or in other words, minimize the loss function?</p> <p>Methods Random pertubation : random changes to parameters</p> <p>How can we make predictions to the value of \(\eta\)$ without performing all calculations via brute-force?</p> <p>Gradient Descent Method</p> <p><a href="path=&quot;assets/img/image.png&quot;">Derivative of a function</a></p> <p>Sometimes the derivative is unknown (not derivable)</p> <h5 id="limitation-of-the-perceptron">Limitation of the Perceptron</h5> <p>a) Linear Separability</p> <p>A single-layer perceptron uses a linear decision boundary to classify data. It only functioned well if the data can be separated by a straight line (or hyperplane in higher dimensions). However, some problems like XOR are not linearly separable.</p> <p>b) Absence of Hidden Layers</p> <p>The perceptron lacks hidden layers. Without them, it cannot model complex relationships between input features.</p> <p>c) Learning is not Incremental</p> <p>Learning is not incremental over time - meaning it has no retention of previous learning.</p> <p>c) Inability to Learn Nonlinear Functions</p> <p>The perceptron updates its weights using a simple rule:</p> \[w_i+1=w_i + \eta(y-\hat{y})x\] <p>This works for linear problems but fails for nonlinear problems, as the perceptron has no mechanism to capture nonlinear patterns.</p> <h5 id="how-backpropagation-solved-this-issue">How Backpropagation solved this issue?</h5> <p>Backpropagation(1986, Hinton) solved these problems by enabling multi-layer networks to learn nonlinear decision boundaries.</p> <p>a) Nonlinear Activation Functions</p> <p>Backpropagation allows the use of nonlinear activation functions (e.g., sigmoid, tanh, ReLU). Nonlinear activations enable the network to combine inputs in complex ways, effectively learning nonlinear decision boundaries.</p> <p>b) Hidden Layers</p> <p>Backpropagation trains networks with multiple layers of neurons (hidden layers). Hidden layers allow the network to create hierarchical representations of data, transforming input features into complex, abstract representations.</p> <p>For example, in the XOR problem:</p> <ol> <li>The first hidden layer transforms the inputs into a new feature space.</li> <li>The second layer uses this new space to create a nonlinear decision boundary.</li> </ol> <p>c) Learning Complex Relationships</p> <p>Backpropagation applies the chain rule to compute gradients layer by layer, allowing the network to adjust weights in all layers based on how they affect the output error. This enables the network to learn mappings for nonlinear functions, solving problems like XOR.</p> <h4 id="chain-rule-behind-backpropagation">Chain Rule behind Backpropagation</h4> <h5 id="composition-of-a-nn">Composition of a NN</h5> <ol> <li>Layers with weights : \(w\) and biases : \(b\)</li> <li>Activation function : \(f\)</li> <li>Ouput function : \(\hat{y}\)</li> <li>Loss function : \(\eta\)</li> </ol> <p>A single layer NN equation, where \(x\) is the input :</p> <ul> <li>The goal is to minimize loss:</li> </ul> \[\hat{y} = f(w \cdot x + b)\] \[\eta=Loss(\hat{y} ,y)\] <h5 id="chain-rule">Chain Rule</h5> <p>The <strong>chain rule</strong>(ì—°ì‡„ë²•ì¹™)</p> <ol> <li>Given functions \(f\) and \(g\) that are <strong>both differentiable</strong>, and a composite function \(F = f(g(x)) = f \circ g\)</li> <li>Then, \(F'(x) = f'(g(x)) \circ g'(x)\)</li> <li>If we assume \(t=g(x)\)</li> <li>Then, \(\frac{dy}{dx} = \frac{dt}{dx} \cdot \frac{dy}{dt}\)</li> </ol> <h3 id="references">References</h3> <ol> <li><a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">Video Reference</a></li> <li><a href="https://evan-moon.github.io/2018/07/19/deep-learning-backpropagation/">Blog Reference</a></li> </ol>]]></content><author><name></name></author><category term="study"/><category term="backpropagation,"/><category term="DL"/><summary type="html"><![CDATA[backpropagation]]></summary></entry><entry><title type="html">(KOR) ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•</title><link href="https://joohunhyun.github.io/blog/2024/markdown-syntax/" rel="alternate" type="text/html" title="(KOR) ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•"/><published>2024-12-12T00:00:00+00:00</published><updated>2024-12-12T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/markdown-syntax</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/markdown-syntax/"><![CDATA[<p>â€˜<a href="https://www.markdownguide.org">ê³µì‹ ë¬¸ì„œ</a>â€™</p> <h4 id="ë¬¸ë‹¨">ë¬¸ë‹¨</h4> <ul> <li>ì¤„ë°”ê¿ˆì„ ë‘ë²ˆ</li> <li>ì¤„ ëì— ì—­ìŠ¬ë˜ì‹œ</li> </ul> <h4 id="ëª©ë¡">ëª©ë¡</h4> <ul> <li>ìˆœì„œê°€ ì—†ëŠ” ëª©ë¡ (bullet points) <ul> <li>(-) ë˜ëŠ” (*) ì‚¬ìš©</li> </ul> </li> <li>ìˆœì„œê°€ ìˆëŠ” ëª©ë¡ <ul> <li>(1.abcd) -&gt; ì´ëŸ°ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë‚˜, ìˆ«ìëŠ” ë°˜ë“œì‹œ ë§ì¶°ì„œ ì“¸ í•„ìš”ëŠ” ì—†ë‹¤.</li> </ul> </li> </ul> <h4 id="ê¸€ì-ëª¨ì–‘">ê¸€ì ëª¨ì–‘</h4> <ul> <li>bold : wrap with ** **</li> <li>italicize : wrap with * *</li> </ul> <h4 id="ì½”ë“œë¸”ëŸ­">ì½”ë“œë¸”ëŸ­</h4> <ul> <li>(```) ì‚¬ìš©</li> <li>ë§¥ í™˜ê²½ì—ì„œëŠ” í‚¤ë³´ë“œê°€ <strong>ì˜ì–´</strong>ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ ë°±í‹±(`)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤</li> </ul> <h4 id="ë¸”ëŸ­">ë¸”ëŸ­</h4> <ul> <li>(`) ì‚¬ìš©</li> <li>ë…¸ì…˜ì˜ CMD+E ê¸°ëŠ¥ê³¼ ë™ì¼</li> </ul> <h4 id="ë§í¬">ë§í¬</h4> <p>-<code class="language-plaintext highlighter-rouge">[link_description](url)</code> í˜•ì‹ìœ¼ë¡œ ì‘ì„±</p> <h4 id="ê°€ë¡œì¤„">ê°€ë¡œì¤„</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>***
OR
- - -
</code></pre></div></div>]]></content><author><name></name></author><category term="study"/><category term="markdown,"/><category term="ë§ˆí¬ë‹¤ìš´,"/><category term="tools"/><summary type="html"><![CDATA[ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•]]></summary></entry><entry><title type="html">(ENG) Github Commit Message Conventions</title><link href="https://joohunhyun.github.io/blog/2024/github-conventions-copy/" rel="alternate" type="text/html" title="(ENG) Github Commit Message Conventions"/><published>2024-12-11T00:00:00+00:00</published><updated>2024-12-11T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/github-conventions%20copy</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/github-conventions-copy/"><![CDATA[<p>Commit messages should abide to the following convention :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Activity: Commit Message    
</code></pre></div></div> <ul> <li>Activivities <ul> <li><code class="language-plaintext highlighter-rouge">feat</code>: new features</li> <li><code class="language-plaintext highlighter-rouge">fix</code>: fix an error or issue</li> <li><code class="language-plaintext highlighter-rouge">chore</code> : ë¹Œë“œ ìˆ˜ì •, íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì •, ìš´ì˜ ì½”ë“œ ë³€ê²½ì´ ì—†ëŠ” ê²½ìš° ë“±</li> <li><code class="language-plaintext highlighter-rouge">mod</code>: modify existing feature</li> <li><code class="language-plaintext highlighter-rouge">rfc</code>: refactor code</li> <li><code class="language-plaintext highlighter-rouge">rmv</code>: remove existing file or directory</li> <li><code class="language-plaintext highlighter-rouge">doc</code>: changes to document or comment</li> </ul> </li> <li>Example <ul> <li><code class="language-plaintext highlighter-rouge">int: initial commit</code></li> <li><code class="language-plaintext highlighter-rouge">add: prettier and eslint</code></li> <li><code class="language-plaintext highlighter-rouge">rfc: refactoring code by prettier</code></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="tools,"/><category term="github,"/><category term="git"/><summary type="html"><![CDATA[gitub commmit message convetions]]></summary></entry><entry><title type="html">(KOR) ë…¼ë¬¸ ë¦¬ë·° ë°©ë²•ë¡ </title><link href="https://joohunhyun.github.io/blog/2024/journal-reading/" rel="alternate" type="text/html" title="(KOR) ë…¼ë¬¸ ë¦¬ë·° ë°©ë²•ë¡ "/><published>2024-12-11T00:00:00+00:00</published><updated>2024-12-11T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/journal-reading</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/journal-reading/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Lecture notes from Stanford CS230 course</code></p> <p><strong>Table of Contents</strong></p> <ul> <li><a href="#ë…¼ë¬¸ì„-ì½ëŠ”-ë‹¨ê³„">ë…¼ë¬¸ì„ ì½ëŠ” ë‹¨ê³„</a></li> <li><a href="#ì»¤ë¦¬ì–´-ì¡°ì–¸---ì±„ìš©-ë‹´ë‹¹ìê°€-ì¤‘ìš”ì‹œí•˜ëŠ”-ìš”ì†Œ">ì»¤ë¦¬ì–´ ì¡°ì–¸ - ì±„ìš© ë‹´ë‹¹ìê°€ ì¤‘ìš”ì‹œí•˜ëŠ” ìš”ì†Œ</a></li> <li><a href="#tldr">TL;DR</a></li> </ul> <hr/> <h4 id="ë…¼ë¬¸ì„-ì½ëŠ”-ë‹¨ê³„">ë…¼ë¬¸ì„ ì½ëŠ” ë‹¨ê³„</h4> <ol> <li>ì •ë³´ ìˆ˜ì§‘ <ul> <li>íŠ¸ìœ„í„°</li> <li>ML ì„œë¸Œë ˆë”§ Â <a href="https://www.reddit.com/r/MachineLearning/">r/MachineLearning</a></li> <li>ML/DL ì»¨í¼ëŸ°ìŠ¤ :Â <a href="https://nips.cc/">NIPS</a>/<a href="https://icml.cc/">ICML</a>/<a href="https://iclr.cc/">ICLR</a></li> </ul> </li> <li>ì–´ë–»ê²Œ ì½ì„ ê²ƒì¸ê°€? <ul> <li>ë§ì€ ì–‘ì˜ ë…¼ë¬¸ì„ ì½ì–´ì•¼í•  ë•Œë§Œ í•´ë‹¹ë¨. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ì •ë…í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤</li> <li>Title/abstract/figuresë¥¼ ì½ì–´ë³¸ë‹¤</li> <li>Intro/conclusion/figuresë¥¼ ì½ê³ , ë‚˜ë¨¸ì§€ëŠ” í›‘ì–´ë³¸ë‹¤ <ul> <li>related workëŠ” ê³¼ê°í•˜ê²Œ ë„˜ì–´ê°„ë‹¤. ëŒ€ê°œì˜ ê²½ìš° ë…¼ë¬¸ì˜ ì €ìê°€ ì§€ë„êµìˆ˜ë‹˜ ë“±ì˜ ë…¼ë¬¸ì„ ì¡´ì¤‘í•˜ëŠ” ì˜ë¯¸ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.</li> </ul> </li> <li>ìˆ˜ì‹ì€ ë„˜ì–´ê°€ê±°ë‚˜ í›‘ì–´ë³¸ë‹¤</li> <li>ë…¼ë¬¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì½ì–´ë³¸ë‹¤ (ì´í•´ê°€ ì•ˆë˜ëŠ” ë¶€ë¶„ì€ ë„˜ì–´ê°„ë‹¤)</li> </ul> </li> <li>ì…€í”„ ì¸í„°ë·° <ul> <li>What did the author try to accomplish?</li> <li>What were key elements?</li> <li>What can you use yourself?</li> <li>What other references do you want to follow?</li> </ul> </li> <li>ì½”ë“œ ì—°ìŠµ <ul> <li>ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ë‹¤ìš´ë°›ì•„ì„œÂ ì‹¤í–‰í•´ë³¸ë‹¤.</li> <li>ë°‘ë°”ë‹¥ë¶€í„° ì§ì ‘ êµ¬í˜„í•´ë³¸ë‹¤.</li> </ul> </li> </ol> <h4 id="ì»¤ë¦¬ì–´-ì¡°ì–¸---ì±„ìš©-ë‹´ë‹¹ìê°€-ì¤‘ìš”ì‹œí•˜ëŠ”-ìš”ì†Œ">ì»¤ë¦¬ì–´ ì¡°ì–¸ - ì±„ìš© ë‹´ë‹¹ìê°€ ì¤‘ìš”ì‹œí•˜ëŠ” ìš”ì†Œ</h4> <ul> <li>ML ê´€ë ¨ ì „ê³µì§€ì‹</li> <li>ì½”ë”© ëŠ¥ë ¥</li> <li>Meaningful work (ì˜ë¯¸ìˆëŠ” í”„ë¡œì íŠ¸)</li> <li>Open source (githubì— ì½”ë“œ ê²Œì‹œ)</li> <li>ì¸í„´ì‰½</li> </ul> <h4 id="tldr">TL;DR</h4> <p><strong>ë…¼ë¬¸ì„ ì½ëŠ” ìˆœì„œ</strong></p> <ol> <li>Title/abstract/figuresë¥¼ ì½ì–´ë³¸ë‹¤</li> <li>Intro/conclusion/figuresë¥¼ ì½ê³ , ë‚˜ë¨¸ì§€ëŠ” í›‘ì–´ë³¸ë‹¤</li> <li>ìˆ˜ì‹ì€ ë„˜ì–´ê°€ê±°ë‚˜ í›‘ì–´ë³¸ë‹¤</li> <li>ë…¼ë¬¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì½ì–´ë³¸ë‹¤ (ì´í•´ê°€ ì•ˆë˜ëŠ” ë¶€ë¶„ì€ ë„˜ì–´ê°„ë‹¤)</li> </ol> <p><strong>ë…¼ë¬¸ì„ ì½ì€ í›„ì— í•´ì•¼í•  ì§ˆë¬¸ 4ê°€ì§€</strong></p> <ul> <li>What did the author try to accomplish?</li> <li>What were key elements?</li> <li>What can you use yourself?</li> <li>What other references do you want to follow?</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="papers,"/><category term="ë…¼ë¬¸ë¦¬ë·°"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° ë°©ë²•]]></summary></entry></feed>