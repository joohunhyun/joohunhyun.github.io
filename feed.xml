<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joohunhyun.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joohunhyun.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-30T04:08:33+00:00</updated><id>https://joohunhyun.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">(KOR) AI Agentë€ ë¬´ì—‡ì¸ê°€?</title><link href="https://joohunhyun.github.io/blog/2024/AI-Agents/" rel="alternate" type="text/html" title="(KOR) AI Agentë€ ë¬´ì—‡ì¸ê°€?"/><published>2024-12-29T00:00:00+00:00</published><updated>2024-12-29T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/AI%20Agents</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/AI-Agents/"><![CDATA[<ul> <li><a href="#1-ai-agents--introduction">1. AI Agents : Introduction</a></li> <li><a href="#2-ai-agent--methodology">2. AI Agent : Methodology</a> <ul> <li><a href="#21-ë™ì‘-ìˆœì„œ">2.1 ë™ì‘ ìˆœì„œ</a></li> <li><a href="#22-ë™ì‘-ì›ë¦¬">2.2 ë™ì‘ ì›ë¦¬</a></li> <li><a href="#23-ìƒí˜¸ì‘ìš©">2.3 ìƒí˜¸ì‘ìš©</a></li> <li><a href="#24-ai-agent-ì¢…ë¥˜">2.4 AI Agent ì¢…ë¥˜</a> <ul> <li><a href="#241-ë‹¨ìˆœ-ë°˜ì‘í˜•-ì—ì´ì „íŠ¸-reactive-agent">2.4.1 ë‹¨ìˆœ ë°˜ì‘í˜• ì—ì´ì „íŠ¸ (Reactive Agent)</a></li> <li><a href="#242-ìƒíƒœ-ê¸°ë°˜-ì—ì´ì „íŠ¸-model-based-agent">2.4.2 ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ (Model-Based Agent)</a></li> <li><a href="#243-ëª©í‘œ-ì§€í–¥í˜•-ì—ì´ì „íŠ¸-goal-based-agent">2.4.3 ëª©í‘œ ì§€í–¥í˜• ì—ì´ì „íŠ¸ (Goal-Based Agent)</a></li> <li><a href="#244-ìœ í‹¸ë¦¬í‹°-ê¸°ë°˜-ì—ì´ì „íŠ¸-utility-based-agent">2.4.4 ìœ í‹¸ë¦¬í‹° ê¸°ë°˜ ì—ì´ì „íŠ¸ (Utility-Based Agent)</a></li> <li><a href="#245-í•™ìŠµ-ì—ì´ì „íŠ¸-learning-agent">2.4.5 í•™ìŠµ ì—ì´ì „íŠ¸ (Learning Agent)</a></li> <li><a href="#246-ë‹¤ì¤‘-ì—ì´ì „íŠ¸-ì‹œìŠ¤í…œ-multi-agent-system">2.4.6 ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ (Multi-Agent System)</a></li> <li><a href="#247-í•˜ì´ë¸Œë¦¬ë“œ-ì—ì´ì „íŠ¸-hybrid-agent">2.4.7 í•˜ì´ë¸Œë¦¬ë“œ ì—ì´ì „íŠ¸ (Hybrid Agent)</a></li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="1-ai-agents--introduction">1. AI Agents : Introduction</h3> <p>AI agentëŠ” ì‚¬ëŒì˜ ê°œì…ì—†ì´ AIê°€ task ìˆ˜í–‰ì„ ìœ„í•œ ëª¨ë“  ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” agentë¥¼ ì˜ë¯¸í•œë‹¤. AI agentëŠ” ì‚¬ìš©ìì˜ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì—†ì´, í•˜ìœ„ taskë¥¼ ì„¤ê³„í•˜ê³ , ê²€í† í•˜ê³ , ìˆ˜í–‰í•˜ëŠ” ê³¼ì •(<strong>Self-Prompting</strong>)ì„ ë°˜ë³µí•˜ëŠ” íŠ¹ì§•ì„ ê°€ì§„ë‹¤. ë³€í•˜ëŠ” ìƒí™©ì— ë”°ë¼ ì ‘ê·¼ ë°©ì‹ì„ ìŠ¤ìŠ¤ë¡œ ì¡°ì •í•˜ë©° ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ì‹¤í–‰í•œë‹¤.</p> <p>AI agentì˜ ìš”ê±´</p> <ol> <li>ì‘ì—… ì‹¤í–‰ì„ ê³„íší•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œ</li> <li>ì‚¬ìš©ìë¥¼ ëŒ€ì‹ í•´ ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨</li> <li>ì¸ê°„ì˜ ì§€ì†ì ì¸ ì…ë ¥ í•„ìš”ì—†ì´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê³ , ê²°ì •í•˜ê³ , í–‰ë™í•¨</li> </ol> <p>ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìì˜ í”„ë¡¬í”„íŠ¸ê°€ â€œ<strong>3ë°•4ì¼ ì¼ì •ìœ¼ë¡œ ì¼ë³¸ ì—¬í–‰ì„ ê°€ê³  ì‹¶ì–´. 200ë§Œì› ì˜ˆì‚° ë‚´ë¡œ ì—¬í–‰ì„ ì„¤ê³„í•´ì¤˜</strong>.â€ì¼ ê²½ìš°, AI AgentëŠ”:</p> <ul> <li>ì‚¬ìš©ìì˜ ìº˜ë¦°ë”ì•±ì„ í™•ì¸í•´ ì—¬í–‰ ì¼ì • ì¶”ì²œ</li> <li>skyscannerë¥¼ í†µí•´ ì—¬í–‰ ì¼ì •ì— ë§ëŠ” í•­ê³µê¶Œ ì¶”ì²œ</li> <li>ìˆ™ë°• ì‚¬ì´íŠ¸ë¥¼ í†µí•´ ì˜ˆì‚°ì— ë§ëŠ” í˜¸í…” ê²€ìƒ‰</li> <li>ê´€ê´‘ì§€ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ ê´€ê´‘ì§€ ìš´ì˜ ì‹œê°„ ë“±ì„ í™•ì¸</li> </ul> <hr/> <h3 id="2-ai-agent--methodology">2. AI Agent : Methodology</h3> <h4 id="21-ë™ì‘-ìˆœì„œ">2.1 ë™ì‘ ìˆœì„œ</h4> <p>steps</p> <ol> <li>*environmentë¥¼ íŒŒì•…í•˜ê³ </li> <li>ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬</li> <li>ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ê³„íšì„ ì„¸ìš°ê³ </li> <li>ì´ë¥¼ ì‹¤í–‰í•¨</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/16-480.webp 480w,/assets/img/16-800.webp 800w,/assets/img/16-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="22-ë™ì‘-ì›ë¦¬">2.2 ë™ì‘ ì›ë¦¬</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/17-480.webp 480w,/assets/img/17-800.webp 800w,/assets/img/17-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>steps</p> <ol> <li>observationsë¥¼ í†µí•´ environmentë¥¼ íŒŒì•…í•˜ê³ </li> <li>past experiences, abilities, goals/preferencees/prior knowledgeë¥¼ ë°”íƒ•ìœ¼ë¡œ actions ìˆ˜í–‰</li> <li>1,2 ë²ˆ (ë°˜ë³µ)</li> </ol> <ul> <li><strong>environment</strong> : agentê°€ ì‘ë™í•˜ëŠ” ì˜ì—­/ë„ë©”ì¸ (ì—¬í–‰ ê³„íš ìˆ˜ë¦½ agentì˜ˆì‹œì˜ ê²½ìš°, ì‚¬ìš©ìì˜ ìº˜ë¦°ë”, ìœ„ì¹˜ì •ë³´, ì–¸ì–´ì„¤ì •, ë“±)</li> <li>abilities/prior knowledge : ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ ì´ë¯¸ í•™ìŠµí•œ ì§€ì‹</li> <li>goals/preferences : ì‚¬ìš©ìì˜ ìš”ì²­ì˜ ëª©ì , ìˆ˜í–‰í•  taskë“¤ì„ agentê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨</li> <li>observations : ì£¼ì–´ì§„ í™˜ê²½ì— ëŒ€í•´ ìˆ˜ì§‘í•œ ì •ë³´</li> <li>past experiences : Agentê°€ ê°€ì§„ ê³¼ê±°ì˜ ê²½í—˜</li> <li></li> </ul> <h4 id="23-ìƒí˜¸ì‘ìš©">2.3 ìƒí˜¸ì‘ìš©</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/18-480.webp 480w,/assets/img/18-800.webp 800w,/assets/img/18-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="24-ai-agent-ì¢…ë¥˜">2.4 AI Agent ì¢…ë¥˜</h4> <p>ì£¼ìš” agentë“¤ : goal-based agent</p> <h5 id="241-ë‹¨ìˆœ-ë°˜ì‘í˜•-ì—ì´ì „íŠ¸-reactive-agent">2.4.1 ë‹¨ìˆœ ë°˜ì‘í˜• ì—ì´ì „íŠ¸ (Reactive Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³ , í˜„ì¬ í™˜ê²½ì—ì„œ ê´€ì°°í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¦‰ê°ì ì¸ ë°˜ì‘ì„ ë³´ì´ëŠ” ì—ì´ì „íŠ¸</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ìë™ë¬¸ ì‹œìŠ¤í…œ: ì„¼ì„œë¥¼ í†µí•´ ì‚¬ëŒì˜ ì¡´ì¬ë¥¼ ì¸ì‹í•˜ê³  ë¬¸ì„ ì—¬ëŠ” ì—­í•  ìˆ˜í–‰</li> <li>ì˜¨ë„ ì¡°ì ˆê¸°: í˜„ì¬ ì˜¨ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚œë°© ë˜ëŠ” ëƒ‰ë°©ì„ ìˆ˜í–‰</li> </ul> </li> </ul> <h5 id="242-ìƒíƒœ-ê¸°ë°˜-ì—ì´ì „íŠ¸-model-based-agent">2.4.2 ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ (Model-Based Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: í˜„ì¬ ìƒíƒœì™€ ê³¼ê±° ìƒíƒœë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ ìˆ˜í–‰í•˜ë©°, í™˜ê²½ì— ëŒ€í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ë„¤ë¹„ê²Œì´ì…˜ ì‹œìŠ¤í…œ: ëª©ì ì§€ì™€ í˜„ì¬ ìœ„ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê²½ë¡œë¥¼ ê³„ì‚°</li> </ul> </li> </ul> <h5 id="243-ëª©í‘œ-ì§€í–¥í˜•-ì—ì´ì „íŠ¸-goal-based-agent">2.4.3 ëª©í‘œ ì§€í–¥í˜• ì—ì´ì „íŠ¸ (Goal-Based Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ëª©í‘œ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ë™í•˜ë©°, í˜„ì¬ ìƒíƒœì™€ ëª©í‘œ ìƒíƒœì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ê³„íšì„ ìˆ˜ë¦½í•¨</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ë¡œë´‡ ì²­ì†Œê¸°: ë°©ì˜ ì²­ì†Œê°€ í•„ìš”í•œ ìœ„ì¹˜ë¥¼ íƒì§€í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì›€ì§ì„</li> <li>ë¬¼ë¥˜ ë¡œë´‡: íŠ¹ì • ì§€ì ì— ë¬¼ê±´ì„ ìš´ë°˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰</li> </ul> </li> </ul> <h5 id="244-ìœ í‹¸ë¦¬í‹°-ê¸°ë°˜-ì—ì´ì „íŠ¸-utility-based-agent">2.4.4 ìœ í‹¸ë¦¬í‹° ê¸°ë°˜ ì—ì´ì „íŠ¸ (Utility-Based Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ëª©í‘œ ë‹¬ì„±ë¿ë§Œ ì•„ë‹ˆë¼ ëª©í‘œ ë‹¬ì„±ì˜ â€œì§ˆâ€ì„ ê³ ë ¤í•˜ì—¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜(ë§Œì¡±ë„, ì´ìµ ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì„ íƒì„ ìˆ˜í–‰í•¨</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ììœ¨ì£¼í–‰ ì°¨ëŸ‰: ì•ˆì „, ì‹œê°„, ì—°ë£Œ íš¨ìœ¨ì„±ì„ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ ê²½ë¡œ ì„ íƒ</li> <li>ì¶”ì²œ ì‹œìŠ¤í…œ: ì‚¬ìš©ì ì„ í˜¸ë„ì— ë”°ë¼ ìµœì ì˜ ì˜µì…˜ì„ ì¶”ì²œ</li> </ul> </li> </ul> <h5 id="245-í•™ìŠµ-ì—ì´ì „íŠ¸-learning-agent">2.4.5 í•™ìŠµ ì—ì´ì „íŠ¸ (Learning Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ê²½í—˜ì„ í†µí•´ í•™ìŠµí•˜ê³  ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì„±ëŠ¥ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ë©°, í•™ìŠµ ìš”ì†Œ, ì„±ëŠ¥ ìš”ì†Œ, ë¹„í‰ ìš”ì†Œ, ë¬¸ì œ ìƒì„±ê¸°ë¥¼ í¬í•¨í•œ êµ¬ì„± ìš”ì†Œë¡œ ì‘ë™</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>AlphaGo: ì²´ìŠ¤ ë˜ëŠ” ë°”ë‘‘ê³¼ ê°™ì€ ê²Œì„ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ê°œì„ </li> <li>ì±—ë´‡: ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ëŒ€í™” ëŠ¥ë ¥ í–¥ìƒ</li> </ul> </li> </ul> <h5 id="246-ë‹¤ì¤‘-ì—ì´ì „íŠ¸-ì‹œìŠ¤í…œ-multi-agent-system">2.4.6 ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ (Multi-Agent System)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ê±°ë‚˜ ê²½ìŸí•˜ë©° ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ, ê° ì—ì´ì „íŠ¸ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•˜ë©° ìƒí˜¸ì‘ìš©ì„ í†µí•´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•¨</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œ: ì „ë ¥ ë¶„ë°°ë¥¼ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œ</li> <li>ì˜¨ë¼ì¸ ê²Œì„ AI: ë‹¤ìˆ˜ì˜ ìºë¦­í„° ê°„ í˜‘ë ¥ ë° ê²½ìŸ (ì˜ˆ: Pacman)</li> </ul> </li> </ul> <h5 id="247-í•˜ì´ë¸Œë¦¬ë“œ-ì—ì´ì „íŠ¸-hybrid-agent">2.4.7 í•˜ì´ë¸Œë¦¬ë“œ ì—ì´ì „íŠ¸ (Hybrid Agent)</h5> <ul> <li><strong>íŠ¹ì§•</strong>: ìœ„ì—ì„œ ì–¸ê¸‰ëœ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì—ì´ì „íŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ì„¤ê³„ëœ ì‹œìŠ¤í…œìœ¼ë¡œ, ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•¨</li> <li><strong>ì˜ˆì‹œ</strong>: <ul> <li>ììœ¨ì£¼í–‰ ì°¨ëŸ‰: ê°ì§€(ë‹¨ìˆœ ë°˜ì‘í˜•)ì™€ ê²½ë¡œ ê³„íš(ëª©í‘œ ì§€í–¥í˜•)ì„ ë™ì‹œì— ìˆ˜í–‰</li> <li>ì¸ê³µì§€ëŠ¥ ë¹„ì„œ(Siri, Alexa): í•™ìŠµ ë° ëª©í‘œ ì§€í–¥í˜• ê¸°ëŠ¥ì„ ê²°í•©</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="NLP"/><summary type="html"><![CDATA[1. AI Agents : Introduction 2. AI Agent : Methodology 2.1 ë™ì‘ ìˆœì„œ 2.2 ë™ì‘ ì›ë¦¬ 2.3 ìƒí˜¸ì‘ìš© 2.4 AI Agent ì¢…ë¥˜ 2.4.1 ë‹¨ìˆœ ë°˜ì‘í˜• ì—ì´ì „íŠ¸ (Reactive Agent) 2.4.2 ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ (Model-Based Agent) 2.4.3 ëª©í‘œ ì§€í–¥í˜• ì—ì´ì „íŠ¸ (Goal-Based Agent) 2.4.4 ìœ í‹¸ë¦¬í‹° ê¸°ë°˜ ì—ì´ì „íŠ¸ (Utility-Based Agent) 2.4.5 í•™ìŠµ ì—ì´ì „íŠ¸ (Learning Agent) 2.4.6 ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ (Multi-Agent System) 2.4.7 í•˜ì´ë¸Œë¦¬ë“œ ì—ì´ì „íŠ¸ (Hybrid Agent)]]></summary></entry><entry><title type="html">(KOR) AutoGPT</title><link href="https://joohunhyun.github.io/blog/2024/AutoGPT/" rel="alternate" type="text/html" title="(KOR) AutoGPT"/><published>2024-12-29T00:00:00+00:00</published><updated>2024-12-29T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/AutoGPT</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/AutoGPT/"><![CDATA[<h3 id="autogpt-ê°œìš”">AutoGPT ê°œìš”</h3>]]></content><author><name></name></author><category term="study"/><category term="NLP"/><summary type="html"><![CDATA[AutoGPT ê°œìš”]]></summary></entry><entry><title type="html">(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model</title><link href="https://joohunhyun.github.io/blog/2024/BERT/" rel="alternate" type="text/html" title="(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model"/><published>2024-12-26T00:00:00+00:00</published><updated>2024-12-26T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/BERT</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/BERT/"><![CDATA[<ul> <li><a href="#1-ë…¼ë¬¸-ìš”ì•½">1. ë…¼ë¬¸ ìš”ì•½</a> <ul> <li><a href="#11-rationale">1.1 Rationale</a></li> <li><a href="#12-methodology-ìš”ì•½">1.2 Methodology ìš”ì•½</a></li> <li><a href="#13-conclusion">1.3 Conclusion</a></li> </ul> </li> <li><a href="#2-mbertì˜-í•œê³„">2. mBERTì˜ í•œê³„</a> <ul> <li><a href="#21-corpusë§ë­‰ì¹˜-domainì˜-í•œê³„">2.1 Corpus(ë§ë­‰ì¹˜) Domainì˜ í•œê³„</a></li> <li><a href="#22-í•œêµ­ì–´ì˜-ì–¸ì–´ì -íŠ¹ì„±ì„-ì¶©ë¶„íˆ-ê³ ë ¤í•˜ì§€-ì•Šì€-ì ">2.2 í•œêµ­ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì§€ ì•Šì€ ì </a></li> <li><a href="#23-ëª¨ë¸ì˜-í¬ê¸°">2.3 ëª¨ë¸ì˜ í¬ê¸°</a></li> </ul> </li> <li><a href="#3-methodology">3. Methodology</a> <ul> <li><a href="#31-subcharacter-text-representation">3.1 Subcharacter Text Representation</a></li> <li><a href="#32-subword-vocabulary">3.2 Subword Vocabulary</a></li> </ul> </li> <li><a href="#4-results">4. Results</a></li> <li><a href="#5-related-work--benchmarks">5. Related Work / Benchmarks</a></li> </ul> <hr/> <p><br/></p> <h3 id="1-ë…¼ë¬¸-ìš”ì•½">1. ë…¼ë¬¸ ìš”ì•½</h3> <p><br/></p> <h4 id="11-rationale">1.1 Rationale</h4> <p>Multilingual-BERT(ì´í•˜ mBERT)ëŠ” ìœ„í‚¤ë°±ê³¼ì˜ 104ê°œ ì–¸ì–´ë¡œ ëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜, ëª¨ë“  ì–¸ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì— ë¹„ì˜ì–´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…(non-English downstream tasks)ì—ëŠ” ì •í™•ë„ê°€ ë‹¤ì†Œ ë–¨ì–´ì§„ ëª¨ìŠµì„ ë³´ì¸ë‹¤. <strong>Downstream tasks</strong>ë€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ìˆ˜í–‰í•˜ëŠ” íŠ¹ì • ì‘ìš© ì‘ì—… (ì˜ˆ: ê°ì • ë¶„ì„, ê°œì²´ëª… ì¸ì‹, ë¬¸ì¥ ë¶„ë¥˜ ë“±)ì„ ì˜ë¯¸í•œë‹¤.</p> <p>í•œêµ­ì–´ëŠ” ë§Œê°œ ì´ìƒì˜ ë¬¸ì(character)ë¥¼ ì‚¬ìš©í•˜ê³ , ë…ì–´ë‚˜ ë¶ˆì–´ì™€ ê°™ì€ êµ´ì ˆì–´ë³´ë‹¤ í˜•íƒœì ìœ¼ë¡œ ë³µì¡í•˜ë‹¤. mBERTì—ì„œëŠ” ì´ ê°€ìš´ë° ì˜¤ì§ 1,187ê°œì˜ ë¬¸ìë§Œì´ í¬í•¨ë˜ì—ˆë‹¤. ë˜í•œ, mBERT ëª¨ë¸ì€ 104ê°œì˜ ì–¸ì–´ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê¸°ì— í¬ê¸°ê°€ ê³¼ë„í•˜ê²Œ í¬ë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. ê·¸ë˜ì„œ, ALBERTë‚˜ DistilBERTì²˜ëŸ¼ ëª¨ë¸ì„ ì¶•ì†Œí•˜ë©´ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•˜ë‹¤.</p> <p>ìš”ì•½í•˜ìë©´:</p> <ol> <li>mBERTëŠ” ëª¨ë¸ í¬ê¸°ê°€ ê³¼ë„í•˜ê²Œ í¬ë‹¤.</li> <li>mBERTëŠ” non-English downstream tasksì—ì„œ ì„±ëŠ¥ì´ ë‹¤ì†Œ ë–¨ì–´ì§„ë‹¤.</li> <li>ëª¨ë¸ì„ ì¶•ì†Œí•˜ë©´ì„œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤.</li> </ol> <p>KR-BERTëŠ” ì´ëŸ° ì ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ìƒˆë¡œìš´ í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ì´ë‹¤.</p> <p><br/></p> <h4 id="12-methodology-ìš”ì•½">1.2 Methodology ìš”ì•½</h4> <p><br/></p> <h4 id="13-conclusion">1.3 Conclusion</h4> <ul> <li>KR-BERTëŠ” ë‹¤ìŒì˜ downstream tasksë“¤ì—ì„œ mBERTì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í–ˆë‹¤. <ul> <li>senti- ment analysis</li> <li>Question-Answering</li> <li>Named Entity Recognition(NER)</li> <li>Paraphrase Detection</li> </ul> </li> <li>KR-BERTëŠ” ë‹¤ë¥¸ í•œêµ­ì–´ ëª¨ë¸ë“¤ì— ë¹„í•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê±°ë‚˜ ë¹„ìŠ·í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.</li> <li>í˜•íƒœì†Œê°€ ë§ì€ í•œêµ­ì–´ì— ë§ê²Œ ì„œë¸Œ-ìºë¦­í„° ê¸°ë°˜ ëª¨ë¸ê³¼ Bidirectional-WordPiece í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì€ ìì›ìœ¼ë¡œë„ íš¨ê³¼ì ì¸ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.</li> </ul> <p><br/> <br/></p> <h3 id="2-mbertì˜-í•œê³„">2. mBERTì˜ í•œê³„</h3> <p><br/></p> <h5 id="21-corpusë§ë­‰ì¹˜-domainì˜-í•œê³„">2.1 Corpus(ë§ë­‰ì¹˜) Domainì˜ í•œê³„</h5> <p>GCamemBERTì™€ ê°™ì€ ì–¸ì–´ë³„ íŠ¹í™”ëœ(Language Specific) BERT ëª¨ë¸ë“¤ì€ ë²•ë¥  ë°ì´í„°, ë‰´ìŠ¤ ê¸°ì‚¬ ë“±, ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ ë°˜ë©´, mBERTëŠ” ìœ ì €ë“¤ì´ ì§ì ‘ ì‘ì„±í•œ ë°ì´í„° ì†ŒìŠ¤(ë¸”ë¡œê·¸ ê¸€, ëŒ“ê¸€, ë“±)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ìœ„í‚¤ë°±ê³¼ í¬ìŠ¤íŠ¸ë“¤ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ í•™ìŠµë˜ì–´ ì–¸ì–´ ì‚¬ìš©(limited in its domain with respect to language usage - ì§ì—­í•˜ë©´ ì–¸ì–´ ì‚¬ìš©ì´ì§€ë§Œ ì–´íœ˜ë ¥ì¼ ê²ƒì´ë¼ê³  ìƒê°ë¨) ì¸¡ë©´ì—ì„œ í•œê³„ê°€ ì¡´ì¬í•œë‹¤.</p> <p><br/></p> <h5 id="22-í•œêµ­ì–´ì˜-ì–¸ì–´ì -íŠ¹ì„±ì„-ì¶©ë¶„íˆ-ê³ ë ¤í•˜ì§€-ì•Šì€-ì ">2.2 í•œêµ­ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì§€ ì•Šì€ ì </h5> <p>2.2.1 Rare Character Problem</p> <p>ë¼í‹´ ë¬¸ì ê¸°ë°˜ ì–¸ì–´ëŠ” ë‹¨ì–´ë¥¼ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, í•œêµ­ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ìƒ ìŒì ˆ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë˜ì–´ Out-of-Vocabulary(OoV)ê°€ ë¼í‹´ ë¬¸ì ê¸°ë°˜ ì–¸ì–´ë³´ë‹¤ ë§ì„ìˆ˜ë°–ì— ì—†ë‹¤. ê·¸ëŸ¼ìœ¼ë¡œ, ì´ëŸ° í•œêµ­ì–´ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ìƒˆë¡œìš´ BERT Vocabularyê°€ í•„ìš”í•˜ë‹¤.</p> <p>2.2.2 êµì°©ì–¸ì–´ì— ì í•©í•˜ì§€ ì•Šì€ ëª¨ë¸</p> <p>êµì°©ì–´(agglutinative language)ë€ ë¬¸ë²•ì  ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì£¼ë¡œ ì ‘ì‚¬ë¥¼ ë‹¨ë‹¨íˆ ê²°í•©í•˜ëŠ” ì–¸ì–´ë¥¼ ì˜ë¯¸í•œë‹¤. êµì°©ì–´ëŠ” í˜•íƒœë¡ ì ì¸ ë³µì¡ì„±(morphological complexity) ë•Œë¬¸ì— vocabularyë¥¼ í‘œí˜„í•˜ê¸° ì–´ë µë‹¤. í•œêµ­ì–´ ì—­ì‹œ êµì°©ì–¸ì–´ì— í•´ë‹¹í•˜ë©°, mBERTëŠ” ì´ ë¬¸ì œë¥¼ ì œëŒ€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.</p> <p>2.2.3 Lack of Meaningful Tokens</p> <p>ë…ì¼ì–´ì˜ ê²½ìš°, mBERT ëª¨ë¸ì˜ ì–´íœ˜ì— ëª…í™•í•œ semantic meaningì´ ì—†ëŠ” subword unitì´ í¬í•¨ë˜ì–´ ìˆì—ˆë‹¤. í•œêµ­ì–´ì—ì„œë„ ì´ì™€ ìœ ì‚¬í•œ ë¬¸ì œê°€ ì¡´ì¬í–ˆëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ê°€ í˜•íƒœì†Œì™€ ê°™ì€ ë‹¨ìœ„ê°€ ì•„ë‹Œ ë‹¨ì¼ ë¬¸ìë¡œ tokenized ë˜ì—ˆê¸° ë–„ë¬¸ì´ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, í•œêµ­ì–´ í…ìŠ¤íŠ¸ì™€ í•œêµ­ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ê³ ë ¤í•œ vocabulary ë° tokenizerë¥¼ êµ¬í˜„í–ˆë‹¤.</p> <p><br/></p> <h5 id="23-ëª¨ë¸ì˜-í¬ê¸°">2.3 ëª¨ë¸ì˜ í¬ê¸°</h5> <p>ëŒ€í˜• ëª¨ë¸ë“¤ì€ ë§ì€ ì–‘ì˜ dataset, parameter, voabularyë¥¼ í•„ìš”ë¡œ í•œë‹¤. mBERTëŠ” 167M, RoBERTaëŠ” 355M íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•´ ìì›ì˜ ì œì•½ì„ ë°›ê¸° ë§ˆë ¨ì¸ë°, KR-BERTëŠ” ì ì€ parameterì™€ í›ˆë ¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ mBERTì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì—ˆë‹¤.</p> <p><br/> <br/></p> <h3 id="3-methodology">3. Methodology</h3> <p>KR-BERTë¥¼ Multilingual BERT, Kor- BERT, ê·¸ë¦¬ê³  KoBERTì™€ ë¹„êµí–ˆë‹¤.</p> <p><br/></p> <h4 id="31-subcharacter-text-representation">3.1 Subcharacter Text Representation</h4> <p>í•œêµ­ì–´ í…ìŠ¤íŠ¸ëŠ” Text-&gt; í•œê¸€ -&gt; graphemesì˜ í˜•íƒœë¡œ ë¶„í•´ ê°€ëŠ¥í•˜ë‹¤. ì´ëŸ° íŠ¹ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´, KR-BERTëŠ” syllable characterì™€ sub-character ë‘ ê°€ì§€ ë§ë­‰ì¹˜ í‘œí˜„ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ new vocabularyì™€ BERT modelì„ í•™ìŠµì‹œì¼°ë‹¤. BPE ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ tokenizationì´ ê°€ëŠ¥í•´ì§„ë‹¤.</p> <p><code class="language-plaintext highlighter-rouge">Example : ëœ€(ttwim, "jumping")ì€ ã„¸ã…Ÿ(ttwi, "jump")ì™€ á„†(m, "-ing")ìœ¼ë¡œ ë¶„í•´ë  ìˆ˜ ìˆë‹¤</code></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/15-480.webp 480w,/assets/img/15-800.webp 800w,/assets/img/15-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>ê¸°ì¡´ mBERTëŠ”</p> <p>KR-BERT ëª¨ë¸ì€ ë‚˜ë¨¸ì§€ ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, ìŒì ˆ ë¬¸ì ì´ì™¸ì—ë„ í•˜ìœ„ ë¬¸ì í‘œí˜„ì„ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ í•œêµ­ì–´ ë™ì‚¬/í˜•ìš©ì‚¬ì˜ ê³µí†µëœ íŠ¹ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆë‹¤.</p> <p><br/></p> <h4 id="32-subword-vocabulary">3.2 Subword Vocabulary</h4> <h3 id="4-results">4. Results</h3> <h3 id="5-related-work--benchmarks">5. Related Work / Benchmarks</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/13-480.webp 480w,/assets/img/13-800.webp 800w,/assets/img/13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/14-480.webp 480w,/assets/img/14-800.webp 800w,/assets/img/14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="study"/><category term="papers"/><category term="NLP"/><summary type="html"><![CDATA[1. ë…¼ë¬¸ ìš”ì•½ 1.1 Rationale 1.2 Methodology ìš”ì•½ 1.3 Conclusion 2. mBERTì˜ í•œê³„ 2.1 Corpus(ë§ë­‰ì¹˜) Domainì˜ í•œê³„ 2.2 í•œêµ­ì–´ì˜ ì–¸ì–´ì  íŠ¹ì„±ì„ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì§€ ì•Šì€ ì  2.3 ëª¨ë¸ì˜ í¬ê¸° 3. Methodology 3.1 Subcharacter Text Representation 3.2 Subword Vocabulary 4. Results 5. Related Work / Benchmarks]]></summary></entry><entry><title type="html">(ENG) MLP and DNN</title><link href="https://joohunhyun.github.io/blog/2024/MLP-DNN/" rel="alternate" type="text/html" title="(ENG) MLP and DNN"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/MLP-DNN</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/MLP-DNN/"><![CDATA[<ul> <li>Perceptron -&gt; delta rule</li> <li>MLP (2~3 layers) -&gt; backpropagation</li> <li>DNN (~10 layers) -&gt; using ReLU instead of sigmoid</li> </ul> <h4 id="mlps-problem">MLPâ€™s problem</h4> <ul> <li>Major issue of designing MLP is how many hidden units are optimal? <ul> <li>ë„ˆë¬´ ë§Œìœ¼ë©´ : overfit</li> <li>ë„ˆë¬´ ì ìœ¼ë©´ : underfit</li> </ul> </li> <li>As the number of hidden layers increase, the sigmoid functionâ€™s relatively small gradient repeatedly gets multiplied. -&gt; this makes the gradient close to 0, and the weights are not updated (<strong>vanishing gradient problem</strong>)</li> </ul> <p><strong>Solution to this problem</strong></p> <ol> <li>Using a ReLU Function</li> </ol> <p>The Rectified Linear Unit (ReLU) function is defined as:</p> \[f(x)=max(0,x)\] <p>ReLU has a gradient of 1 1 for positive inputs, which prevents the gradient from shrinking excessively as it is propagated through the network.</p> <p>Advantages of ReLU:</p> <ul> <li>Avoids vanishing gradients: The gradient remains constant for positive inputs, ensuring that weights continue to be updated.</li> <li>Computational efficiency: ReLU is simpler to compute than sigmoid or tanh.</li> <li>Sparsity: It introduces sparsity in activations (many outputs are zero), which can improve generalization.</li> </ul> <p>However, ReLU can suffer from the dying ReLU problem, where neurons become inactive (outputting zero) due to large negative gradients. Variants like Leaky ReLU and Parametric ReLU address this issue.</p> <ol> <li>Xavier Initialization</li> </ol> <p>Proper weight initialization is crucial for mitigating vanishing or exploding gradients. The Xavier initialization (or Glorot initialization) ensures that the variance of activations and gradients is maintained across layers.</p> <p>Benefits:</p> <ul> <li>Prevents vanishing/exploding gradients by keeping the variance of inputs and outputs consistent across layers.</li> <li>Helps the network converge faster.</li> </ul> <ol> <li>Batch Normalization</li> </ol> <p>Batch Normalization (BatchNorm) normalizes the inputs to each layer by adjusting the mean and variance of the activations during training.</p> <p>Benefits</p> <ul> <li>Improves gradient flow: Normalization reduces internal covariate shift, allowing deeper networks to train effectively.</li> <li>Stabilizes learning: It reduces sensitivity to initialization and learning rate.</li> <li>Acts as regularization: BatchNorm has a slight regularization effect, reducing the need for dropout in some cases.</li> </ul> <ol> <li>Early stopping</li> </ol> <p>Terminating training once a performance plateau has been reached.</p> <p>Example performance plateus</p> <ul> <li>Error is small enough</li> <li>Number of epochs</li> <li>Pruning the network</li> <li>Training with noisy samples</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Perceptron -&gt; delta rule MLP (2~3 layers) -&gt; backpropagation DNN (~10 layers) -&gt; using ReLU instead of sigmoid]]></summary></entry><entry><title type="html">(ENG) RNN and LSTM</title><link href="https://joohunhyun.github.io/blog/2024/RNN/" rel="alternate" type="text/html" title="(ENG) RNN and LSTM"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/RNN</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/RNN/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></li> <li><a href="#exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</a></li> <li><a href="#long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</a> <ul> <li><a href="#lstm-cell-structure">LSTM Cell Structure</a></li> <li><a href="#differences-between-rnn-and-lstm">Differences Between RNN and LSTM</a></li> </ul> </li> <li><a href="#tldr">TL;DR</a></li> </ul> <hr/> <h4 id="recurrent-neural-networks">Recurrent Neural Networks</h4> <p>Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining an internal state that captures information about previous time steps. This makes them ideal for tasks like speech recognition, machine translation, and time series analysis.</p> <ul> <li><strong>Structure</strong>: In an RNN, each hidden unit has connections to itself and to other hidden units, allowing information to flow across time steps.</li> <li><strong>Mathematics</strong>: At each time step \(t\), the hidden state \(h_t\) is updated using the input \(x_t\) and the previous hidden state \(h_{t-1}\): \(h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\) Where: <ul> <li>\(W_{xh}\): Weight matrix for the input.</li> <li>\(W_{hh}\): Weight matrix for the hidden state.</li> <li>\(b_h\): Bias term.</li> </ul> </li> </ul> <h4 id="exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</h4> <p>RNNs use the same weight matrices \(W_{hh}\) and \(W_{xh}\) at every time step. During backpropagation through time (BPTT), gradients are repeatedly multiplied by these matrices, which can cause:</p> <ul> <li><strong>Vanishing gradients</strong>: Gradients shrink exponentially, making it difficult to update weights and learn long-term dependencies.</li> <li><strong>Exploding gradients</strong>: Gradients grow exponentially, leading to instability.</li> </ul> <h4 id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h4> <p><strong>Long Short-Term Memory (LSTM)</strong> networks are a type of RNN specifically designed to handle the vanishing gradient problem. They achieve this by introducing a more complex structure within each unit that allows information to be selectively remembered or forgotten over long time steps.</p> <h5 id="lstm-cell-structure">LSTM Cell Structure</h5> <p>An LSTM unit consists of four interacting layers that control the flow of information:</p> <ol> <li> <p><strong>Forget Gate</strong>: Determines which parts of the previous cell state \(C_{t-1}\) should be forgotten: \(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\) Where \(\sigma\) is the sigmoid activation function.</p> </li> <li> <p><strong>Input Gate</strong>: Decides what new information to store in the cell state: \(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\) A candidate cell state \(\tilde{C}_t\) is created: \(\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\)</p> </li> <li> <p><strong>Cell State Update</strong>: Combines the forget gate and the input gate to update the cell state: \(C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t\) Where \(\odot\) denotes element-wise multiplication.</p> </li> <li> <p><strong>Output Gate</strong>: Determines the next hidden state \(h_t\): \(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\) The hidden state is calculated as: \(h_t = o_t \odot \tanh(C_t)\)</p> </li> </ol> <hr/> <h5 id="differences-between-rnn-and-lstm">Differences Between RNN and LSTM</h5> <table> <thead> <tr> <th>Feature</th> <th>RNN</th> <th>LSTM</th> </tr> </thead> <tbody> <tr> <td><strong>Architecture</strong></td> <td>Simple structure with one update equation.</td> <td>Complex structure with gates for controlling memory.</td> </tr> <tr> <td><strong>Gradient Issues</strong></td> <td>Suffers from vanishing/exploding gradients.</td> <td>Designed to avoid vanishing gradients.</td> </tr> <tr> <td><strong>Long-Term Dependencies</strong></td> <td>Struggles to learn long-term patterns.</td> <td>Efficiently captures long-term dependencies.</td> </tr> <tr> <td><strong>Use Cases</strong></td> <td>Short-term dependencies.</td> <td>Long-term dependencies (e.g., speech, text).</td> </tr> </tbody> </table> <hr/> <h4 id="tldr">TL;DR</h4> <p>While RNNs are effective for sequence modeling, their training can be hindered by the vanishing gradient problem, especially for long-term dependencies. LSTMs address this limitation by introducing gating mechanisms that allow the model to selectively retain and forget information, making them a powerful tool for tasks involving sequential data.</p>]]></content><author><name></name></author><category term="study"/><category term="DL"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(ENG) Training and Evaluating Deep Networks</title><link href="https://joohunhyun.github.io/blog/2024/deep-learning/" rel="alternate" type="text/html" title="(ENG) Training and Evaluating Deep Networks"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/deep-learning</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/deep-learning/"><![CDATA[<h4 id="training-and-evaluating-deep-networks">Training and Evaluating Deep Networks</h4> <p>When dealing with DL, the following components are vital</p> <ol> <li>training set</li> <li>test set</li> <li>validation set : used to tune a modelâ€™s hyperparameters for model selection. Also, it prevents overfitting by early stopping</li> </ol> <h5 id="hyperparameter-tuning">Hyperparameter Tuning</h5> <p>Hyperparameters in DL are usually tuned heuristically by hand or using grid search.</p> <h5 id="dropout">Dropout</h5> <p>Dropout is a form of regualization that randomly deletes units and their connections during training.</p> <p>Pros</p> <ul> <li>ê³¼ì í•© ë°©ì§€, ë‰´ëŸ°ê°„ì˜ ì˜ì¡´ì„±ì„ ì¤„ì—¬ í•™ìŠµëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ</li> <li>ê°„ë‹¨í•œ êµ¬í˜„</li> </ul> <p>Cons</p> <ul> <li>Additional overhead : í›ˆë ¨ ì¤‘ì— ë§¤ë²ˆ maskë¥¼ ìƒì„±í•˜ê³  ì ìš©</li> <li>í•™ìŠµ ì†ë„ ê°ì†Œ</li> </ul> <h5 id="unsupervised-pretraining">Unsupervised Pretraining</h5> <p>This method can be useful when the volume of labeled data is small relative to the modelâ€™s capacity.</p> <ul> <li>ë”¥ëŸ¬ë‹ ì´ˆê¸°ì˜ í•™ìŠµ ì•ˆì •í™”ì™€ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ê³ ì•ˆëœ ê¸°ë²•</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="DL"/><summary type="html"><![CDATA[Training and Evaluating Deep Networks]]></summary></entry><entry><title type="html">(ENG) Feature Selection, Extraction, and Ensamble Methods</title><link href="https://joohunhyun.github.io/blog/2024/feature/" rel="alternate" type="text/html" title="(ENG) Feature Selection, Extraction, and Ensamble Methods"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/feature</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/feature/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="feature-subset-selection">Feature Subset Selection</h4> <p>Definition : Feature subset selection finds a subset of original features without new features</p> <h5 id="1-filter-method">1. Filter Method</h5> <ul> <li>Evaluates features with performance measures based on information gain or distance between each attribute in the class</li> <li>ì¥ì  : computationally siple and fast</li> <li>ë‹¨ì  : slow searching</li> <li>í•´ê²°ì±… : use a single-attribute evaluator with ranking</li> </ul> <h5 id="2-wrapper-method">2. Wrapper Method</h5> <ul> <li>Searches for best combination of features among <strong>all</strong> posttible combination</li> <li>ì¥ì  : generally better performance, simple and direct</li> <li>ë‹¨ì  : \(O(n^2)\) time complexity - slow</li> <li>í•´ê²°ì±… <ul> <li>eliminate irrelevant features</li> <li>eliminate redundant attributes</li> </ul> </li> </ul> <h5 id="3-embedded-method">3. Embedded method</h5> <ul> <li>Uses ML models for classification and then an obtimal subset of features/ranking of feature is built by the classifier algorithm</li> </ul> <h5 id="search-methods">Search Methods</h5> <ul> <li>Weka ì˜ˆì œì—ì„œëŠ” classfier(wrapper, etc)ì™€ search method ë‘˜ ë‹¤ ì •ì˜í•´ì•¼í•œë‹¤.</li> </ul> <h5 id="different-search-methods">Different Search Methods</h5> <ol> <li>Exhaustive : \(2^n\) subsets</li> <li>Backwards</li> <li>Forwards</li> <li>Bidirectional</li> </ol> <h2><br/></h2> <h4 id="feature-extraction-aka-reducing-dimentionality">Feature Extraction (AKA: Reducing Dimentionality)</h4> <p>Definition : extracts new set of features from the original features</p> <h5 id="pca">PCA</h5> <ul> <li>Definition : unsupervised approach to examine relations among a set of variables</li> <li>When to use this? <ul> <li>PCA is useful in reducing the dimentionality of 3-D, 4-D scatterplots (ì™œ ì‚¬ìš©í•˜ëƒ : visually difficult to interpret data points in high-dimensional space)</li> </ul> </li> <li>Ex : Compressing MNIST- dataset using PCA (to reduce dimension)</li> </ul> <h2 id="-1"><br/></h2> <h4 id="ensamble-method">Ensamble Method</h4> <ul> <li>Definition : ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ë“¤ì„ ì¡°í•©í•´ì„œ ë”ìš± ì •í™•í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ë°©ë²• (ìœ„ì›íšŒ ìš´ì˜ê³¼ ë¹„ìŠ·)</li> </ul> <p>Types of Ensamble Methods:</p> <ol> <li>Boosting</li> <li>Bagging</li> <li>Stacking</li> </ol> <h4 id="1-boosting-adaboost"><strong>1. Boosting: AdaBoost</strong></h4> <p>Boosting is an ensemble learning technique that combines the predictions of several weak learners (simple models, such as decision trees) to create a strong learner. <strong>AdaBoost (Adaptive Boosting)</strong> is one of the most popular boosting algorithms.</p> <h5 id="how-adaboost-works"><strong>How AdaBoost Works</strong></h5> <p>AdaBoost works iteratively, focusing on the data points that are harder to classify by assigning them higher weights in subsequent iterations.</p> <ol> <li><strong>Initialize Weights</strong>: <ul> <li>Assign equal weights to all training samples: \(w_i = \frac{1}{N}, \, \forall i \in \{1, 2, \dots, N\}\) Where \(N\) is the total number of training samples.</li> </ul> </li> <li><strong>Train Weak Learner</strong>: <ul> <li>Train a weak learner (e.g., a shallow decision tree) using the weighted training dataset.</li> </ul> </li> <li><strong>Calculate Weighted Error</strong>: <ul> <li>Compute the error \(e_t\) of the weak learner: \(e_t = \frac{\sum_{i=1}^N w_i \cdot I(y_i \neq \hat{y}_i)}{\sum_{i=1}^N w_i}\) Where: <ul> <li>\(y_i\): True label of sample \(i\).</li> <li>\(\hat{y}_i\): Predicted label by the weak learner.</li> <li>\(I(y_i \neq \hat{y}_i)\): Indicator function that equals 1 if the prediction is incorrect.</li> </ul> </li> </ul> </li> <li><strong>Compute Alpha (Model Weight)</strong>: <ul> <li>Calculate the weight of the weak learner: \(\alpha_t = \frac{1}{2} \ln\left(\frac{1 - e_t}{e_t}\right)\) The higher the accuracy of the weak learner, the larger its contribution to the final model.</li> </ul> </li> <li><strong>Update Sample Weights</strong>: <ul> <li>Increase the weights of misclassified samples to emphasize them in the next iteration: \(w_i \leftarrow w_i \cdot \exp(\alpha_t \cdot I(y_i \neq \hat{y}_i))\)</li> <li>Normalize the weights so they sum to 1: \(w_i \leftarrow \frac{w_i}{\sum_{j=1}^N w_j}\)</li> </ul> </li> <li><strong>Repeat</strong>: <ul> <li>Train the next weak learner using the updated weights, and repeat for \(T\) iterations.</li> </ul> </li> <li><strong>Final Prediction</strong>: <ul> <li>Combine the predictions of all weak learners using their weights \(\alpha_t\): \(H(x) = \text{sign} \left( \sum_{t=1}^T \alpha_t \cdot h_t(x) \right)\) Where: <ul> <li>\(h_t(x)\): Prediction from the \(t\)-th weak learner.</li> </ul> </li> </ul> </li> </ol> <hr/> <h5 id="advantages-of-adaboost"><strong>Advantages of AdaBoost</strong></h5> <ul> <li>Improves accuracy by focusing on difficult samples.</li> <li>Works well with simple weak learners.</li> <li>Robust to overfitting in many cases.</li> </ul> <h5 id="disadvantages-of-adaboost"><strong>Disadvantages of AdaBoost</strong></h5> <ul> <li>Sensitive to noisy data and outliers, as they receive higher weights.</li> <li>Can be computationally expensive for large datasets.</li> </ul> <h5 id="2-baggingstands-for-boostrap-aggrevating">2. Bagging(stands for: Boostrap Aggrevating)</h5> <p>Baggingì€ ì•™ìƒë¸” ê¸°ë²•ë“¤ ê°€ìš´ë° ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì´ë‹¤(taking a vote, or weighted vote). Decision Treeì˜ ê²½ìš°, training setì— ë”°ë¼ì„œ ê·¸ êµ¬ì¡°ê°€ ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°”ë€ë‹¤. ì´ëŸ° ê²½ìš°ì—ëŠ” 1) training setì„ ì—¬ëŸ¬ ê°œ ë§Œë“¤ê³  2) ê°ê° ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ 3) votingì„ í•œë‹¤. In numerical prediction, taking the average of all predictions.</p> <p>Breiman(1996) noticed that an ensamble of trees improved when the trees differed significantly from each other, today called <strong>Random Forest</strong>. Bagging was able to create a diverse ensamble of classifiers by introducing randomness into the learning algâ€™s input, which resulted in <strong>better results</strong>. In a DT, picking the best option can be randomized by picking one of the N best options at random instead of a signle winner (ex: top 3 options, top 5 options)</p> <p><strong>Methods of picking different â€œwinnersâ€ in bagging</strong></p> <ol> <li>elite : picking the best one</li> <li>randomization : picking 1 randomly from top 5 candidates</li> <li>stochastic : top5 ì¤‘ì—ì„œ ë£°ë ›ìœ¼ë¡œ ê³ ë¥´ëŠ” ë°©ë²• (weighted)</li> </ol> <h5 id="3-stacking">3. Stacking</h5> <p>Stacking is used on models built by different learning algorithsm. For example, stacking can be used when you want to form a classifier for a given dataset with A) deicision tree inducer B) naive bayes learnner and C) instance based learning scheme.</p> <p>Stacking involves a <strong>metalearner</strong> that replaces the voting procedure of boosing and bagging. Metalearner is used to discover how to best combine the output of the base learners to determine which classifiers are the reliable ones.</p> <ul> <li>base learners : level-0 models</li> <li>meta learner : level-1 model</li> <li>predictions from base models are inputs to the meta learner. (funnelì²˜ëŸ¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‚´ë ¤ì˜¨ë‹¤)</li> </ul> <p>TL;DR : Stacking combines predictions of base learners using metalearner(NOT voting)</p>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(KOR) ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë°°ì†í•˜ëŠ” ë°©ë²•</title><link href="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/" rel="alternate" type="text/html" title="(KOR) ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë°°ì†í•˜ëŠ” ë°©ë²•"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/skip-safety-skip%20copy</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/"><![CDATA[<p>24â€™12ì›” ê¸°ì¤€, ì´ ë°©ë²•ì´ ì œì¼ í¸í•œ ë“¯. ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ë‹¤ ë§‰íŒ ë“¯ í•˜ë‹¤.</p> <p>ì°¸ê³  :</p> <ul> <li>ì¤‘ê°„ì¤‘ê°„ì— í€´ì¦ˆë„ ìˆìœ¼ë‹ˆ ìœ ì˜í•´ì•¼í•œë‹¤.</li> <li>ë°°ì†ì€ 16ë°°ê¹Œì§€ë§Œ ê°€ëŠ¥í•˜ë‹¤.</li> </ul> <p><strong>í¬ë¡¬</strong> í™˜ê²½ì—ì„œ ì—°êµ¬ì‹¤ ì•ˆì „êµìœ¡ ë™ì˜ìƒ ì¬ìƒ í›„ ê°œë°œìëª¨ë“œ ì½˜ì†” <code class="language-plaintext highlighter-rouge">F12</code>ì— í•´ë‹¹ ì½”ë“œ ë¶™ì—¬ë„£ê¸°:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>document.querySelector('video').playbackRate = 16;
</code></pre></div></div>]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[24â€™12ì›” ê¸°ì¤€, ì´ ë°©ë²•ì´ ì œì¼ í¸í•œ ë“¯. ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ë‹¤ ë§‰íŒ ë“¯ í•˜ë‹¤.]]></summary></entry><entry><title type="html">(ENG) SVM, SVC</title><link href="https://joohunhyun.github.io/blog/2024/SVM-SVC/" rel="alternate" type="text/html" title="(ENG) SVM, SVC"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/SVM-SVC</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/SVM-SVC/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="classification-models">Classification Models</h4> <p>Definition : ë‘ê°œì˜ class A and Bë¥¼ feature spaceì—ì„œ ë¶„ë¦¬í•˜ëŠ” ëª¨ë¸</p> <ul> <li>Decision Trees</li> <li>Probabilistic models</li> <li>Linear models</li> <li>Non-linear models</li> </ul> <p>In the case of non-linear data, there are different methods to model this.</p> <ol> <li>MLP : achieve non-linearlity by combining perceptrons(linear models)</li> <li>SVM : supervised machine learning algorithm used for classification and regression tasks.</li> <li>SVC :</li> </ol> <h5 id="svm">SVM</h5> <p>Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. The key idea is to find the best hyperplane that separates data points into different classes. In an ideal case, this hyperplane maximizes the margin between data points of different classes.</p> <ul> <li>Core Concepts: <ul> <li>The hyperplane is the decision boundary.</li> <li>SVM focuses on data points (support vectors) near the decision boundary that influence its position.</li> <li>The algorithm uses kernels to transform data into higher-dimensional spaces when it is not linearly separable in the original space.</li> </ul> </li> <li>Applications: Image classification, text categorization, and other binary/multi-class problems.</li> <li>Pros: <ul> <li>Effective for high-dimensional spaces.</li> <li>Works well with a clear margin of separation.</li> </ul> </li> <li>Cons: <ul> <li>Computationally expensive for large datasets.</li> <li>Sensitive to the choice of kernel and hyperparameters.</li> </ul> </li> </ul> <h6 id="rbf-kernel">RBF Kernel</h6> <p>The RBF kernel, also known as the Gaussian kernel, is one of the most commonly used kernels in SVM. It is a non-linear kernel that maps data points into a higher-dimensional space where a linear hyperplane can effectively separate them.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: It can model complex relationships in the data.</li> <li>Local influence: Points that are closer to each other have a higher similarity, and their influence decreases exponentially with distance</li> <li> <p>Scalability: Effective for problems where the decision boundary is not linear.</p> </li> <li>Pros: <ul> <li>Handles non-linearly separable data well.</li> <li>Requires fewer hyperparameters compared to other kernels.</li> </ul> </li> <li>Cons: <ul> <li>Î³ must be carefully tuned; poor tuning can lead to overfitting or underfitting.</li> </ul> </li> </ul> <h6 id="polynomial-kernel">Polynomial Kernel</h6> <p>The Polynomial kernel is another non-linear kernel used in SVM. It computes the similarity between two data points as a polynomial function of their inner product in the input space.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: Maps the input space to a higher-dimensional space defined by polynomial terms.</li> <li>Flexibility: The degree \(d\) controls the complexity of the decision boundary.</li> <li>Global influence: Unlike the RBF kernel, it considers global features of the data.</li> <li>Pros: <ul> <li>Can model non-linear relationships with adjustable complexity via \(d\)</li> <li>Effective for datasets where classes are distinguishable by polynomial decision boundaries.</li> </ul> </li> <li>Cons: <ul> <li>More sensitive to feature scaling than the RBF kernel.</li> <li>Higher degrees can lead to overfitting.</li> </ul> </li> </ul> <h5 id="svc">SVC</h5> <ul> <li> <p>SVC stands for Support Vector Classifier and is the implementation of SVM for classification tasks in libraries like scikit-learn.</p> </li> <li>Key Differences Between SVM and SVC: SVM is the broader concept that encompasses both classification and regression tasks (e.g., SVM for regression is called SVR).</li> <li>SVC is specifically the classification implementation of SVM in scikit-learn.</li> <li>Features of SVC: <ul> <li>It supports linear and non-linear kernels like polynomial, RBF (Radial Basis Function), and sigmoid.</li> <li>Provides hyperparameters like C (regularization strength) and gamma (kernel coefficient) for tuning the model.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(KOR) VsCode Shortcut Keys(MacOS)</title><link href="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/" rel="alternate" type="text/html" title="(KOR) VsCode Shortcut Keys(MacOS)"/><published>2024-12-13T00:00:00+00:00</published><updated>2024-12-13T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/"><![CDATA[<p>ğŸ’¡ìµìˆ™í•´ì§„ ë‹¨ì¶•í‚¤ëŠ” ë¬¸ì„œì—ì„œ ì‚­ì œ</p> <p>ğŸ’¡ì›ë³¸ì€ .md íŒŒì¼ì— ì¡´ì•ˆ</p> <hr/> <p><strong>Table of contents</strong></p> <ul> <li><a href="#vscode-shortcuts">VScode Shortcuts</a></li> <li><a href="#vscode-git-status">VSCode git status</a></li> </ul> <p><br/></p> <h4 id="vscode-shortcuts">VScode Shortcuts</h4> <table> <thead> <tr> <th>Action</th> <th>Shortcut</th> </tr> </thead> <tbody> <tr> <td><strong>General</strong></td> <td>Â </td> </tr> <tr> <td>Open Command Palette</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + P</code></td> </tr> <tr> <td>Open Settings</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + ,</code></td> </tr> <tr> <td>Open Keyboard Shortcuts</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + K âŒ˜ + S</code></td> </tr> <tr> <td>Open Extensions View</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + X</code></td> </tr> <tr> <td>Show Integrated Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + \</code></td> </tr> <tr> <td>Close Window</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + W</code></td> </tr> <tr> <td>Quit VSCode</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + Q</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>GitHub Copilot</strong></td> <td>Â </td> </tr> <tr> <td>Accept Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Tab</code></td> </tr> <tr> <td>Next Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + ]</code></td> </tr> <tr> <td>Previous Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + [</code></td> </tr> <tr> <td>Show Suggestions</td> <td><code class="language-plaintext highlighter-rouge">Ctrl + Enter</code></td> </tr> <tr> <td>Approve Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Cmd + â†</code> or <code class="language-plaintext highlighter-rouge">Cmd + â†’</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>File Management</strong></td> <td>Â </td> </tr> <tr> <td>Open File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + O</code></td> </tr> <tr> <td>Save File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + S</code></td> </tr> <tr> <td>Save All Files</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + âŒ˜ + S</code></td> </tr> <tr> <td>Close Editor</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + W</code></td> </tr> <tr> <td>Reopen Closed Editor</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + T</code></td> </tr> <tr> <td>New File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + N</code></td> </tr> <tr> <td>Open Recent Files</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + R</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Editing</strong></td> <td>Â </td> </tr> <tr> <td>Cut Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + X</code></td> </tr> <tr> <td>Copy Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + C</code></td> </tr> <tr> <td>Paste</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + V</code></td> </tr> <tr> <td>Delete Line</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + K</code></td> </tr> <tr> <td>Duplicate Line</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†“</code> or <code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†‘</code></td> </tr> <tr> <td>Move Line Up/Down</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + â†‘</code> or <code class="language-plaintext highlighter-rouge">âŒ¥ + â†“</code></td> </tr> <tr> <td>Indent Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + ]</code></td> </tr> <tr> <td>Outdent Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + [</code></td> </tr> <tr> <td>Comment Line</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + /</code></td> </tr> <tr> <td>Add Multi-Cursor</td> <td><code class="language-plaintext highlighter-rouge">âŒ¥ + Click</code></td> </tr> <tr> <td>Select All Occurrences</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + â‡§ + L</code></td> </tr> <tr> <td>Expand Selection</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†’</code></td> </tr> <tr> <td>Shrink Selection</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + â†</code></td> </tr> <tr> <td>Format Document</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ¥ + F</code></td> </tr> <tr> <td>Go to Matching Bracket</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + \</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Navigation</strong></td> <td>Â </td> </tr> <tr> <td>Go to File</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + P</code></td> </tr> <tr> <td>Go to Line</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + G</code></td> </tr> <tr> <td>Go to Definition</td> <td><code class="language-plaintext highlighter-rouge">F12</code></td> </tr> <tr> <td>Go to Implementation</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + F12</code></td> </tr> <tr> <td>Show References</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F12</code></td> </tr> <tr> <td>Navigate Back</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + -</code></td> </tr> <tr> <td>Navigate Forward</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + â‡§ + -</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Search and Replace</strong></td> <td>Â </td> </tr> <tr> <td>Find</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + F</code></td> </tr> <tr> <td>Find in Files</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + F</code></td> </tr> <tr> <td>Replace</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + âŒ¥ + F</code></td> </tr> <tr> <td>Replace in Files</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + H</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Debugging</strong></td> <td>Â </td> </tr> <tr> <td>Start/Continue Debugging</td> <td><code class="language-plaintext highlighter-rouge">F5</code></td> </tr> <tr> <td>Step Over</td> <td><code class="language-plaintext highlighter-rouge">F10</code></td> </tr> <tr> <td>Step Into</td> <td><code class="language-plaintext highlighter-rouge">F11</code></td> </tr> <tr> <td>Step Out</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F11</code></td> </tr> <tr> <td>Restart Debugging</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + F5</code></td> </tr> <tr> <td>Stop Debugging</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + F5</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Terminal</strong></td> <td>Â </td> </tr> <tr> <td>Create New Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + â‡§ + </code>`</td> </tr> <tr> <td>Split Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + \</code>`</td> </tr> <tr> <td>Kill Terminal</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + K</code> (inside terminal)</td> </tr> <tr> <td>Navigate Terminals</td> <td><code class="language-plaintext highlighter-rouge">âŒƒ + â†</code> or <code class="language-plaintext highlighter-rouge">âŒƒ + â†’</code></td> </tr> <tr> <td>Â </td> <td>Â </td> </tr> <tr> <td><strong>Version Control</strong></td> <td>Â </td> </tr> <tr> <td>Open Source Control</td> <td><code class="language-plaintext highlighter-rouge">â‡§ + âŒ˜ + G</code></td> </tr> <tr> <td>Commit Changes</td> <td><code class="language-plaintext highlighter-rouge">âŒ˜ + Enter</code> (in source control)</td> </tr> </tbody> </table> <p><br/></p> <h4 id="vscode-git-status">VSCode git status</h4> <table> <thead> <tr> <th>Code</th> <th>Status</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>Added</td> <td>This is a new file that has been added to the repository</td> </tr> <tr> <td>M</td> <td>Modified</td> <td>An existing file has been changed</td> </tr> <tr> <td>D</td> <td>Deleted</td> <td>A file has been deleted</td> </tr> <tr> <td>U</td> <td>Untracked</td> <td>The file is new or has been changed but has not been added to the repository yet</td> </tr> <tr> <td>C</td> <td>Conflict</td> <td>There is a conflict in the file</td> </tr> <tr> <td>R</td> <td>Renamed</td> <td>The file has been renamed</td> </tr> <tr> <td>S</td> <td>Submodule</td> <td>In repository exists another subrepository</td> </tr> <tr> <td>T</td> <td>Typechange</td> <td>The file changed from symlink to regular file, or vice versa</td> </tr> </tbody> </table> ]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[ğŸ’¡ìµìˆ™í•´ì§„ ë‹¨ì¶•í‚¤ëŠ” ë¬¸ì„œì—ì„œ ì‚­ì œ]]></summary></entry></feed>