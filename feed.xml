<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://joohunhyun.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joohunhyun.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-30T04:08:33+00:00</updated><id>https://joohunhyun.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">(KOR) AI Agent란 무엇인가?</title><link href="https://joohunhyun.github.io/blog/2024/AI-Agents/" rel="alternate" type="text/html" title="(KOR) AI Agent란 무엇인가?"/><published>2024-12-29T00:00:00+00:00</published><updated>2024-12-29T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/AI%20Agents</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/AI-Agents/"><![CDATA[<ul> <li><a href="#1-ai-agents--introduction">1. AI Agents : Introduction</a></li> <li><a href="#2-ai-agent--methodology">2. AI Agent : Methodology</a> <ul> <li><a href="#21-동작-순서">2.1 동작 순서</a></li> <li><a href="#22-동작-원리">2.2 동작 원리</a></li> <li><a href="#23-상호작용">2.3 상호작용</a></li> <li><a href="#24-ai-agent-종류">2.4 AI Agent 종류</a> <ul> <li><a href="#241-단순-반응형-에이전트-reactive-agent">2.4.1 단순 반응형 에이전트 (Reactive Agent)</a></li> <li><a href="#242-상태-기반-에이전트-model-based-agent">2.4.2 상태 기반 에이전트 (Model-Based Agent)</a></li> <li><a href="#243-목표-지향형-에이전트-goal-based-agent">2.4.3 목표 지향형 에이전트 (Goal-Based Agent)</a></li> <li><a href="#244-유틸리티-기반-에이전트-utility-based-agent">2.4.4 유틸리티 기반 에이전트 (Utility-Based Agent)</a></li> <li><a href="#245-학습-에이전트-learning-agent">2.4.5 학습 에이전트 (Learning Agent)</a></li> <li><a href="#246-다중-에이전트-시스템-multi-agent-system">2.4.6 다중 에이전트 시스템 (Multi-Agent System)</a></li> <li><a href="#247-하이브리드-에이전트-hybrid-agent">2.4.7 하이브리드 에이전트 (Hybrid Agent)</a></li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="1-ai-agents--introduction">1. AI Agents : Introduction</h3> <p>AI agent는 사람의 개입없이 AI가 task 수행을 위한 모든 역할을 수행하는 agent를 의미한다. AI agent는 사용자의 프롬프트 입력 없이, 하위 task를 설계하고, 검토하고, 수행하는 과정(<strong>Self-Prompting</strong>)을 반복하는 특징을 가진다. 변하는 상황에 따라 접근 방식을 스스로 조정하며 목표 달성을 위한 전략을 수립하고 실행한다.</p> <p>AI agent의 요건</p> <ol> <li>작업 실행을 계획하고 사용 가능한 도구를 활용하여 작업을 자율적으로 수행하는 시스템</li> <li>사용자를 대신해 자율적으로 작업을 수행할 수 있는 프로그램</li> <li>인간의 지속적인 입력 필요없이 스스로 생각하고, 결정하고, 행동함</li> </ol> <p>예를 들어, 사용자의 프롬프트가 “<strong>3박4일 일정으로 일본 여행을 가고 싶어. 200만원 예산 내로 여행을 설계해줘</strong>.”일 경우, AI Agent는:</p> <ul> <li>사용자의 캘린더앱을 확인해 여행 일정 추천</li> <li>skyscanner를 통해 여행 일정에 맞는 항공권 추천</li> <li>숙박 사이트를 통해 예산에 맞는 호텔 검색</li> <li>관광지 웹사이트를 확인해 관광지 운영 시간 등을 확인</li> </ul> <hr/> <h3 id="2-ai-agent--methodology">2. AI Agent : Methodology</h3> <h4 id="21-동작-순서">2.1 동작 순서</h4> <p>steps</p> <ol> <li>*environment를 파악하고</li> <li>수집한 정보를 활용하여</li> <li>목표 달성을 위한 계획을 세우고</li> <li>이를 실행함</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/16-480.webp 480w,/assets/img/16-800.webp 800w,/assets/img/16-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="22-동작-원리">2.2 동작 원리</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/17-480.webp 480w,/assets/img/17-800.webp 800w,/assets/img/17-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>steps</p> <ol> <li>observations를 통해 environment를 파악하고</li> <li>past experiences, abilities, goals/preferencees/prior knowledge를 바탕으로 actions 수행</li> <li>1,2 번 (반복)</li> </ol> <ul> <li><strong>environment</strong> : agent가 작동하는 영역/도메인 (여행 계획 수립 agent예시의 경우, 사용자의 캘린더, 위치정보, 언어설정, 등)</li> <li>abilities/prior knowledge : 모델이 데이터를 통해 이미 학습한 지식</li> <li>goals/preferences : 사용자의 요청의 목적, 수행할 task들을 agent가 스스로 판단</li> <li>observations : 주어진 환경에 대해 수집한 정보</li> <li>past experiences : Agent가 가진 과거의 경험</li> <li></li> </ul> <h4 id="23-상호작용">2.3 상호작용</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/18-480.webp 480w,/assets/img/18-800.webp 800w,/assets/img/18-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="24-ai-agent-종류">2.4 AI Agent 종류</h4> <p>주요 agent들 : goal-based agent</p> <h5 id="241-단순-반응형-에이전트-reactive-agent">2.4.1 단순 반응형 에이전트 (Reactive Agent)</h5> <ul> <li><strong>특징</strong>: 과거의 데이터를 저장하지 않고, 현재 환경에서 관찰한 정보를 바탕으로 즉각적인 반응을 보이는 에이전트</li> <li><strong>예시</strong>: <ul> <li>자동문 시스템: 센서를 통해 사람의 존재를 인식하고 문을 여는 역할 수행</li> <li>온도 조절기: 현재 온도를 기준으로 난방 또는 냉방을 수행</li> </ul> </li> </ul> <h5 id="242-상태-기반-에이전트-model-based-agent">2.4.2 상태 기반 에이전트 (Model-Based Agent)</h5> <ul> <li><strong>특징</strong>: 현재 상태와 과거 상태를 내부적으로 저장하여 의사결정을 수행하며, 환경에 대한 모델을 사용하여 복잡한 작업을 처리할 수 있음</li> <li><strong>예시</strong>: <ul> <li>네비게이션 시스템: 목적지와 현재 위치를 바탕으로 최적의 경로를 계산</li> </ul> </li> </ul> <h5 id="243-목표-지향형-에이전트-goal-based-agent">2.4.3 목표 지향형 에이전트 (Goal-Based Agent)</h5> <ul> <li><strong>특징</strong>: 목표 중심으로 작동하며, 현재 상태와 목표 상태의 차이를 줄이기 위한 계획을 수립함</li> <li><strong>예시</strong>: <ul> <li>로봇 청소기: 방의 청소가 필요한 위치를 탐지하고 효율적으로 움직임</li> <li>물류 로봇: 특정 지점에 물건을 운반하는 작업을 수행</li> </ul> </li> </ul> <h5 id="244-유틸리티-기반-에이전트-utility-based-agent">2.4.4 유틸리티 기반 에이전트 (Utility-Based Agent)</h5> <ul> <li><strong>특징</strong>: 목표 달성뿐만 아니라 목표 달성의 “질”을 고려하여 유틸리티 함수(만족도, 이익 등)를 기반으로 최적의 선택을 수행함</li> <li><strong>예시</strong>: <ul> <li>자율주행 차량: 안전, 시간, 연료 효율성을 동시에 고려하여 경로 선택</li> <li>추천 시스템: 사용자 선호도에 따라 최적의 옵션을 추천</li> </ul> </li> </ul> <h5 id="245-학습-에이전트-learning-agent">2.4.5 학습 에이전트 (Learning Agent)</h5> <ul> <li><strong>특징</strong>: 경험을 통해 학습하고 시간이 지남에 따라 성능을 점진적으로 개선하며, 학습 요소, 성능 요소, 비평 요소, 문제 생성기를 포함한 구성 요소로 작동</li> <li><strong>예시</strong>: <ul> <li>AlphaGo: 체스 또는 바둑과 같은 게임을 스스로 학습하고 개선</li> <li>챗봇: 사용자와의 대화 기록을 기반으로 점진적으로 대화 능력 향상</li> </ul> </li> </ul> <h5 id="246-다중-에이전트-시스템-multi-agent-system">2.4.6 다중 에이전트 시스템 (Multi-Agent System)</h5> <ul> <li><strong>특징</strong>: 여러 에이전트가 협력하거나 경쟁하며 목표를 달성하는 시스템으로, 각 에이전트는 독립적으로 작동하며 상호작용을 통해 복잡한 문제를 해결함</li> <li><strong>예시</strong>: <ul> <li>스마트 그리드: 전력 분배를 최적화하는 시스템</li> <li>온라인 게임 AI: 다수의 캐릭터 간 협력 및 경쟁 (예: Pacman)</li> </ul> </li> </ul> <h5 id="247-하이브리드-에이전트-hybrid-agent">2.4.7 하이브리드 에이전트 (Hybrid Agent)</h5> <ul> <li><strong>특징</strong>: 위에서 언급된 여러 종류의 에이전트를 조합하여 설계된 시스템으로, 복잡한 문제 해결을 위해 다양한 접근 방식을 사용함</li> <li><strong>예시</strong>: <ul> <li>자율주행 차량: 감지(단순 반응형)와 경로 계획(목표 지향형)을 동시에 수행</li> <li>인공지능 비서(Siri, Alexa): 학습 및 목표 지향형 기능을 결합</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="NLP"/><summary type="html"><![CDATA[1. AI Agents : Introduction 2. AI Agent : Methodology 2.1 동작 순서 2.2 동작 원리 2.3 상호작용 2.4 AI Agent 종류 2.4.1 단순 반응형 에이전트 (Reactive Agent) 2.4.2 상태 기반 에이전트 (Model-Based Agent) 2.4.3 목표 지향형 에이전트 (Goal-Based Agent) 2.4.4 유틸리티 기반 에이전트 (Utility-Based Agent) 2.4.5 학습 에이전트 (Learning Agent) 2.4.6 다중 에이전트 시스템 (Multi-Agent System) 2.4.7 하이브리드 에이전트 (Hybrid Agent)]]></summary></entry><entry><title type="html">(KOR) AutoGPT</title><link href="https://joohunhyun.github.io/blog/2024/AutoGPT/" rel="alternate" type="text/html" title="(KOR) AutoGPT"/><published>2024-12-29T00:00:00+00:00</published><updated>2024-12-29T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/AutoGPT</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/AutoGPT/"><![CDATA[<h3 id="autogpt-개요">AutoGPT 개요</h3>]]></content><author><name></name></author><category term="study"/><category term="NLP"/><summary type="html"><![CDATA[AutoGPT 개요]]></summary></entry><entry><title type="html">(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model</title><link href="https://joohunhyun.github.io/blog/2024/BERT/" rel="alternate" type="text/html" title="(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model"/><published>2024-12-26T00:00:00+00:00</published><updated>2024-12-26T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/BERT</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/BERT/"><![CDATA[<ul> <li><a href="#1-논문-요약">1. 논문 요약</a> <ul> <li><a href="#11-rationale">1.1 Rationale</a></li> <li><a href="#12-methodology-요약">1.2 Methodology 요약</a></li> <li><a href="#13-conclusion">1.3 Conclusion</a></li> </ul> </li> <li><a href="#2-mbert의-한계">2. mBERT의 한계</a> <ul> <li><a href="#21-corpus말뭉치-domain의-한계">2.1 Corpus(말뭉치) Domain의 한계</a></li> <li><a href="#22-한국어의-언어적-특성을-충분히-고려하지-않은-점">2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점</a></li> <li><a href="#23-모델의-크기">2.3 모델의 크기</a></li> </ul> </li> <li><a href="#3-methodology">3. Methodology</a> <ul> <li><a href="#31-subcharacter-text-representation">3.1 Subcharacter Text Representation</a></li> <li><a href="#32-subword-vocabulary">3.2 Subword Vocabulary</a></li> </ul> </li> <li><a href="#4-results">4. Results</a></li> <li><a href="#5-related-work--benchmarks">5. Related Work / Benchmarks</a></li> </ul> <hr/> <p><br/></p> <h3 id="1-논문-요약">1. 논문 요약</h3> <p><br/></p> <h4 id="11-rationale">1.1 Rationale</h4> <p>Multilingual-BERT(이하 mBERT)는 위키백과의 104개 언어로 된 문서들을 기반으로 학습된 모델이다. 그러나, 모든 언어의 언어적 특성을 학습시킬 수는 없기 때문에 비영어 다운스트림 작업(non-English downstream tasks)에는 정확도가 다소 떨어진 모습을 보인다. <strong>Downstream tasks</strong>란 자연어 처리(NLP)에서 사전 학습된 언어 모델을 활용하여 수행하는 특정 응용 작업 (예: 감정 분석, 개체명 인식, 문장 분류 등)을 의미한다.</p> <p>한국어는 만개 이상의 문자(character)를 사용하고, 독어나 불어와 같은 굴절어보다 형태적으로 복잡하다. mBERT에서는 이 가운데 오직 1,187개의 문자만이 포함되었다. 또한, mBERT 모델은 104개의 언어 데이터를 포함하기에 크기가 과도하게 크다는 단점이 존재한다. 그래서, ALBERT나 DistilBERT처럼 모델을 축소하면서도 성능을 유지하는 방법이 필요하다.</p> <p>요약하자면:</p> <ol> <li>mBERT는 모델 크기가 과도하게 크다.</li> <li>mBERT는 non-English downstream tasks에서 성능이 다소 떨어진다.</li> <li>모델을 축소하면서 성능을 유지하는 한국어 언어모델이 필요하다.</li> </ol> <p>KR-BERT는 이런 점들을 해결하기 위해 고안된 새로운 한국어 언어모델이다.</p> <p><br/></p> <h4 id="12-methodology-요약">1.2 Methodology 요약</h4> <p><br/></p> <h4 id="13-conclusion">1.3 Conclusion</h4> <ul> <li>KR-BERT는 다음의 downstream tasks들에서 mBERT의 성능을 능가했다. <ul> <li>senti- ment analysis</li> <li>Question-Answering</li> <li>Named Entity Recognition(NER)</li> <li>Paraphrase Detection</li> </ul> </li> <li>KR-BERT는 다른 한국어 모델들에 비해 다운스트림 작업에서 더 좋은 성능을 보이거나 비슷한 성과를 보였다.</li> <li>형태소가 많은 한국어에 맞게 서브-캐릭터 기반 모델과 Bidirectional-WordPiece 토크나이저를 사용하여 적은 자원으로도 효과적인 성능을 달성했다.</li> </ul> <p><br/> <br/></p> <h3 id="2-mbert의-한계">2. mBERT의 한계</h3> <p><br/></p> <h5 id="21-corpus말뭉치-domain의-한계">2.1 Corpus(말뭉치) Domain의 한계</h5> <p>GCamemBERT와 같은 언어별 특화된(Language Specific) BERT 모델들은 법률 데이터, 뉴스 기사 등, 다양한 데이터 소스를 사용하여 학습된 반면, mBERT는 유저들이 직접 작성한 데이터 소스(블로그 글, 댓글, 등)를 사용하지 않고, 위키백과 포스트들의 언어적 특성을 기반으로만 학습되어 언어 사용(limited in its domain with respect to language usage - 직역하면 언어 사용이지만 어휘력일 것이라고 생각됨) 측면에서 한계가 존재한다.</p> <p><br/></p> <h5 id="22-한국어의-언어적-특성을-충분히-고려하지-않은-점">2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점</h5> <p>2.2.1 Rare Character Problem</p> <p>라틴 문자 기반 언어는 단어를 문자 단위로 분리하여 처리할 수 있는 반면, 한국어의 언어적 특성상 음절 단위로 처리되어 Out-of-Vocabulary(OoV)가 라틴 문자 기반 언어보다 많을수밖에 없다. 그럼으로, 이런 한국어의 특성을 고려한 새로운 BERT Vocabulary가 필요하다.</p> <p>2.2.2 교착언어에 적합하지 않은 모델</p> <p>교착어(agglutinative language)란 문법적 의미를 전달하기 위해 주로 접사를 단단히 결합하는 언어를 의미한다. 교착어는 형태론적인 복잡성(morphological complexity) 때문에 vocabulary를 표현하기 어렵다. 한국어 역시 교착언어에 해당하며, mBERT는 이 문제를 제대로 처리하지 않은 것으로 보인다.</p> <p>2.2.3 Lack of Meaningful Tokens</p> <p>독일어의 경우, mBERT 모델의 어휘에 명확한 semantic meaning이 없는 subword unit이 포함되어 있었다. 한국어에서도 이와 유사한 문제가 존재했는데, 대부분의 단어가 형태소와 같은 단위가 아닌 단일 문자로 tokenized 되었기 떄문이다. 이 문제를 해결하기 위해, 한국어 텍스트와 한국어의 언어적 특성을 고려한 vocabulary 및 tokenizer를 구현했다.</p> <p><br/></p> <h5 id="23-모델의-크기">2.3 모델의 크기</h5> <p>대형 모델들은 많은 양의 dataset, parameter, voabulary를 필요로 한다. mBERT는 167M, RoBERTa는 355M 파라미터를 사용해 자원의 제약을 받기 마련인데, KR-BERT는 적은 parameter와 훈련 데이터를 사용하면서 mBERT와 비슷한 성능을 유지할 수 있었다.</p> <p><br/> <br/></p> <h3 id="3-methodology">3. Methodology</h3> <p>KR-BERT를 Multilingual BERT, Kor- BERT, 그리고 KoBERT와 비교했다.</p> <p><br/></p> <h4 id="31-subcharacter-text-representation">3.1 Subcharacter Text Representation</h4> <p>한국어 텍스트는 Text-&gt; 한글 -&gt; graphemes의 형태로 분해 가능하다. 이런 특성을 반영하기 위해, KR-BERT는 syllable character와 sub-character 두 가지 말뭉치 표현 방식을 사용하여 new vocabulary와 BERT model을 학습시켰다. BPE 알고리즘을 적용하면, 다음과 같은 tokenization이 가능해진다.</p> <p><code class="language-plaintext highlighter-rouge">Example : 뜀(ttwim, "jumping")은 ㄸㅟ(ttwi, "jump")와 ᄆ(m, "-ing")으로 분해될 수 있다</code></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/15-480.webp 480w,/assets/img/15-800.webp 800w,/assets/img/15-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>기존 mBERT는</p> <p>KR-BERT 모델은 나머지 벤치마크 모델들과 달리, 음절 문자 이외에도 하위 문자 표현을 사용해 다양한 한국어 동사/형용사의 공통된 특성을 학습할 수 있었다.</p> <p><br/></p> <h4 id="32-subword-vocabulary">3.2 Subword Vocabulary</h4> <h3 id="4-results">4. Results</h3> <h3 id="5-related-work--benchmarks">5. Related Work / Benchmarks</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/13-480.webp 480w,/assets/img/13-800.webp 800w,/assets/img/13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/14-480.webp 480w,/assets/img/14-800.webp 800w,/assets/img/14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="study"/><category term="papers"/><category term="NLP"/><summary type="html"><![CDATA[1. 논문 요약 1.1 Rationale 1.2 Methodology 요약 1.3 Conclusion 2. mBERT의 한계 2.1 Corpus(말뭉치) Domain의 한계 2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점 2.3 모델의 크기 3. Methodology 3.1 Subcharacter Text Representation 3.2 Subword Vocabulary 4. Results 5. Related Work / Benchmarks]]></summary></entry><entry><title type="html">(ENG) MLP and DNN</title><link href="https://joohunhyun.github.io/blog/2024/MLP-DNN/" rel="alternate" type="text/html" title="(ENG) MLP and DNN"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/MLP-DNN</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/MLP-DNN/"><![CDATA[<ul> <li>Perceptron -&gt; delta rule</li> <li>MLP (2~3 layers) -&gt; backpropagation</li> <li>DNN (~10 layers) -&gt; using ReLU instead of sigmoid</li> </ul> <h4 id="mlps-problem">MLP’s problem</h4> <ul> <li>Major issue of designing MLP is how many hidden units are optimal? <ul> <li>너무 만으면 : overfit</li> <li>너무 적으면 : underfit</li> </ul> </li> <li>As the number of hidden layers increase, the sigmoid function’s relatively small gradient repeatedly gets multiplied. -&gt; this makes the gradient close to 0, and the weights are not updated (<strong>vanishing gradient problem</strong>)</li> </ul> <p><strong>Solution to this problem</strong></p> <ol> <li>Using a ReLU Function</li> </ol> <p>The Rectified Linear Unit (ReLU) function is defined as:</p> \[f(x)=max(0,x)\] <p>ReLU has a gradient of 1 1 for positive inputs, which prevents the gradient from shrinking excessively as it is propagated through the network.</p> <p>Advantages of ReLU:</p> <ul> <li>Avoids vanishing gradients: The gradient remains constant for positive inputs, ensuring that weights continue to be updated.</li> <li>Computational efficiency: ReLU is simpler to compute than sigmoid or tanh.</li> <li>Sparsity: It introduces sparsity in activations (many outputs are zero), which can improve generalization.</li> </ul> <p>However, ReLU can suffer from the dying ReLU problem, where neurons become inactive (outputting zero) due to large negative gradients. Variants like Leaky ReLU and Parametric ReLU address this issue.</p> <ol> <li>Xavier Initialization</li> </ol> <p>Proper weight initialization is crucial for mitigating vanishing or exploding gradients. The Xavier initialization (or Glorot initialization) ensures that the variance of activations and gradients is maintained across layers.</p> <p>Benefits:</p> <ul> <li>Prevents vanishing/exploding gradients by keeping the variance of inputs and outputs consistent across layers.</li> <li>Helps the network converge faster.</li> </ul> <ol> <li>Batch Normalization</li> </ol> <p>Batch Normalization (BatchNorm) normalizes the inputs to each layer by adjusting the mean and variance of the activations during training.</p> <p>Benefits</p> <ul> <li>Improves gradient flow: Normalization reduces internal covariate shift, allowing deeper networks to train effectively.</li> <li>Stabilizes learning: It reduces sensitivity to initialization and learning rate.</li> <li>Acts as regularization: BatchNorm has a slight regularization effect, reducing the need for dropout in some cases.</li> </ul> <ol> <li>Early stopping</li> </ol> <p>Terminating training once a performance plateau has been reached.</p> <p>Example performance plateus</p> <ul> <li>Error is small enough</li> <li>Number of epochs</li> <li>Pruning the network</li> <li>Training with noisy samples</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Perceptron -&gt; delta rule MLP (2~3 layers) -&gt; backpropagation DNN (~10 layers) -&gt; using ReLU instead of sigmoid]]></summary></entry><entry><title type="html">(ENG) RNN and LSTM</title><link href="https://joohunhyun.github.io/blog/2024/RNN/" rel="alternate" type="text/html" title="(ENG) RNN and LSTM"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/RNN</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/RNN/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></li> <li><a href="#exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</a></li> <li><a href="#long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</a> <ul> <li><a href="#lstm-cell-structure">LSTM Cell Structure</a></li> <li><a href="#differences-between-rnn-and-lstm">Differences Between RNN and LSTM</a></li> </ul> </li> <li><a href="#tldr">TL;DR</a></li> </ul> <hr/> <h4 id="recurrent-neural-networks">Recurrent Neural Networks</h4> <p>Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining an internal state that captures information about previous time steps. This makes them ideal for tasks like speech recognition, machine translation, and time series analysis.</p> <ul> <li><strong>Structure</strong>: In an RNN, each hidden unit has connections to itself and to other hidden units, allowing information to flow across time steps.</li> <li><strong>Mathematics</strong>: At each time step \(t\), the hidden state \(h_t\) is updated using the input \(x_t\) and the previous hidden state \(h_{t-1}\): \(h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\) Where: <ul> <li>\(W_{xh}\): Weight matrix for the input.</li> <li>\(W_{hh}\): Weight matrix for the hidden state.</li> <li>\(b_h\): Bias term.</li> </ul> </li> </ul> <h4 id="exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</h4> <p>RNNs use the same weight matrices \(W_{hh}\) and \(W_{xh}\) at every time step. During backpropagation through time (BPTT), gradients are repeatedly multiplied by these matrices, which can cause:</p> <ul> <li><strong>Vanishing gradients</strong>: Gradients shrink exponentially, making it difficult to update weights and learn long-term dependencies.</li> <li><strong>Exploding gradients</strong>: Gradients grow exponentially, leading to instability.</li> </ul> <h4 id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h4> <p><strong>Long Short-Term Memory (LSTM)</strong> networks are a type of RNN specifically designed to handle the vanishing gradient problem. They achieve this by introducing a more complex structure within each unit that allows information to be selectively remembered or forgotten over long time steps.</p> <h5 id="lstm-cell-structure">LSTM Cell Structure</h5> <p>An LSTM unit consists of four interacting layers that control the flow of information:</p> <ol> <li> <p><strong>Forget Gate</strong>: Determines which parts of the previous cell state \(C_{t-1}\) should be forgotten: \(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\) Where \(\sigma\) is the sigmoid activation function.</p> </li> <li> <p><strong>Input Gate</strong>: Decides what new information to store in the cell state: \(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\) A candidate cell state \(\tilde{C}_t\) is created: \(\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\)</p> </li> <li> <p><strong>Cell State Update</strong>: Combines the forget gate and the input gate to update the cell state: \(C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t\) Where \(\odot\) denotes element-wise multiplication.</p> </li> <li> <p><strong>Output Gate</strong>: Determines the next hidden state \(h_t\): \(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\) The hidden state is calculated as: \(h_t = o_t \odot \tanh(C_t)\)</p> </li> </ol> <hr/> <h5 id="differences-between-rnn-and-lstm">Differences Between RNN and LSTM</h5> <table> <thead> <tr> <th>Feature</th> <th>RNN</th> <th>LSTM</th> </tr> </thead> <tbody> <tr> <td><strong>Architecture</strong></td> <td>Simple structure with one update equation.</td> <td>Complex structure with gates for controlling memory.</td> </tr> <tr> <td><strong>Gradient Issues</strong></td> <td>Suffers from vanishing/exploding gradients.</td> <td>Designed to avoid vanishing gradients.</td> </tr> <tr> <td><strong>Long-Term Dependencies</strong></td> <td>Struggles to learn long-term patterns.</td> <td>Efficiently captures long-term dependencies.</td> </tr> <tr> <td><strong>Use Cases</strong></td> <td>Short-term dependencies.</td> <td>Long-term dependencies (e.g., speech, text).</td> </tr> </tbody> </table> <hr/> <h4 id="tldr">TL;DR</h4> <p>While RNNs are effective for sequence modeling, their training can be hindered by the vanishing gradient problem, especially for long-term dependencies. LSTMs address this limitation by introducing gating mechanisms that allow the model to selectively retain and forget information, making them a powerful tool for tasks involving sequential data.</p>]]></content><author><name></name></author><category term="study"/><category term="DL"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(ENG) Training and Evaluating Deep Networks</title><link href="https://joohunhyun.github.io/blog/2024/deep-learning/" rel="alternate" type="text/html" title="(ENG) Training and Evaluating Deep Networks"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/deep-learning</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/deep-learning/"><![CDATA[<h4 id="training-and-evaluating-deep-networks">Training and Evaluating Deep Networks</h4> <p>When dealing with DL, the following components are vital</p> <ol> <li>training set</li> <li>test set</li> <li>validation set : used to tune a model’s hyperparameters for model selection. Also, it prevents overfitting by early stopping</li> </ol> <h5 id="hyperparameter-tuning">Hyperparameter Tuning</h5> <p>Hyperparameters in DL are usually tuned heuristically by hand or using grid search.</p> <h5 id="dropout">Dropout</h5> <p>Dropout is a form of regualization that randomly deletes units and their connections during training.</p> <p>Pros</p> <ul> <li>과적합 방지, 뉴런간의 의존성을 줄여 학습된 모델의 일반화 성능 향상</li> <li>간단한 구현</li> </ul> <p>Cons</p> <ul> <li>Additional overhead : 훈련 중에 매번 mask를 생성하고 적용</li> <li>학습 속도 감소</li> </ul> <h5 id="unsupervised-pretraining">Unsupervised Pretraining</h5> <p>This method can be useful when the volume of labeled data is small relative to the model’s capacity.</p> <ul> <li>딥러닝 초기의 학습 안정화와 데이터 부족 문제를 해결하기 위해고안된 기법</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="DL"/><summary type="html"><![CDATA[Training and Evaluating Deep Networks]]></summary></entry><entry><title type="html">(ENG) Feature Selection, Extraction, and Ensamble Methods</title><link href="https://joohunhyun.github.io/blog/2024/feature/" rel="alternate" type="text/html" title="(ENG) Feature Selection, Extraction, and Ensamble Methods"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/feature</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/feature/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="feature-subset-selection">Feature Subset Selection</h4> <p>Definition : Feature subset selection finds a subset of original features without new features</p> <h5 id="1-filter-method">1. Filter Method</h5> <ul> <li>Evaluates features with performance measures based on information gain or distance between each attribute in the class</li> <li>장점 : computationally siple and fast</li> <li>단점 : slow searching</li> <li>해결책 : use a single-attribute evaluator with ranking</li> </ul> <h5 id="2-wrapper-method">2. Wrapper Method</h5> <ul> <li>Searches for best combination of features among <strong>all</strong> posttible combination</li> <li>장점 : generally better performance, simple and direct</li> <li>단점 : \(O(n^2)\) time complexity - slow</li> <li>해결책 <ul> <li>eliminate irrelevant features</li> <li>eliminate redundant attributes</li> </ul> </li> </ul> <h5 id="3-embedded-method">3. Embedded method</h5> <ul> <li>Uses ML models for classification and then an obtimal subset of features/ranking of feature is built by the classifier algorithm</li> </ul> <h5 id="search-methods">Search Methods</h5> <ul> <li>Weka 예제에서는 classfier(wrapper, etc)와 search method 둘 다 정의해야한다.</li> </ul> <h5 id="different-search-methods">Different Search Methods</h5> <ol> <li>Exhaustive : \(2^n\) subsets</li> <li>Backwards</li> <li>Forwards</li> <li>Bidirectional</li> </ol> <h2><br/></h2> <h4 id="feature-extraction-aka-reducing-dimentionality">Feature Extraction (AKA: Reducing Dimentionality)</h4> <p>Definition : extracts new set of features from the original features</p> <h5 id="pca">PCA</h5> <ul> <li>Definition : unsupervised approach to examine relations among a set of variables</li> <li>When to use this? <ul> <li>PCA is useful in reducing the dimentionality of 3-D, 4-D scatterplots (왜 사용하냐 : visually difficult to interpret data points in high-dimensional space)</li> </ul> </li> <li>Ex : Compressing MNIST- dataset using PCA (to reduce dimension)</li> </ul> <h2 id="-1"><br/></h2> <h4 id="ensamble-method">Ensamble Method</h4> <ul> <li>Definition : 여러개의 모델들을 조합해서 더욱 정확한 결정을 내리는 방법 (위원회 운영과 비슷)</li> </ul> <p>Types of Ensamble Methods:</p> <ol> <li>Boosting</li> <li>Bagging</li> <li>Stacking</li> </ol> <h4 id="1-boosting-adaboost"><strong>1. Boosting: AdaBoost</strong></h4> <p>Boosting is an ensemble learning technique that combines the predictions of several weak learners (simple models, such as decision trees) to create a strong learner. <strong>AdaBoost (Adaptive Boosting)</strong> is one of the most popular boosting algorithms.</p> <h5 id="how-adaboost-works"><strong>How AdaBoost Works</strong></h5> <p>AdaBoost works iteratively, focusing on the data points that are harder to classify by assigning them higher weights in subsequent iterations.</p> <ol> <li><strong>Initialize Weights</strong>: <ul> <li>Assign equal weights to all training samples: \(w_i = \frac{1}{N}, \, \forall i \in \{1, 2, \dots, N\}\) Where \(N\) is the total number of training samples.</li> </ul> </li> <li><strong>Train Weak Learner</strong>: <ul> <li>Train a weak learner (e.g., a shallow decision tree) using the weighted training dataset.</li> </ul> </li> <li><strong>Calculate Weighted Error</strong>: <ul> <li>Compute the error \(e_t\) of the weak learner: \(e_t = \frac{\sum_{i=1}^N w_i \cdot I(y_i \neq \hat{y}_i)}{\sum_{i=1}^N w_i}\) Where: <ul> <li>\(y_i\): True label of sample \(i\).</li> <li>\(\hat{y}_i\): Predicted label by the weak learner.</li> <li>\(I(y_i \neq \hat{y}_i)\): Indicator function that equals 1 if the prediction is incorrect.</li> </ul> </li> </ul> </li> <li><strong>Compute Alpha (Model Weight)</strong>: <ul> <li>Calculate the weight of the weak learner: \(\alpha_t = \frac{1}{2} \ln\left(\frac{1 - e_t}{e_t}\right)\) The higher the accuracy of the weak learner, the larger its contribution to the final model.</li> </ul> </li> <li><strong>Update Sample Weights</strong>: <ul> <li>Increase the weights of misclassified samples to emphasize them in the next iteration: \(w_i \leftarrow w_i \cdot \exp(\alpha_t \cdot I(y_i \neq \hat{y}_i))\)</li> <li>Normalize the weights so they sum to 1: \(w_i \leftarrow \frac{w_i}{\sum_{j=1}^N w_j}\)</li> </ul> </li> <li><strong>Repeat</strong>: <ul> <li>Train the next weak learner using the updated weights, and repeat for \(T\) iterations.</li> </ul> </li> <li><strong>Final Prediction</strong>: <ul> <li>Combine the predictions of all weak learners using their weights \(\alpha_t\): \(H(x) = \text{sign} \left( \sum_{t=1}^T \alpha_t \cdot h_t(x) \right)\) Where: <ul> <li>\(h_t(x)\): Prediction from the \(t\)-th weak learner.</li> </ul> </li> </ul> </li> </ol> <hr/> <h5 id="advantages-of-adaboost"><strong>Advantages of AdaBoost</strong></h5> <ul> <li>Improves accuracy by focusing on difficult samples.</li> <li>Works well with simple weak learners.</li> <li>Robust to overfitting in many cases.</li> </ul> <h5 id="disadvantages-of-adaboost"><strong>Disadvantages of AdaBoost</strong></h5> <ul> <li>Sensitive to noisy data and outliers, as they receive higher weights.</li> <li>Can be computationally expensive for large datasets.</li> </ul> <h5 id="2-baggingstands-for-boostrap-aggrevating">2. Bagging(stands for: Boostrap Aggrevating)</h5> <p>Bagging은 앙상블 기법들 가운데 가장 간단한 방법이다(taking a vote, or weighted vote). Decision Tree의 경우, training set에 따라서 그 구조가 매우 민감하게 바뀐다. 이런 경우에는 1) training set을 여러 개 만들고 2) 각각 모델을 만들어서 3) voting을 한다. In numerical prediction, taking the average of all predictions.</p> <p>Breiman(1996) noticed that an ensamble of trees improved when the trees differed significantly from each other, today called <strong>Random Forest</strong>. Bagging was able to create a diverse ensamble of classifiers by introducing randomness into the learning alg’s input, which resulted in <strong>better results</strong>. In a DT, picking the best option can be randomized by picking one of the N best options at random instead of a signle winner (ex: top 3 options, top 5 options)</p> <p><strong>Methods of picking different “winners” in bagging</strong></p> <ol> <li>elite : picking the best one</li> <li>randomization : picking 1 randomly from top 5 candidates</li> <li>stochastic : top5 중에서 룰렛으로 고르는 방법 (weighted)</li> </ol> <h5 id="3-stacking">3. Stacking</h5> <p>Stacking is used on models built by different learning algorithsm. For example, stacking can be used when you want to form a classifier for a given dataset with A) deicision tree inducer B) naive bayes learnner and C) instance based learning scheme.</p> <p>Stacking involves a <strong>metalearner</strong> that replaces the voting procedure of boosing and bagging. Metalearner is used to discover how to best combine the output of the base learners to determine which classifiers are the reliable ones.</p> <ul> <li>base learners : level-0 models</li> <li>meta learner : level-1 model</li> <li>predictions from base models are inputs to the meta learner. (funnel처럼 위에서 아래로 내려온다)</li> </ul> <p>TL;DR : Stacking combines predictions of base learners using metalearner(NOT voting)</p>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(KOR) 연구실 안전교육 배속하는 방법</title><link href="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/" rel="alternate" type="text/html" title="(KOR) 연구실 안전교육 배속하는 방법"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/skip-safety-skip%20copy</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/skip-safety-skip-copy/"><![CDATA[<p>24’12월 기준, 이 방법이 제일 편한 듯. 다른 방법들은 다 막힌 듯 하다.</p> <p>참고 :</p> <ul> <li>중간중간에 퀴즈도 있으니 유의해야한다.</li> <li>배속은 16배까지만 가능하다.</li> </ul> <p><strong>크롬</strong> 환경에서 연구실 안전교육 동영상 재생 후 개발자모드 콘솔 <code class="language-plaintext highlighter-rouge">F12</code>에 해당 코드 붙여넣기:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>document.querySelector('video').playbackRate = 16;
</code></pre></div></div>]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[24’12월 기준, 이 방법이 제일 편한 듯. 다른 방법들은 다 막힌 듯 하다.]]></summary></entry><entry><title type="html">(ENG) SVM, SVC</title><link href="https://joohunhyun.github.io/blog/2024/SVM-SVC/" rel="alternate" type="text/html" title="(ENG) SVM, SVC"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/SVM-SVC</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/SVM-SVC/"><![CDATA[<p><strong>Table of Contents</strong></p> <hr/> <h4 id="classification-models">Classification Models</h4> <p>Definition : 두개의 class A and B를 feature space에서 분리하는 모델</p> <ul> <li>Decision Trees</li> <li>Probabilistic models</li> <li>Linear models</li> <li>Non-linear models</li> </ul> <p>In the case of non-linear data, there are different methods to model this.</p> <ol> <li>MLP : achieve non-linearlity by combining perceptrons(linear models)</li> <li>SVM : supervised machine learning algorithm used for classification and regression tasks.</li> <li>SVC :</li> </ol> <h5 id="svm">SVM</h5> <p>Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. The key idea is to find the best hyperplane that separates data points into different classes. In an ideal case, this hyperplane maximizes the margin between data points of different classes.</p> <ul> <li>Core Concepts: <ul> <li>The hyperplane is the decision boundary.</li> <li>SVM focuses on data points (support vectors) near the decision boundary that influence its position.</li> <li>The algorithm uses kernels to transform data into higher-dimensional spaces when it is not linearly separable in the original space.</li> </ul> </li> <li>Applications: Image classification, text categorization, and other binary/multi-class problems.</li> <li>Pros: <ul> <li>Effective for high-dimensional spaces.</li> <li>Works well with a clear margin of separation.</li> </ul> </li> <li>Cons: <ul> <li>Computationally expensive for large datasets.</li> <li>Sensitive to the choice of kernel and hyperparameters.</li> </ul> </li> </ul> <h6 id="rbf-kernel">RBF Kernel</h6> <p>The RBF kernel, also known as the Gaussian kernel, is one of the most commonly used kernels in SVM. It is a non-linear kernel that maps data points into a higher-dimensional space where a linear hyperplane can effectively separate them.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: It can model complex relationships in the data.</li> <li>Local influence: Points that are closer to each other have a higher similarity, and their influence decreases exponentially with distance</li> <li> <p>Scalability: Effective for problems where the decision boundary is not linear.</p> </li> <li>Pros: <ul> <li>Handles non-linearly separable data well.</li> <li>Requires fewer hyperparameters compared to other kernels.</li> </ul> </li> <li>Cons: <ul> <li>γ must be carefully tuned; poor tuning can lead to overfitting or underfitting.</li> </ul> </li> </ul> <h6 id="polynomial-kernel">Polynomial Kernel</h6> <p>The Polynomial kernel is another non-linear kernel used in SVM. It computes the similarity between two data points as a polynomial function of their inner product in the input space.</p> <p>Characteristics:</p> <ul> <li>Non-linearity: Maps the input space to a higher-dimensional space defined by polynomial terms.</li> <li>Flexibility: The degree \(d\) controls the complexity of the decision boundary.</li> <li>Global influence: Unlike the RBF kernel, it considers global features of the data.</li> <li>Pros: <ul> <li>Can model non-linear relationships with adjustable complexity via \(d\)</li> <li>Effective for datasets where classes are distinguishable by polynomial decision boundaries.</li> </ul> </li> <li>Cons: <ul> <li>More sensitive to feature scaling than the RBF kernel.</li> <li>Higher degrees can lead to overfitting.</li> </ul> </li> </ul> <h5 id="svc">SVC</h5> <ul> <li> <p>SVC stands for Support Vector Classifier and is the implementation of SVM for classification tasks in libraries like scikit-learn.</p> </li> <li>Key Differences Between SVM and SVC: SVM is the broader concept that encompasses both classification and regression tasks (e.g., SVM for regression is called SVR).</li> <li>SVC is specifically the classification implementation of SVM in scikit-learn.</li> <li>Features of SVC: <ul> <li>It supports linear and non-linear kernels like polynomial, RBF (Radial Basis Function), and sigmoid.</li> <li>Provides hyperparameters like C (regularization strength) and gamma (kernel coefficient) for tuning the model.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ML"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry><entry><title type="html">(KOR) VsCode Shortcut Keys(MacOS)</title><link href="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/" rel="alternate" type="text/html" title="(KOR) VsCode Shortcut Keys(MacOS)"/><published>2024-12-13T00:00:00+00:00</published><updated>2024-12-13T00:00:00+00:00</updated><id>https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys</id><content type="html" xml:base="https://joohunhyun.github.io/blog/2024/VsCode-Shortcut-Keys/"><![CDATA[<p>💡익숙해진 단축키는 문서에서 삭제</p> <p>💡원본은 .md 파일에 존안</p> <hr/> <p><strong>Table of contents</strong></p> <ul> <li><a href="#vscode-shortcuts">VScode Shortcuts</a></li> <li><a href="#vscode-git-status">VSCode git status</a></li> </ul> <p><br/></p> <h4 id="vscode-shortcuts">VScode Shortcuts</h4> <table> <thead> <tr> <th>Action</th> <th>Shortcut</th> </tr> </thead> <tbody> <tr> <td><strong>General</strong></td> <td> </td> </tr> <tr> <td>Open Command Palette</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + P</code></td> </tr> <tr> <td>Open Settings</td> <td><code class="language-plaintext highlighter-rouge">⌘ + ,</code></td> </tr> <tr> <td>Open Keyboard Shortcuts</td> <td><code class="language-plaintext highlighter-rouge">⌘ + K ⌘ + S</code></td> </tr> <tr> <td>Open Extensions View</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + X</code></td> </tr> <tr> <td>Show Integrated Terminal</td> <td><code class="language-plaintext highlighter-rouge">⌃ + \</code></td> </tr> <tr> <td>Close Window</td> <td><code class="language-plaintext highlighter-rouge">⌘ + W</code></td> </tr> <tr> <td>Quit VSCode</td> <td><code class="language-plaintext highlighter-rouge">⌘ + Q</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>GitHub Copilot</strong></td> <td> </td> </tr> <tr> <td>Accept Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Tab</code></td> </tr> <tr> <td>Next Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + ]</code></td> </tr> <tr> <td>Previous Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Option(Alt) + [</code></td> </tr> <tr> <td>Show Suggestions</td> <td><code class="language-plaintext highlighter-rouge">Ctrl + Enter</code></td> </tr> <tr> <td>Approve Suggestion</td> <td><code class="language-plaintext highlighter-rouge">Cmd + ←</code> or <code class="language-plaintext highlighter-rouge">Cmd + →</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>File Management</strong></td> <td> </td> </tr> <tr> <td>Open File</td> <td><code class="language-plaintext highlighter-rouge">⌘ + O</code></td> </tr> <tr> <td>Save File</td> <td><code class="language-plaintext highlighter-rouge">⌘ + S</code></td> </tr> <tr> <td>Save All Files</td> <td><code class="language-plaintext highlighter-rouge">⌥ + ⌘ + S</code></td> </tr> <tr> <td>Close Editor</td> <td><code class="language-plaintext highlighter-rouge">⌘ + W</code></td> </tr> <tr> <td>Reopen Closed Editor</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + T</code></td> </tr> <tr> <td>New File</td> <td><code class="language-plaintext highlighter-rouge">⌘ + N</code></td> </tr> <tr> <td>Open Recent Files</td> <td><code class="language-plaintext highlighter-rouge">⌘ + R</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Editing</strong></td> <td> </td> </tr> <tr> <td>Cut Line</td> <td><code class="language-plaintext highlighter-rouge">⌘ + X</code></td> </tr> <tr> <td>Copy Line</td> <td><code class="language-plaintext highlighter-rouge">⌘ + C</code></td> </tr> <tr> <td>Paste</td> <td><code class="language-plaintext highlighter-rouge">⌘ + V</code></td> </tr> <tr> <td>Delete Line</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + K</code></td> </tr> <tr> <td>Duplicate Line</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌥ + ↓</code> or <code class="language-plaintext highlighter-rouge">⇧ + ⌥ + ↑</code></td> </tr> <tr> <td>Move Line Up/Down</td> <td><code class="language-plaintext highlighter-rouge">⌥ + ↑</code> or <code class="language-plaintext highlighter-rouge">⌥ + ↓</code></td> </tr> <tr> <td>Indent Line</td> <td><code class="language-plaintext highlighter-rouge">⌘ + ]</code></td> </tr> <tr> <td>Outdent Line</td> <td><code class="language-plaintext highlighter-rouge">⌘ + [</code></td> </tr> <tr> <td>Comment Line</td> <td><code class="language-plaintext highlighter-rouge">⌘ + /</code></td> </tr> <tr> <td>Add Multi-Cursor</td> <td><code class="language-plaintext highlighter-rouge">⌥ + Click</code></td> </tr> <tr> <td>Select All Occurrences</td> <td><code class="language-plaintext highlighter-rouge">⌘ + ⇧ + L</code></td> </tr> <tr> <td>Expand Selection</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌥ + →</code></td> </tr> <tr> <td>Shrink Selection</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌥ + ←</code></td> </tr> <tr> <td>Format Document</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌥ + F</code></td> </tr> <tr> <td>Go to Matching Bracket</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + \</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Navigation</strong></td> <td> </td> </tr> <tr> <td>Go to File</td> <td><code class="language-plaintext highlighter-rouge">⌘ + P</code></td> </tr> <tr> <td>Go to Line</td> <td><code class="language-plaintext highlighter-rouge">⌃ + G</code></td> </tr> <tr> <td>Go to Definition</td> <td><code class="language-plaintext highlighter-rouge">F12</code></td> </tr> <tr> <td>Go to Implementation</td> <td><code class="language-plaintext highlighter-rouge">⌘ + F12</code></td> </tr> <tr> <td>Show References</td> <td><code class="language-plaintext highlighter-rouge">⇧ + F12</code></td> </tr> <tr> <td>Navigate Back</td> <td><code class="language-plaintext highlighter-rouge">⌃ + -</code></td> </tr> <tr> <td>Navigate Forward</td> <td><code class="language-plaintext highlighter-rouge">⌃ + ⇧ + -</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Search and Replace</strong></td> <td> </td> </tr> <tr> <td>Find</td> <td><code class="language-plaintext highlighter-rouge">⌘ + F</code></td> </tr> <tr> <td>Find in Files</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + F</code></td> </tr> <tr> <td>Replace</td> <td><code class="language-plaintext highlighter-rouge">⌘ + ⌥ + F</code></td> </tr> <tr> <td>Replace in Files</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + H</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Debugging</strong></td> <td> </td> </tr> <tr> <td>Start/Continue Debugging</td> <td><code class="language-plaintext highlighter-rouge">F5</code></td> </tr> <tr> <td>Step Over</td> <td><code class="language-plaintext highlighter-rouge">F10</code></td> </tr> <tr> <td>Step Into</td> <td><code class="language-plaintext highlighter-rouge">F11</code></td> </tr> <tr> <td>Step Out</td> <td><code class="language-plaintext highlighter-rouge">⇧ + F11</code></td> </tr> <tr> <td>Restart Debugging</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + F5</code></td> </tr> <tr> <td>Stop Debugging</td> <td><code class="language-plaintext highlighter-rouge">⇧ + F5</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Terminal</strong></td> <td> </td> </tr> <tr> <td>Create New Terminal</td> <td><code class="language-plaintext highlighter-rouge">⌘ + ⇧ + </code>`</td> </tr> <tr> <td>Split Terminal</td> <td><code class="language-plaintext highlighter-rouge">⌘ + \</code>`</td> </tr> <tr> <td>Kill Terminal</td> <td><code class="language-plaintext highlighter-rouge">⌘ + K</code> (inside terminal)</td> </tr> <tr> <td>Navigate Terminals</td> <td><code class="language-plaintext highlighter-rouge">⌃ + ←</code> or <code class="language-plaintext highlighter-rouge">⌃ + →</code></td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td><strong>Version Control</strong></td> <td> </td> </tr> <tr> <td>Open Source Control</td> <td><code class="language-plaintext highlighter-rouge">⇧ + ⌘ + G</code></td> </tr> <tr> <td>Commit Changes</td> <td><code class="language-plaintext highlighter-rouge">⌘ + Enter</code> (in source control)</td> </tr> </tbody> </table> <p><br/></p> <h4 id="vscode-git-status">VSCode git status</h4> <table> <thead> <tr> <th>Code</th> <th>Status</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>Added</td> <td>This is a new file that has been added to the repository</td> </tr> <tr> <td>M</td> <td>Modified</td> <td>An existing file has been changed</td> </tr> <tr> <td>D</td> <td>Deleted</td> <td>A file has been deleted</td> </tr> <tr> <td>U</td> <td>Untracked</td> <td>The file is new or has been changed but has not been added to the repository yet</td> </tr> <tr> <td>C</td> <td>Conflict</td> <td>There is a conflict in the file</td> </tr> <tr> <td>R</td> <td>Renamed</td> <td>The file has been renamed</td> </tr> <tr> <td>S</td> <td>Submodule</td> <td>In repository exists another subrepository</td> </tr> <tr> <td>T</td> <td>Typechange</td> <td>The file changed from symlink to regular file, or vice versa</td> </tr> </tbody> </table> ]]></content><author><name></name></author><category term="study"/><category term="tools"/><summary type="html"><![CDATA[💡익숙해진 단축키는 문서에서 삭제]]></summary></entry></feed>