---
layout: post
title: (ENG) DeepSeek-R1 Paper Review
date: 2024-12-29
description: 
tags: NLP papers
categories: study
featured: false
---

### Introduction

{% include figure.liquid loading="eager" path="assets/img/30.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

DeepSeek-R1-Zero and is the first generation reasoning model by DeepSeek AI trained via reinforcement learning without Supervised fine-tuning. Its primary limitations was that it had poor readability and language mixing. To address these issues, DeepSeek-R1 that utilizes multi-stage training and cold-start data before RL was introuduced in early 2025.

### Approach

```
Key features are GRPO, Cold Start,
```

#### Reinforcement Learning Algorithm

##### GRPO

To reduce the training cost of reinforcement learning (RL), a critic model of equivalent size to the policy model was omitted. Instead, Grouped Reinforcement Policy Optimization (GRPO) was used, which estimates the baseline using group scores.

GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº } from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:


$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim p(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O | q) \right]
$$

$$
\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)} A_i, \operatorname{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) \right)
$$

$$
\mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) = \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1
$$

where, 

- $$\epsilon$$ & $$\beta$$ = hyperparameters
- $$A_i$$ = advantage

The advantage is computed using a group of rewards {ğ‘Ÿ1, ğ‘Ÿ2, . . . , ğ‘Ÿğº } corresponding to the outputs within each group

$$ A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})} $$



##### Rewards

The reward decides the optimixation direction of RL. A rule-based reward system that mainly consists of two types of rewards is adopted.

- Accuracy Rewards
  - TL;DR : Evaluates whether response is correct
  - i.e. : For LeetCode problems, a compiler can be used to generate feedback
- Format Rewards
  - Forward-thinking model that enforces the model to place its thinking process between `<think>` tags

