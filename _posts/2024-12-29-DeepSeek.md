---
layout: post
title: (ENG) DeepSeek-R1 Paper Review
date: 2024-12-29
description: 
tags: NLP papers
categories: study
featured: false
---

### Introduction

```
Key Features : GRPO, Distillation
```

{% include figure.liquid loading="eager" path="assets/img/30.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

DeepSeek-R1-Zero and is the first generation reasoning model by DeepSeek AI trained via reinforcement learning without Supervised fine-tuning. Its primary limitations was that it had poor readability and language mixing. To address these issues, DeepSeek-R1 that utilizes multi-stage training and cold-start data before RL was introuduced in early 2025.

### Approach

```
Key features are GRPO, Cold Start,
```

#### Reinforcement Learning Algorithm

##### GRPO

To reduce the training cost of reinforcement learning (RL), a critic model of equivalent size to the policy model was omitted. Instead, Grouped Reinforcement Policy Optimization (GRPO) was used, which estimates the baseline using group scores.

GRPO samples a group of outputs $${𝑜_1, 𝑜_2, · · · , 𝑜_𝐺 }$$ from the old policy $$\pi \theta_old$$ and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:


$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim p(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O | q) \right]
$$

$$
\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)} A_i, \operatorname{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) \right)
$$

$$
\mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) = \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1
$$

where, 

- $$\epsilon$$ & $$\beta$$ = hyperparameters
- $$A_i$$ = advantage

The advantage is computed using a group of rewards {𝑟1, 𝑟2, . . . , 𝑟𝐺 } corresponding to the outputs within each group

$$ A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})} $$



##### Rule Based Reward System

The reward decides the optimixation direction of RL. A rule-based reward system that mainly consists of two types of rewards is adopted.

- Accuracy Rewards
  - TL;DR : Evaluates whether response is correct
  - i.e. : For LeetCode problems, a compiler can be used to generate feedback
- Format Rewards
  - TL;DR : Is the `<think>` tag appropriatelly annotated
  - Forward-thinking model that enforces the model to place its thinking process between `<think>` tags
  - 

##### Training Template


### Distillation
- encapsulates complex reasoning in smaller models for efficiency
- Large model trains a smaller model
- syntheic reasoning data

R1 Zero : only RL
R1 : multi stage traning

### Training Pipeline of DeepSeek-R1

{% include figure.liquid loading="eager" path="assets/img/31.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

1. initial SFT w/ high quality examples (→ ollected from DeepSeek-R1Zero)
2. RL focusing on reasoning tasks (coding, math, ...)
3. collection of new training data through rejection sampling & SFT correct and readable samples in more general domains
4. final RL across all task types: Rule-based reward or LLM feedback

SFT 없이 강화학습만 해도 추론능력 향상


{% include figure.liquid loading="eager" path="assets/img/32.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

### limitations
- general capability
- SE tasks
- Language Mixing in multilingual context


### 서울대학교 자연어처리 랩실 랩미팅 내용

- 중국어와 영어로 트레이닝이 되었다.
- 성능은?
  - Inference Test : llama 8b, qwen2의 경우 한국어 성능이 떨어짐 (토큰 단순반복, 등의 이슈 발생)
  - 중의적인지 알려줘 등, 한국어의 유형을 판별하는 프롬프트에서는 잘못된 답변을 낸다
  - 답변을 영어로 출력하는 경우도 있다.
- 연구실 작업에 사용 가능성
  - 모델을 사용할 수 없다면, API로 모델 output을 수집하여 랩실 LLM 성능 증강 (현재 OpenAI가 DeepSeek가 이런 방식으로 모델을 학습했다고 하는 것 아닌가...?)


OpenAI(OpenAI, 2024b)에서 inference time scaling을 사용해서 CoT reasoning processs의 길이를 늘리려는 시도.


### Personal Thoughts

