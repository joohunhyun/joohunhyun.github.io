---
layout: post
title: (ENG) DeepSeek-R1 Paper Review
date: 2024-12-29
description: 
tags: NLP papers
categories: study
featured: false
---

### Introduction

```
Key Features : GRPO, Distillation
```

{% include figure.liquid loading="eager" path="assets/img/30.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

DeepSeek-R1-Zero and is the first generation reasoning model by DeepSeek AI trained via reinforcement learning without Supervised fine-tuning. Its primary limitations was that it had poor readability and language mixing. To address these issues, DeepSeek-R1 that utilizes multi-stage training and cold-start data before RL was introuduced in early 2025.

### Approach

```
Key features are GRPO, Cold Start,
```

#### Reinforcement Learning Algorithm

##### GRPO

To reduce the training cost of reinforcement learning (RL), a critic model of equivalent size to the policy model was omitted. Instead, Grouped Reinforcement Policy Optimization (GRPO) was used, which estimates the baseline using group scores.

GRPO samples a group of outputs $${ğ‘œ_1, ğ‘œ_2, Â· Â· Â· , ğ‘œ_ğº }$$ from the old policy $$\pi \theta_old$$ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:


$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim p(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O | q) \right]
$$

$$
\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)} A_i, \operatorname{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) \right)
$$

$$
\mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) = \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1
$$

where, 

- $$\epsilon$$ & $$\beta$$ = hyperparameters
- $$A_i$$ = advantage

The advantage is computed using a group of rewards {ğ‘Ÿ1, ğ‘Ÿ2, . . . , ğ‘Ÿğº } corresponding to the outputs within each group

$$ A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})} $$



##### Rule Based Reward System

The reward decides the optimixation direction of RL. A rule-based reward system that mainly consists of two types of rewards is adopted.

- Accuracy Rewards
  - TL;DR : Evaluates whether response is correct
  - i.e. : For LeetCode problems, a compiler can be used to generate feedback
- Format Rewards
  - TL;DR : Is the `<think>` tag appropriatelly annotated
  - Forward-thinking model that enforces the model to place its thinking process between `<think>` tags
  - 

##### Training Template


### Distillation
- encapsulates complex reasoning in smaller models for efficiency
- Large model trains a smaller model
- syntheic reasoning data

R1 Zero : only RL
R1 : multi stage traning

### Training Pipeline of DeepSeek-R1

{% include figure.liquid loading="eager" path="assets/img/31.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

1. initial SFT w/ high quality examples (â†’ ollected from DeepSeek-R1î‚ˆZero)
2. RL focusing on reasoning tasks (coding, math, ...)
3. collection of new training data through rejection sampling & SFTî‚’ correct and readable samples in more general domains
4. final RL across all task types: Rule-based reward or LLM feedback

SFT ì—†ì´ ê°•í™”í•™ìŠµë§Œ í•´ë„ ì¶”ë¡ ëŠ¥ë ¥ í–¥ìƒ


{% include figure.liquid loading="eager" path="assets/img/32.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

### limitations
- general capability
- SE tasks
- Language Mixing in multilingual context


### ì„œìš¸ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ ë©ì‹¤ ë©ë¯¸íŒ… ë‚´ìš©

- ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¡œ íŠ¸ë ˆì´ë‹ì´ ë˜ì—ˆë‹¤.
- ì„±ëŠ¥ì€?
  - Inference Test : llama 8b, qwen2ì˜ ê²½ìš° í•œêµ­ì–´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ (í† í° ë‹¨ìˆœë°˜ë³µ, ë“±ì˜ ì´ìŠˆ ë°œìƒ)
  - ì¤‘ì˜ì ì¸ì§€ ì•Œë ¤ì¤˜ ë“±, í•œêµ­ì–´ì˜ ìœ í˜•ì„ íŒë³„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” ì˜ëª»ëœ ë‹µë³€ì„ ë‚¸ë‹¤
  - ë‹µë³€ì„ ì˜ì–´ë¡œ ì¶œë ¥í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤.
- ì—°êµ¬ì‹¤ ì‘ì—…ì— ì‚¬ìš© ê°€ëŠ¥ì„±
  - ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ë©´, APIë¡œ ëª¨ë¸ outputì„ ìˆ˜ì§‘í•˜ì—¬ ë©ì‹¤ LLM ì„±ëŠ¥ ì¦ê°• (í˜„ì¬ OpenAIê°€ DeepSeekê°€ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí–ˆë‹¤ê³  í•˜ëŠ” ê²ƒ ì•„ë‹Œê°€...?)


OpenAI(OpenAI, 2024b)ì—ì„œ inference time scalingì„ ì‚¬ìš©í•´ì„œ CoT reasoning processsì˜ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ë ¤ëŠ” ì‹œë„.


### Personal Thoughts

