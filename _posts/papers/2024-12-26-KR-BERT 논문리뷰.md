---
layout: distill
title: (작성중)(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model
date: 2024-12-26
description: 
tags: papers NLP
categories: study
featured: false
toc: true
---
- [1. 논문 요약](#1-논문-요약)
  - [1.1 Rationale](#11-rationale)
  - [1.2 Methodology 요약](#12-methodology-요약)
  - [1.3 Conclusion](#13-conclusion)
- [2. mBERT의 한계](#2-mbert의-한계)
    - [2.1 Corpus(말뭉치) Domain의 한계](#21-corpus말뭉치-domain의-한계)
    - [2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점](#22-한국어의-언어적-특성을-충분히-고려하지-않은-점)
    - [2.3 모델의 크기](#23-모델의-크기)
- [3. Methodology](#3-methodology)
  - [3.1 Subcharacter Text Representation](#31-subcharacter-text-representation)
  - [3.2 Subword Vocabulary](#32-subword-vocabulary)
- [4. Results](#4-results)
  - [Masked LM Accuracy](#masked-lm-accuracy)
- [5. Related Work / Benchmarks](#5-related-work--benchmarks)

---


### 1. 논문 요약

<br>

#### 1.1 Rationale
Multilingual-BERT(이하 mBERT)는 위키백과의 104개 언어로 된 문서들을 기반으로 학습된 모델이다. 그러나, 모든 언어의 언어적 특성을 학습시킬 수는 없기 때문에 비영어 다운스트림 작업(non-English downstream tasks)에는 정확도가 다소 떨어진 모습을 보인다. **Downstream tasks**란 자연어 처리(NLP)에서 사전 학습된 언어 모델을 활용하여 수행하는 특정 응용 작업 (예: 감정 분석, 개체명 인식, 문장 분류 등)을 의미한다.

한국어는 10,000개 이상의 문자(character)를 사용하고, 독어나 불어와 같은 굴절어보다 형태적으로 복잡하다. mBERT에서는 이 가운데 오직 1,187개의 문자만이 포함되었다. 또한, mBERT 모델은 104개의 언어 데이터를 포함하기에 크기가 과도하게 크다는 단점이 존재한다. 그래서, ALBERT나 Distil-BERT처럼 모델을 축소하면서도 성능을 유지하는 방법이 필요하다.

요약하자면: 

1. mBERT는 모델 크기가 과도하게 크다.
2. mBERT는 non-English downstream tasks에서 성능이 다소 떨어진다.
3. 모델을 축소하면서 성능을 유지하는 한국어 언어모델이 필요하다.

KR-BERT는 이런 점들을 해결하기 위해 고안된 새로운 한국어 언어모델이다.

<br>

#### 1.2 Methodology 요약


<br>

#### 1.3 Conclusion

- KR-BERT는 다음의 downstream tasks들에서 mBERT의 성능을 능가했다.
  - senti- ment analysis
  - Question-Answering
  - Named Entity Recognition(NER)
  - Paraphrase Detection
- KR-BERT는 다른 한국어 모델들에 비해 다운스트림 작업에서 더 좋은 성능을 보이거나 비슷한 성과를 보였다.
- 형태소가 많은 한국어에 맞게 서브-캐릭터 기반 모델과 Bidirectional-WordPiece 토크나이저를 사용하여 적은 자원으로도 효과적인 성능을 달성했다.

<br>
<br>


### 2. mBERT의 한계

<br>

##### 2.1 Corpus(말뭉치) Domain의 한계
GCamemBERT와 같은 언어별 특화된(Language Specific) BERT 모델들은 법률 데이터, 뉴스 기사 등, 다양한 데이터 소스를 사용하여 학습된 반면, mBERT는 유저들이 직접 작성한 데이터 소스(블로그 글, 댓글, 등)를 사용하지 않고, 위키백과 포스트들의 언어적 특성을 기반으로만 학습되어 언어 사용(limited in its domain with respect to language usage - 직역하면 언어 사용이지만 어휘력일 것이라고 생각됨) 측면에서 한계가 존재한다.

<br>

##### 2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점

2.2.1 Rare Character Problem

라틴 문자 기반 언어는 단어를 문자 단위로 분리하여 처리할 수 있는 반면, 한국어의 언어적 특성상 음절 단위로 처리되어 Out-of-Vocabulary(OoV)가 라틴 문자 기반 언어보다 많을수밖에 없다. 그럼으로, 이런 한국어의 특성을 고려한 새로운 BERT Vocabulary가 필요하다.

2.2.2 교착언어에 적합하지 않은 모델

교착어(agglutinative language)란 문법적 의미를 전달하기 위해 주로 접사를 단단히 결합하는 언어를 의미한다. 교착어는 형태론적인 복잡성(morphological complexity) 때문에 vocabulary를 표현하기 어렵다. 한국어 역시 교착언어에 해당하며, mBERT는 이 문제를 제대로 처리하지 않은 것으로 보인다.

2.2.3 Lack of Meaningful Tokens

독일어의 경우, mBERT 모델의 어휘에 명확한 semantic meaning이 없는 subword unit이 포함되어 있었다. 한국어에서도 이와 유사한 문제가 존재했는데, 대부분의 단어가 형태소와 같은 단위가 아닌 단일 문자로 tokenized 되었기 떄문이다. 이 문제를 해결하기 위해, 한국어 텍스트와 한국어의 언어적 특성을 고려한 vocabulary 및 tokenizer를 구현했다.

<br>

##### 2.3 모델의 크기
대형 모델들은 많은 양의 dataset, parameter, voabulary를 필요로 한다. mBERT는 167M, RoBERTa는 355M 파라미터를 사용해 자원의 제약을 받기 마련인데, KR-BERT는 적은 parameter와 훈련 데이터를 사용하면서 mBERT와 비슷한 성능을 유지할 수 있었다.

<br>
<br>


### 3. Methodology


KR-BERT를 Multilingual BERT, Kor- BERT, 그리고 KoBERT와 비교했다.

<br>

#### 3.1 Subcharacter Text Representation

한국어 텍스트는 Text-> 한글 -> graphemes의 형태로 분해 가능하다. 이런 특성을 반영하기 위해, KR-BERT는 syllable character와 sub-character 두 가지 말뭉치 표현 방식을 사용하여 new vocabulary와 BERT model을 학습시켰다. BPE 알고리즘을 적용하면, 다음과 같은 tokenization이 가능해진다.

`Example : 뜀(ttwim, "jumping")은 ㄸㅟ(ttwi, "jump")와 ᄆ(m, "-ing")으로 분해될 수 있다`

{% include figure.liquid loading="eager" path="assets/img/15.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

KR-BERT 모델은 나머지 벤치마크 모델들과 달리, 음절 문자 이외에도 하위 문자 표현을 사용해 다양한 한국어 동사/형용사의 공통된 특성을 학습할 수 있었다.

<br>

#### 3.2 Subword Vocabulary

### 4. Results

#### Masked LM Accuracy

### 5. Related Work / Benchmarks

{% include figure.liquid loading="eager" path="assets/img/13.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="assets/img/14.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

