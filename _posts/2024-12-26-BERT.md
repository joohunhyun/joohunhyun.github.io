---
layout: post
title: (KOR) KR-BERT
date: 2024-12-26
description: 
tags: DL, NLP
categories: study
featured: false
---
**Table of Contents**

---


### 1. Introduction, Conclusion 요약

#### Rationale
Multilingual-BERT(이하 mBERT)는 위키백과의 104개 언어로 된 문서들을 기반으로 학습된 모델이다. 그러나, 모든 언어의 언어적 특성을 학습시킬 수는 없기 때문에 *비영어 다운스트림 작업(non-English downstream tasks)에는 정확도가 다소 떨어진 모습을 보인다. 한국어는 만개 이상의 문자(character)를 사용하고, 독어나 불어와 같은 굴절어보다 형태적으로 복잡하다. mBERT에서는 이 가운데 오직 1,187개의 문자만이 포함되었다. 또한, mBERT 모델은 104개의 언어 데이터를 포함하기에 크기가 과도하게 크다는 단점이 존재한다. 그래서, ALBERT나 DistilBERT처럼 모델을 축소하면서도 성능을 유지하는 방법이 필요하다.


요약하자면, KR-BERT는 : 

1. mBERT는 모델 크기가 과도하게 크다.
2. mBERT는 non-English downstream tasks에서 성능이 다소 떨어진다.
3. 모델을 축소하면서 성능을 유지하는 한국어 언어모델이 필요하다.

라는 점들을 해결하기 위해 고안된 새로운 한국어 언어모델이다.

*Downstream tasks란 자연어 처리(NLP)에서 사전 학습된 언어 모델을 활용하여 수행하는 특정 응용 작업 (예: 감정 분석, 개체명 인식, 문장 분류 등)을 의미한다.


#### Conclusion

- KR-BERT는 mBERT보다 좋은 성능을 보이면서 한국어의 언어적 특성을 잘 반영했다.
- KR-BERT는 다른 한국어 모델들에 비해 다운스트림 작업에서 더 좋은 성능을 보이거나 비슷한 성과를 보였다.
- 형태소가 많은 한국어에 맞게 서브-캐릭터 기반 모델과 Bidirectional-WordPiece 토크나이저를 도입하여, 적은 자원으로도 효과적인 성능을 달성했다.

### Related Work / Benchmarks

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/13.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>

</div>

</div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/14.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
