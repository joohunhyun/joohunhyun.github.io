---
layout: post
title: (KOR) KR-BERT:A Small-Scale Korean-Specific Language Model
date: 2024-12-26
description: 
tags: papers, NLP
categories: study
featured: false
---
- [1. Introduction, Conclusion 요약](#1-introduction-conclusion-요약)
  - [1.1 Rationale](#11-rationale)
  - [1.2 Methodology 요약](#12-methodology-요약)
  - [1.3 Conclusion](#13-conclusion)
- [2. mBERT의 한계](#2-mbert의-한계)
    - [2.1 Corpus(말뭉치) Domain의 한계](#21-corpus말뭉치-domain의-한계)
    - [2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점](#22-한국어의-언어적-특성을-충분히-고려하지-않은-점)
    - [2.3 모델의 크기](#23-모델의-크기)
- [Related Work / Benchmarks](#related-work--benchmarks)


---


### 1. Introduction, Conclusion 요약

#### 1.1 Rationale
Multilingual-BERT(이하 mBERT)는 위키백과의 104개 언어로 된 문서들을 기반으로 학습된 모델이다. 그러나, 모든 언어의 언어적 특성을 학습시킬 수는 없기 때문에 *비영어 다운스트림 작업(non-English downstream tasks)에는 정확도가 다소 떨어진 모습을 보인다. 한국어는 만개 이상의 문자(character)를 사용하고, 독어나 불어와 같은 굴절어보다 형태적으로 복잡하다. mBERT에서는 이 가운데 오직 1,187개의 문자만이 포함되었다. 또한, mBERT 모델은 104개의 언어 데이터를 포함하기에 크기가 과도하게 크다는 단점이 존재한다. 그래서, ALBERT나 DistilBERT처럼 모델을 축소하면서도 성능을 유지하는 방법이 필요하다.


요약하자면, KR-BERT는 : 

1. mBERT는 모델 크기가 과도하게 크다.
2. mBERT는 non-English downstream tasks에서 성능이 다소 떨어진다.
3. 모델을 축소하면서 성능을 유지하는 한국어 언어모델이 필요하다.

라는 점들을 해결하기 위해 고안된 새로운 한국어 언어모델이다.

*Downstream tasks란 자연어 처리(NLP)에서 사전 학습된 언어 모델을 활용하여 수행하는 특정 응용 작업 (예: 감정 분석, 개체명 인식, 문장 분류 등)을 의미한다.

#### 1.2 Methodology 요약



#### 1.3 Conclusion

- KR-BERT는 다음의 downstream tasks들에서 mBERT의 성능을 능가했다.
  - senti- ment analysis
  - Question-Answering
  - Named Entity Recognition(NER)
  - Paraphrase Detection
- KR-BERT는 다른 한국어 모델들에 비해 다운스트림 작업에서 더 좋은 성능을 보이거나 비슷한 성과를 보였다.
- 형태소가 많은 한국어에 맞게 서브-캐릭터 기반 모델과 Bidirectional-WordPiece 토크나이저를 사용하여 적은 자원으로도 효과적인 성능을 달성했다.



### 2. mBERT의 한계

##### 2.1 Corpus(말뭉치) Domain의 한계
GCamemBERT와 같은 언어별 특화된(Language Specific) BERT 모델들은 법률 데이터, 뉴스 기사 등, 다양한 데이터 소스를 사용하여 학습된 반면, mBERT는 유저들이 직접 작성한 데이터 소스(블로그 글, 댓글, 등)를 사용하지 않고, 위키백과 포스트들의 언어적 특성을 기반으로만 학습되어 언어 사용(limited in its domain with respect to language usage - 직역하면 언어 사용이지만 어휘력일 것이라고 생각됨) 측면에서 한계가 존재한다.

##### 2.2 한국어의 언어적 특성을 충분히 고려하지 않은 점

2.2.1 Rare Character Problem

라틴 문자 기반 언어는 단어를 문자 단위로 분리하여 처리할 수 있는 반면, 한국어의 언어적 특성상 음절 단위로 처리되어 Out-of-Vocabulary(OoV)가 라틴 문자 기반 언어보다 많을수밖에 없다. 그럼으로, 이런 한국어의 특성을 고려한 새로운 BERT Vocabulary가 필요하다.

2.2.2 교착언어

형태소가 풍부한 언어에 대한 부적합성: 

한국어는 agglutinative language*임으로, 하나의 


한국어는 교착어로, 하나의 접미사에 여러 의미를 담을 수 있어 형태소 처리가 복잡합니다. 특히 동사의 활용 형태가 많아 다국어 BERT 모델에서 제대로 처리되지 않습니다.

교착어(agglutinative language)*란 

문법적 의미를 전달하기 위해 주로 접사를 단단히 결합하는 언어를 말합니다. 교착어는 주로 접미사나 접두사 형태로 여러 가지 문법적 요소(시제, 존댓말, 격 등)를 한 단어에 추가하는 특징이 있습니다. 각 접사는 그 자체로 하나의 명확한 의미를 가지고 있으며, 결합된 접사들은 원형 단어에 하나씩 붙어가며 의미를 점진적으로 확장합니다.

예를 들어, 한국어와 터키어는 대표적인 교착어입니다. 한국어에서는 동사나 형용사 뒤에 접미사를 붙여 시제, 존경 표현, 부정 등을 나타냅니다. 예를 들어:

"가다" (to go) + "-는" (present tense) → "가는" (going)
"가다" (to go) + "-았" (past tense) + "-다" (to finish) → "갔다" (went)


반면, **굴절어 (Inflectional Language)**는 단어 형태 변화가 한 번에 여러 의미를 복합적으로 표현한다.



2.2.3

의미 있는 토큰 부족: 한국어에서는 대부분의 단어가 단일 문자로 토크나이즈되어, 의미를 제대로 표현하지 못하는 문제가 발생합니다.


##### 2.3 모델의 크기
대형 모델들은 많은 양의 dataset, parameter, voabulary를 필요로 한다. mBERT는 167M, RoBERTa는 355M 파라미터를 사용해 자원의 제약을 받기 마련인데, KR-BERT는 적은 parameter와 훈련 데이터를 사용하면서 mBERT와 비슷한 성능을 유지할 수 있었다.



### Related Work / Benchmarks

{% include figure.liquid loading="eager" path="assets/img/13.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="assets/img/14.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

