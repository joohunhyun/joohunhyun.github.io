<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> (ENG) DeepSeek-R1 Paper Review | JooHun Hyun </title> <meta name="author" content="JooHun Hyun"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joohunhyun.github.io/blog/2024/DeepSeek/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "(ENG) DeepSeek-R1 Paper Review",
            "description": "",
            "published": "December 29, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">JooHun</span> Hyun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>(ENG) DeepSeek-R1 Paper Review</h1> <p></p> </d-title> <d-article> <p><strong>Table of Contents</strong></p> <ul> <li><a href="#1-introduction">1. Introduction</a></li> <li> <a href="#2-approach">2. Approach</a> <ul> <li> <a href="#21-reinforcement-learning-algorithm-on-the-base-model-r-1-zero">2.1 Reinforcement Learning Algorithm on the Base Model (R-1 Zero)</a> <ul> <li><a href="#grpo">GRPO</a></li> <li><a href="#rule-based-reward-system">Rule Based Reward System</a></li> <li><a href="#training-template">Training Template</a></li> <li><a href="#performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</a></li> <li><a href="#aha-moment-of-deepseek-r1-zero">Aha moment of DeepSeek-R1-Zero</a></li> </ul> </li> <li> <a href="#21-reinforcement-learning-with-cold-start">2.1 Reinforcement Learning with Cold Start</a> <ul> <li><a href="#stage-1-cold-start">Stage 1: Cold Start</a></li> <li><a href="#stage-2-reasoning-oriented-reinforcement-learning">Stage 2: Reasoning-oriented Reinforcement Learning</a></li> <li><a href="#stage-3-rejection-sampling-and-supervised-fine-tuning">Stage 3: Rejection Sampling and Supervised Fine-Tuning</a></li> <li><a href="#stage-4-reinforcement-learning-for-all-scenarios">Stage 4: Reinforcement Learning for All Scenarios</a></li> </ul> </li> </ul> </li> <li><a href="#3-distillation">3. Distillation</a></li> <li> <a href="#4-experiment">4. Experiment</a> <ul> <li><a href="#evaluation-prompts--setup">Evaluation Prompts \&amp; Setup</a></li> <li><a href="#baselines">Baselines</a></li> <li><a href="#generation-constraints">Generation Constraints</a></li> </ul> </li> <li> <a href="#5-discussion-unsuccessful-attepts">5. Discussion (Unsuccessful Attepts)</a> <ul> <li><a href="#process-reward-model-prm">Process Reward Model (PRM)</a></li> <li><a href="#monte-carlo-tree-search-mcts">Monte Carlo Tree Search (MCTS)</a></li> </ul> </li> <li> <a href="#6-%EC%84%9C%EC%9A%B8%EB%8C%80%ED%95%99%EA%B5%90-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EB%9E%A9%EC%8B%A4-%EB%9E%A9%EB%AF%B8%ED%8C%85-%EB%82%B4%EC%9A%A9">6. 서울대학교 자연어처리 랩실 랩미팅 내용</a> <ul> <li><a href="#%ED%8E%98%EC%9D%B4%ED%8D%BC-%EB%A6%AC%EB%B7%B0-%EC%9A%94%EC%95%BD">페이퍼 리뷰 요약</a></li> <li><a href="#training-pipeline-of-deepseek-r1">Training Pipeline of DeepSeek-R1</a></li> <li><a href="#limitations">limitations</a></li> </ul> </li> </ul> <hr> <h2 id="1-introduction">1. Introduction</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Key Features : RL via GRPO algorithm, "aha" moment, Distillation, limitations of the MCTS and PRM in training R1
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/30-480.webp 480w,/assets/img/30-800.webp 800w,/assets/img/30-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>DeepSeek-R1-Zero and is the first generation reasoning model by DeepSeek AI trained via reinforcement learning without Supervised fine-tuning. Its primary limitations was that it had poor readability and language mixing<d-footnote>Language Mixing : the practice of using two or more languages in the same conversation or sentence.</d-footnote>. To address these issues, DeepSeek-R1 was introuduced in early 2025, that utilizes multi-stage training and cold-start data before RL.</p> <h2 id="2-approach">2. Approach</h2> <h3 id="21-reinforcement-learning-algorithm-on-the-base-model-r-1-zero">2.1 Reinforcement Learning Algorithm on the Base Model (R-1 Zero)</h3> <h4 id="grpo">GRPO</h4> <p>To reduce the training cost of reinforcement learning (RL), a critic model of equivalent size to the policy model was omitted. Instead, Grouped Reinforcement Policy Optimization (GRPO) was used, which estimates the baseline using group scores.</p> <p>GRPO samples a group of outputs (\({𝑜_1, 𝑜_2, · · · , 𝑜_𝐺 }\)) from the old policy \((\pi \theta_{\text{old}}\))and then optimizes the policy model \(\pi \theta\) by maximizing the following objective:</p> \[\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim p(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O | q) \right]\] \[\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)} A_i, \operatorname{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) \right)\] \[\mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) = \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1\] <p>where,</p> <ul> <li>\(\epsilon\) &amp; \(\beta\) are <strong>hyperparameters</strong> </li> <li>\(A_i\) is the <strong>advantage</strong> </li> </ul> <p>The advantage is computed using a group of rewards (\({𝑟_1, 𝑟_2, . . . , 𝑟_G }\)) corresponding to the outputs within each group.</p> \[A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})}\] <h4 id="rule-based-reward-system">Rule Based Reward System</h4> <p>The reward decides the optimization direction of the RL. A rule-based reward system that mainly consists of two types of rewards is adopted.</p> <ul> <li>Accuracy Rewards <ul> <li> <strong>TL;DR</strong> : Evaluates whether response is correct</li> <li>i.e. : For LeetCode problems, a compiler can be used to generate feedback</li> </ul> </li> <li>Format Rewards <ul> <li> <strong>TL;DR</strong> : Is the <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tag appropriatelly annotated</li> <li>Forward-thinking model that enforces the model to place its thinking process between <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags</li> </ul> </li> </ul> <h4 id="training-template">Training Template</h4> <p>This is the training template for DeepSeek-R1-Zero. The <code class="language-plaintext highlighter-rouge">prompt</code> portion is replaced with the question for the actual training.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt;
&lt;answer&gt; answer here &lt;/answer&gt;. User: prompt. Assistant:
</code></pre></div></div> <h4 id="performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</h4> <p>The following figure depicts the performance trajectory of the R-1 Zero model on the AIME 2024 Benchmark throught the RL training process.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/30-480.webp 480w,/assets/img/30-800.webp 800w,/assets/img/30-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The following is a brief explanation of each benchmark:</p> <ul> <li> <p><strong>AIME 2024</strong>: The American Invitational Mathematics Examination (AIME) is a 15-question, 3-hour test focusing on advanced high school mathematics, including algebra, geometry, and number theory. It serves as a qualifier for the United States Mathematical Olympiad. <a href="https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>MATH-500</strong>: This dataset comprises 500 problems from the MATH benchmark, covering various topics such as algebra, calculus, and probability. It is designed to evaluate mathematical reasoning and problem-solving abilities of AI models. <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>GPQA Diamond</strong>: The Graduate-Level Google-Proof Q&amp;A Benchmark (GPQA) is a challenging dataset of 448 multiple-choice questions in biology, physics, and chemistry, crafted by domain experts. The “Diamond” subset represents the most difficult questions, assessing the advanced reasoning capabilities of AI models. <a href="https://arxiv.org/abs/2311.12022" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>LiveCodeBench</strong>: LiveCodeBench is a benchmark designed for evaluating large language models’ coding capabilities. It continuously collects new problems from platforms like LeetCode, AtCoder, and CodeForces, and assesses various aspects such as code generation, execution, and self-repair. <a href="https://github.com/LiveCodeBench/LiveCodeBench" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>CodeForces</strong>: CodeForces is a competitive programming platform that hosts regular contests where participants solve algorithmic problems under time constraints. It serves as a benchmark for evaluating coding and problem-solving skills, with a rating system to rank participants. <a href="https://codeforces.com/blog/entry/22260" rel="external nofollow noopener" target="_blank">link</a></p> </li> </ul> <h4 id="aha-moment-of-deepseek-r1-zero">Aha moment of DeepSeek-R1-Zero</h4> <p>The significance of the “aha moment” is that through RL, the model autonomously develops advanced problem-solving strategies, given the right incentives. The model learns to rethink using an anthropomorphic tone(aha!).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/32-480.webp 480w,/assets/img/32-800.webp 800w,/assets/img/32-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="21-reinforcement-learning-with-cold-start">2.1 Reinforcement Learning with Cold Start</h3> <p>Two questions arise by the results of DeepSeek-R1-Zero.</p> <ol> <li>Can reasoning performance be further improved or can convergence be accelerated by incorporating a small amount of high-quality data as a cold start?</li> <li>How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities</li> </ol> <p>The pipeline of DeepSeek-R1 attempts to address these questions. The pipeline consists of four stages, outlined as follows(stages 1~4).</p> <h4 id="stage-1-cold-start">Stage 1: Cold Start</h4> <p>To prevent instability in the early phases of RL training, DeepSeek-R1 incorporates a cold start phase by fine-tuning the base model with a small dataset of high-quality long CoT data. This dataset is constructed through multiple approaches, including:</p> <ul> <li> <strong>Few-shot prompting</strong> with long CoT examples.</li> <li> <strong>Direct prompting</strong> for detailed answers with reflection and verification.</li> <li> <strong>Extracting readable outputs</strong> from DeepSeek-R1-Zero.</li> <li> <strong>Human post-processing</strong> to refine results.</li> </ul> <p>By introducing cold-start data, DeepSeek-R1 achieves improved readability<d-footnote>Readability : How easy, accessible, or comprehensible that text is (가독성)</d-footnote> and performance over DeepSeek-R1-Zero. The structured format includes a reasoning process and a summary, ensuring clear and coherent outputs.</p> <h4 id="stage-2-reasoning-oriented-reinforcement-learning">Stage 2: Reasoning-oriented Reinforcement Learning</h4> <p>After fine-tuning DeepSeek-V3-Base on cold-start data, a large-scale reinforcement learning (RL) process similar to DeepSeek-R1-Zero was applied. This phase enhances the model’s reasoning abilities, particularly in coding, mathematics, science, and logical reasoning—domains with well-defined problems and clear solutions.</p> <p>One challenge observed during training is language mixing in CoT responses, especially when RL prompts involve multiple languages. To address a language consistency reward was implemented, which measures the proportion of target-language words in the response. While ablation studies indicated a minor drop in performance, this alignment improved readability and human preference. The final reward combines reasoning accuracy and language consistency, and RL training continues until convergence.</p> <h4 id="stage-3-rejection-sampling-and-supervised-fine-tuning">Stage 3: Rejection Sampling and Supervised Fine-Tuning</h4> <p>Once reasoning-oriented RL converges, the checkpoint is used to generate Supervised Fine-Tuning (SFT) data for the next training phase. Unlike the cold-start data, which primarily targets reasoning, this stage incorporates diverse data types to improve DeepSeek-R1’s general-purpose capabilities.</p> <ul> <li> <p>Reasoning Data: Prompts are curated, and multiple reasoning trajectories are generated using rejection sampling. Unlike earlier phases that relied solely on rule-based rewards, this stage integrates generative reward models by comparing model outputs with ground-truth data using DeepSeek-V3. Additionally, unreadable outputs with mixed languages, excessive length, or unnecessary code blocks are filtered out. Ultimately, this process yields 600k high-quality reasoning samples.</p> </li> <li> <p>Non-Reasoning Data: To enhance tasks such as writing, factual QA, self-cognition, and translation, 200k additional training samples are incorporated from DeepSeek-V3’s SFT dataset. For complex queries, an intermediate CoT is generated before answering, while for simple queries (e.g., “hello”), CoT is omitted.</p> </li> </ul> <p>Overall, DeepSeek-V3-Base is fine-tuned for two epochs using an 800k-sample dataset.</p> <h4 id="stage-4-reinforcement-learning-for-all-scenarios">Stage 4: Reinforcement Learning for All Scenarios</h4> <p>To further align the model with human preferences, a secondary RL stage is introduced, refining both reasoning capabilities and alignment with user expectations. This phase integrates reward signals and diverse prompt distributions:</p> <ul> <li>Reasoning Data: Rule-based rewards, as in DeepSeek-R1-Zero, are applied to reinforce skills in mathematics, coding, and logical reasoning.</li> <li> <p>General Data: A reward model evaluates helpfulness and harmlessness in open-ended tasks, following the DeepSeek-V3 preference pipeline.</p> </li> <li>Helpfulness: The reward model assesses only the final summary, ensuring it is useful and relevant while preserving reasoning integrity.</li> <li>Harmlessness: The entire response, including both the reasoning process and summary, is reviewed to mitigate potential risks, biases, or harmful content.</li> </ul> <p>By combining reinforcement learning, reward models, and diverse data sources, DeepSeek-R1 is trained to excel in reasoning while maintaining strong alignment with human preferences.</p> <h2 id="3-distillation">3. Distillation</h2> <p>To equip smaller models with reasoning capabilities similar to DeepSeek-R1, open-source models like Qwen and Llama was fine tuned using the 800k curated samples from stage 3 of the pipeline.</p> <p>The base models used include:</p> <ul> <li>Qwen2.5: Math-1.5B, Math-7B, 14B, 32B</li> <li>Llama: 3.1-8B, 3.3-70B-Instruct (chosen for its superior reasoning ability over Llama-3.1)</li> </ul> <p>Unlike DeepSeek-R1, distilled models undergo only <strong>SFT</strong><d-footnote>Supervised Fine Tuning : 정답에 해당하는 답안을 제시하고 이를 학습시키는 지도학습 방법론. 학습 속도가 빠르고, 계산량이 상대적으로 적다. </d-footnote> and RL methodologies are not applied.</p> <h2 id="4-experiment">4. Experiment</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/35-480.webp 480w,/assets/img/35-800.webp 800w,/assets/img/35-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The “flagship model” DeepSeek-R1 was evaluated using a range of benchmarks covering math, coding, factual knowledge, and reasoning, including:</p> <ul> <li>Math &amp; Reasoning: AIME 2024, MATH-500, GPQA Diamond, CNMO 2024</li> <li>Coding: LiveCodeBench, Codeforces, SWE-Bench Verified, Aider 1</li> <li>General Knowledge &amp; QA: MMLU, MMLU-Redux, MMLU-Pro, C-Eval, CMMLU, SimpleQA, C-SimpleQA, IFEval, FRAMES</li> <li>LLM-based Open-Ended Generation: AlpacaEval 2.0, Arena-Hard</li> </ul> <p>For distilled R1 models, the results was reported on the AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.</p> <h3 id="evaluation-prompts--setup">Evaluation Prompts &amp; Setup</h3> <ul> <li>Benchmarks use standard prompt formats (e.g., simple-evals for MMLU, Zero-Eval for MMLU-Redux).</li> <li>Few-shot CoT prompts are modified to zero-shot, as CoT in few-shot settings may hurt DeepSeek-R1 performance.</li> <li>LiveCodeBench uses CoT-based evaluation on data collected between Aug 2024 – Jan 2025.</li> <li>Codeforces evaluation includes 10 Div.2 contests with expert-crafted test cases.</li> </ul> <p>For long-output reasoning models, we use pass@k evaluation to reduce variability, with pass@1 calculated using temperature 0.6 and top-p 0.95. AIME 2024 additionally reports cons@64 (majority vote results).</p> <h3 id="baselines">Baselines</h3> <p>DeepSeek-R1 is compared against strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, OpenAI-o1-1217, and for distilled models, QwQ-32B-Preview.</p> <h3 id="generation-constraints">Generation Constraints</h3> <ul> <li>Maximum token length: 32,768</li> <li>Pass@k evaluation with 4 to 64 samples per query to ensure stable performance estimates.</li> </ul> <h2 id="5-discussion-unsuccessful-attepts">5. Discussion (Unsuccessful Attepts)</h2> <p>During the development of DeepSeek-R1, Process Reward Models (PRM) and Monte Carlo Tree Search (MCTS) were unsuccessful in training.</p> <h4 id="process-reward-model-prm">Process Reward Model (PRM)</h4> <p>PRM aims to guide the model toward better reasoning by rewarding correct intermediate steps. However, it has three major limitations:</p> <ol> <li>Defining fine-grained<d-footnote>Fine-Grained Approach : 작업을 큰 단위 안에서도 세분화된 작업 프로세스를 만들어서 실행하는 방식</d-footnote> reasoning steps is difficult.</li> <li>Determining the correctness of an intermediate step is challenging, as automated annotations are unreliable and manual annotation does not scale.</li> <li>PRM introduces reward hacking, requiring frequent retraining and additional resources.</li> </ol> <p>While PRM can be effective in reranking responses or guided searches, its computational overhead outweighs its benefits in large-scale reinforcement learning.</p> <h4 id="monte-carlo-tree-search-mcts">Monte Carlo Tree Search (MCTS)</h4> <p>Inspired by AlphaGo and AlphaZero, MCTS was tested as a method to enhance reasoning by systematically exploring solution paths. The process involved tagging reasoning steps, using a value model to guide searches, and iteratively refining the training process. However, two major challenges emerged:</p> <ol> <li>The search space for token generation is exponentially larger than structured games like chess, making it difficult to scale effectively.</li> <li>The value model plays a crucial role in search quality, but training a reliable fine-grained value model is inherently difficult.</li> </ol> <p>While MCTS can improve inference when used with a pre-trained value model, iteratively boosting model performance through self-search remains a challenge.</p> <h2 id="6-서울대학교-자연어처리-랩실-랩미팅-내용">6. 서울대학교 자연어처리 랩실 랩미팅 내용</h2> <h3 id="페이퍼-리뷰-요약">페이퍼 리뷰 요약</h3> <ul> <li>딥시크 모델은 중국어와 영어로 트레이닝이 되었다.</li> <li>한국어 성능은? <ul> <li>Inference Test : llama 8b, qwen2의 경우 한국어 성능이 떨어짐 (토큰 단순반복, 등의 이슈 발생)</li> <li>중의적인지 알려줘 등, 한국어의 유형을 판별하는 프롬프트에서도 잘못된 approach를 보여줌</li> <li>답변을 영어로 출력하는 경우도 있다.</li> </ul> </li> <li>연구실 작업에 사용 가능성 <ul> <li>모델을 사용할 수 없다면, API로 모델 output을 수집하여 랩실 LLM 성능 증강 (현재 OpenAI가 DeepSeek가 이런 방식으로 모델을 학습했다고 하는 것 아닌가…?)</li> </ul> </li> </ul> <h3 id="training-pipeline-of-deepseek-r1">Training Pipeline of DeepSeek-R1</h3> <ol> <li>initial SFT w/ high quality examples → collected from DeepSeek-R1 Zero</li> <li>RL focusing on reasoning tasks (coding, math, …)</li> <li>collection of new training data through rejection sampling &amp; SFT correct and readable samples in more general domains</li> <li>final RL across all task types: Rule-based reward or LLM feedback</li> </ol> <p>SFT 없이 강화학습만 해도 추론능력을 향상시킬 수 있다는 점이 이번 페이퍼의 key finding.</p> <h3 id="limitations">limitations</h3> <ul> <li>general capability</li> <li>SE tasks</li> <li>Language Mixing in multilingual context</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 JooHun Hyun. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-projects",title:"projects",description:"An archive of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-kor-github-al-folio-\uc124\uce58\ubd80\ud130-\ube14\ub85c\uadf8-\ud14c\ub9c8-\ubcc0\uacbd\uae4c\uc9c0",title:"(KOR) Github al-folio \uc124\uce58\ubd80\ud130 \ube14\ub85c\uadf8 \ud14c\ub9c8 \ubcc0\uacbd\uae4c\uc9c0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Al-Folio-%EC%84%A4%EC%B9%98-%EB%B0%A9%EB%B2%95,-%EC%82%AC%EC%9A%A9%EB%B2%95,-%EC%BB%A4%EC%8A%A4%ED%85%80-%EC%82%AC%EC%9A%A9/"}},{id:"post-eng-deepseek-r1-paper-review",title:"(ENG) DeepSeek-R1 Paper Review",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DeepSeek/"}},{id:"post-eng-autogpt-installation",title:"(ENG) AutoGPT Installation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/AutoGPT/"}},{id:"post-kor-ai-agents-\uc2a4\ud130\ub514-\uc790\ub8cc-snu-cl-lab",title:"(KOR) AI Agents \uc2a4\ud130\ub514 \uc790\ub8cc[SNU CL LAB]",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/AI-Agents/"}},{id:"post-eng-cs224n-lecture-1",title:"(ENG) CS224n - Lecture 1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs224n-1%EA%B0%95/"}},{id:"post-kor-kr-bert-a-small-scale-korean-specific-language-model",title:"(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/KR-BERT-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"}},{id:"post-eng-feature-selection-extraction-and-ensamble-methods",title:"(ENG) Feature Selection, Extraction, and Ensamble Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/feature/"}},{id:"post-eng-training-and-evaluating-deep-networks",title:"(ENG) Training and Evaluating Deep Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"post-eng-rnn-and-lstm",title:"(ENG) RNN and LSTM",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/RNN/"}},{id:"post-eng-mlp-and-dnn",title:"(ENG) MLP and DNN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/MLP-DNN/"}},{id:"post-kor-\uc5f0\uad6c\uc2e4-\uc548\uc804\uad50\uc721-\ubc30\uc18d\ud558\ub294-\ubc29\ubc95",title:"(KOR) \uc5f0\uad6c\uc2e4 \uc548\uc804\uad50\uc721 \ubc30\uc18d\ud558\ub294 \ubc29\ubc95",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/skip-safety-skip-copy/"}},{id:"post-eng-svm-svc",title:"(ENG) SVM, SVC",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/SVM-SVC/"}},{id:"post-kor-vscode-shortcut-keys-macos",title:"(KOR) VsCode Shortcut Keys[MacOS]",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/VsCode-Shortcut-Keys/"}},{id:"post-eng-development-of-artificial-intelligence",title:"(ENG) Development of Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/history-of-ML/"}},{id:"post-kor-\ub9c8\ud06c\ub2e4\uc6b4-\ubb38\ubc95-\uc815\ub9ac",title:"(KOR) \ub9c8\ud06c\ub2e4\uc6b4 \ubb38\ubc95 \uc815\ub9ac",description:"\ub9c8\ud06c\ub2e4\uc6b4 \ubb38\ubc95",section:"Posts",handler:()=>{window.location.href="/blog/2024/markdown-syntax/"}},{id:"post-eng-backpropagation",title:"(ENG) Backpropagation",description:"backpropagation",section:"Posts",handler:()=>{window.location.href="/blog/2024/backpropagation/"}},{id:"post-eng-github-commit-message-conventions",title:"(ENG) Github Commit Message Conventions",description:"gitub commmit message convetions",section:"Posts",handler:()=>{window.location.href="/blog/2024/github-conventions-copy/"}},{id:"post-kor-\ub17c\ubb38-\ub9ac\ubdf0-\ubc29\ubc95\ub860",title:"(KOR) \ub17c\ubb38 \ub9ac\ubdf0 \ubc29\ubc95\ub860",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/journal-reading-methodology/"}},{id:"post-kor-\uc544\ub098\ucf58\ub2e4-venv-\uc124\uc815",title:"(KOR) \uc544\ub098\ucf58\ub2e4 venv \uc124\uc815",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda-venv/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-2025-snu-nlp-lab-winter-internship",title:"2025 SNU NLP Lab Winter Internship",description:"2025 \uc11c\uc6b8\ub300\ud559\uad50 \uc790\uc5f0\uc5b4\ucc98\ub9ac \uc5f0\uad6c\uc2e4 \ud65c\ub3d9 \uc815\ub9ac",section:"Projects",handler:()=>{window.location.href="/projects/project1/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>