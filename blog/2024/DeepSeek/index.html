<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> (ENG) DeepSeek-R1 Paper Review | JooHun Hyun </title> <meta name="author" content="JooHun Hyun"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joohunhyun.github.io/blog/2024/DeepSeek/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "(ENG) DeepSeek-R1 Paper Review",
            "description": "",
            "published": "December 29, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">JooHun</span> Hyun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>(ENG) DeepSeek-R1 Paper Review</h1> <p></p> </d-title> <d-article> <p><strong>Table of Contents</strong></p> <ul> <li><a href="#1-introduction">1. Introduction</a></li> <li> <a href="#2-approach">2. Approach</a> <ul> <li> <a href="#21-reinforcement-learning-algorithm-on-the-base-model-r-1-zero">2.1 Reinforcement Learning Algorithm on the Base Model (R-1 Zero)</a> <ul> <li><a href="#grpo">GRPO</a></li> <li><a href="#rule-based-reward-system">Rule Based Reward System</a></li> <li><a href="#training-template">Training Template</a></li> <li><a href="#performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</a></li> <li><a href="#aha-moment-of-deepseek-r1-zero">Aha moment of DeepSeek-R1-Zero</a></li> </ul> </li> <li> <a href="#21-reinforcement-learning-with-cold-start">2.1 Reinforcement Learning with Cold Start</a> <ul> <li><a href="#stage-1-cold-start">Stage 1: Cold Start</a></li> <li><a href="#stage-2-reasoning-oriented-reinforcement-learning">Stage 2: Reasoning-oriented Reinforcement Learning</a></li> <li><a href="#stage-3-rejection-sampling-and-supervised-fine-tuning">Stage 3: Rejection Sampling and Supervised Fine-Tuning</a></li> <li><a href="#stage-4-reinforcement-learning-for-all-scenarios">Stage 4: Reinforcement Learning for All Scenarios</a></li> </ul> </li> </ul> </li> <li><a href="#3-distillation">3. Distillation</a></li> <li> <a href="#4-experiment">4. Experiment</a> <ul> <li><a href="#evaluation-prompts--setup">Evaluation Prompts \&amp; Setup</a></li> <li><a href="#baselines">Baselines</a></li> <li><a href="#generation-constraints">Generation Constraints</a></li> </ul> </li> <li> <a href="#5-discussion-unsuccessful-attepts">5. Discussion (Unsuccessful Attepts)</a> <ul> <li><a href="#process-reward-model-prm">Process Reward Model (PRM)</a></li> <li><a href="#monte-carlo-tree-search-mcts">Monte Carlo Tree Search (MCTS)</a></li> </ul> </li> <li> <a href="#6-%EC%84%9C%EC%9A%B8%EB%8C%80%ED%95%99%EA%B5%90-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EB%9E%A9%EC%8B%A4-%EB%9E%A9%EB%AF%B8%ED%8C%85-%EB%82%B4%EC%9A%A9">6. ì„œìš¸ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ ë©ì‹¤ ë©ë¯¸íŒ… ë‚´ìš©</a> <ul> <li><a href="#%ED%8E%98%EC%9D%B4%ED%8D%BC-%EB%A6%AC%EB%B7%B0-%EC%9A%94%EC%95%BD">í˜ì´í¼ ë¦¬ë·° ìš”ì•½</a></li> <li><a href="#training-pipeline-of-deepseek-r1">Training Pipeline of DeepSeek-R1</a></li> <li><a href="#limitations">limitations</a></li> </ul> </li> </ul> <hr> <h2 id="1-introduction">1. Introduction</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Key Features : RL via GRPO algorithm, "aha" moment, Distillation, limitations of the MCTS and PRM in training R1
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/30-480.webp 480w,/assets/img/30-800.webp 800w,/assets/img/30-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>DeepSeek-R1-Zero and is the first generation reasoning model by DeepSeek AI trained via reinforcement learning without Supervised fine-tuning. Its primary limitations was that it had poor readability and language mixing<d-footnote>Language Mixing : the practice of using two or more languages in the same conversation or sentence.</d-footnote>. To address these issues, DeepSeek-R1 was introuduced in early 2025, that utilizes multi-stage training and cold-start data before RL.</p> <h2 id="2-approach">2. Approach</h2> <h3 id="21-reinforcement-learning-algorithm-on-the-base-model-r-1-zero">2.1 Reinforcement Learning Algorithm on the Base Model (R-1 Zero)</h3> <h4 id="grpo">GRPO</h4> <p>To reduce the training cost of reinforcement learning (RL), a critic model of equivalent size to the policy model was omitted. Instead, Grouped Reinforcement Policy Optimization (GRPO) was used, which estimates the baseline using group scores.</p> <p>GRPO samples a group of outputs (\({ğ‘œ_1, ğ‘œ_2, Â· Â· Â· , ğ‘œ_ğº }\)) from the old policy \((\pi \theta_{\text{old}}\))and then optimizes the policy model \(\pi \theta\) by maximizing the following objective:</p> \[\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim p(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O | q) \right]\] \[\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)} A_i, \operatorname{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) \right)\] \[\mathbb{D}_{KL} (\pi_{\theta} \| \pi_{ref}) = \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{ref}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1\] <p>where,</p> <ul> <li>\(\epsilon\) &amp; \(\beta\) are <strong>hyperparameters</strong> </li> <li>\(A_i\) is the <strong>advantage</strong> </li> </ul> <p>The advantage is computed using a group of rewards (\({ğ‘Ÿ_1, ğ‘Ÿ_2, . . . , ğ‘Ÿ_G }\)) corresponding to the outputs within each group.</p> \[A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})}\] <h4 id="rule-based-reward-system">Rule Based Reward System</h4> <p>The reward decides the optimization direction of the RL. A rule-based reward system that mainly consists of two types of rewards is adopted.</p> <ul> <li>Accuracy Rewards <ul> <li> <strong>TL;DR</strong> : Evaluates whether response is correct</li> <li>i.e. : For LeetCode problems, a compiler can be used to generate feedback</li> </ul> </li> <li>Format Rewards <ul> <li> <strong>TL;DR</strong> : Is the <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tag appropriatelly annotated</li> <li>Forward-thinking model that enforces the model to place its thinking process between <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags</li> </ul> </li> </ul> <h4 id="training-template">Training Template</h4> <p>This is the training template for DeepSeek-R1-Zero. The <code class="language-plaintext highlighter-rouge">prompt</code> portion is replaced with the question for the actual training.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt;
&lt;answer&gt; answer here &lt;/answer&gt;. User: prompt. Assistant:
</code></pre></div></div> <h4 id="performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</h4> <p>The following figure depicts the performance trajectory of the R-1 Zero model on the AIME 2024 Benchmark throught the RL training process.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/30-480.webp 480w,/assets/img/30-800.webp 800w,/assets/img/30-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The following is a brief explanation of each benchmark:</p> <ul> <li> <p><strong>AIME 2024</strong>: The American Invitational Mathematics Examination (AIME) is a 15-question, 3-hour test focusing on advanced high school mathematics, including algebra, geometry, and number theory. It serves as a qualifier for the United States Mathematical Olympiad. <a href="https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>MATH-500</strong>: This dataset comprises 500 problems from the MATH benchmark, covering various topics such as algebra, calculus, and probability. It is designed to evaluate mathematical reasoning and problem-solving abilities of AI models. <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>GPQA Diamond</strong>: The Graduate-Level Google-Proof Q&amp;A Benchmark (GPQA) is a challenging dataset of 448 multiple-choice questions in biology, physics, and chemistry, crafted by domain experts. The â€œDiamondâ€ subset represents the most difficult questions, assessing the advanced reasoning capabilities of AI models. <a href="https://arxiv.org/abs/2311.12022" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>LiveCodeBench</strong>: LiveCodeBench is a benchmark designed for evaluating large language modelsâ€™ coding capabilities. It continuously collects new problems from platforms like LeetCode, AtCoder, and CodeForces, and assesses various aspects such as code generation, execution, and self-repair. <a href="https://github.com/LiveCodeBench/LiveCodeBench" rel="external nofollow noopener" target="_blank">link</a></p> </li> <li> <p><strong>CodeForces</strong>: CodeForces is a competitive programming platform that hosts regular contests where participants solve algorithmic problems under time constraints. It serves as a benchmark for evaluating coding and problem-solving skills, with a rating system to rank participants. <a href="https://codeforces.com/blog/entry/22260" rel="external nofollow noopener" target="_blank">link</a></p> </li> </ul> <h4 id="aha-moment-of-deepseek-r1-zero">Aha moment of DeepSeek-R1-Zero</h4> <p>The significance of the â€œaha momentâ€ is that through RL, the model autonomously develops advanced problem-solving strategies, given the right incentives. The model learns to rethink using an anthropomorphic tone(aha!).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/32-480.webp 480w,/assets/img/32-800.webp 800w,/assets/img/32-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="21-reinforcement-learning-with-cold-start">2.1 Reinforcement Learning with Cold Start</h3> <p>Two questions arise by the results of DeepSeek-R1-Zero.</p> <ol> <li>Can reasoning performance be further improved or can convergence be accelerated by incorporating a small amount of high-quality data as a cold start?</li> <li>How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities</li> </ol> <p>The pipeline of DeepSeek-R1 attempts to address these questions. The pipeline consists of four stages, outlined as follows(stages 1~4).</p> <h4 id="stage-1-cold-start">Stage 1: Cold Start</h4> <p>To prevent instability in the early phases of RL training, DeepSeek-R1 incorporates a cold start phase by fine-tuning the base model with a small dataset of high-quality long CoT data. This dataset is constructed through multiple approaches, including:</p> <ul> <li> <strong>Few-shot prompting</strong> with long CoT examples.</li> <li> <strong>Direct prompting</strong> for detailed answers with reflection and verification.</li> <li> <strong>Extracting readable outputs</strong> from DeepSeek-R1-Zero.</li> <li> <strong>Human post-processing</strong> to refine results.</li> </ul> <p>By introducing cold-start data, DeepSeek-R1 achieves improved readability<d-footnote>Readability : How easy, accessible, or comprehensible that text is (ê°€ë…ì„±)</d-footnote> and performance over DeepSeek-R1-Zero. The structured format includes a reasoning process and a summary, ensuring clear and coherent outputs.</p> <h4 id="stage-2-reasoning-oriented-reinforcement-learning">Stage 2: Reasoning-oriented Reinforcement Learning</h4> <p>After fine-tuning DeepSeek-V3-Base on cold-start data, a large-scale reinforcement learning (RL) process similar to DeepSeek-R1-Zero was applied. This phase enhances the modelâ€™s reasoning abilities, particularly in coding, mathematics, science, and logical reasoningâ€”domains with well-defined problems and clear solutions.</p> <p>One challenge observed during training is language mixing in CoT responses, especially when RL prompts involve multiple languages. To address a language consistency reward was implemented, which measures the proportion of target-language words in the response. While ablation studies indicated a minor drop in performance, this alignment improved readability and human preference. The final reward combines reasoning accuracy and language consistency, and RL training continues until convergence.</p> <h4 id="stage-3-rejection-sampling-and-supervised-fine-tuning">Stage 3: Rejection Sampling and Supervised Fine-Tuning</h4> <p>Once reasoning-oriented RL converges, the checkpoint is used to generate Supervised Fine-Tuning (SFT) data for the next training phase. Unlike the cold-start data, which primarily targets reasoning, this stage incorporates diverse data types to improve DeepSeek-R1â€™s general-purpose capabilities.</p> <ul> <li> <p>Reasoning Data: Prompts are curated, and multiple reasoning trajectories are generated using rejection sampling. Unlike earlier phases that relied solely on rule-based rewards, this stage integrates generative reward models by comparing model outputs with ground-truth data using DeepSeek-V3. Additionally, unreadable outputs with mixed languages, excessive length, or unnecessary code blocks are filtered out. Ultimately, this process yields 600k high-quality reasoning samples.</p> </li> <li> <p>Non-Reasoning Data: To enhance tasks such as writing, factual QA, self-cognition, and translation, 200k additional training samples are incorporated from DeepSeek-V3â€™s SFT dataset. For complex queries, an intermediate CoT is generated before answering, while for simple queries (e.g., â€œhelloâ€), CoT is omitted.</p> </li> </ul> <p>Overall, DeepSeek-V3-Base is fine-tuned for two epochs using an 800k-sample dataset.</p> <h4 id="stage-4-reinforcement-learning-for-all-scenarios">Stage 4: Reinforcement Learning for All Scenarios</h4> <p>To further align the model with human preferences, a secondary RL stage is introduced, refining both reasoning capabilities and alignment with user expectations. This phase integrates reward signals and diverse prompt distributions:</p> <ul> <li>Reasoning Data: Rule-based rewards, as in DeepSeek-R1-Zero, are applied to reinforce skills in mathematics, coding, and logical reasoning.</li> <li> <p>General Data: A reward model evaluates helpfulness and harmlessness in open-ended tasks, following the DeepSeek-V3 preference pipeline.</p> </li> <li>Helpfulness: The reward model assesses only the final summary, ensuring it is useful and relevant while preserving reasoning integrity.</li> <li>Harmlessness: The entire response, including both the reasoning process and summary, is reviewed to mitigate potential risks, biases, or harmful content.</li> </ul> <p>By combining reinforcement learning, reward models, and diverse data sources, DeepSeek-R1 is trained to excel in reasoning while maintaining strong alignment with human preferences.</p> <h2 id="3-distillation">3. Distillation</h2> <p>To equip smaller models with reasoning capabilities similar to DeepSeek-R1, open-source models like Qwen and Llama was fine tuned using the 800k curated samples from stage 3 of the pipeline.</p> <p>The base models used include:</p> <ul> <li>Qwen2.5: Math-1.5B, Math-7B, 14B, 32B</li> <li>Llama: 3.1-8B, 3.3-70B-Instruct (chosen for its superior reasoning ability over Llama-3.1)</li> </ul> <p>Unlike DeepSeek-R1, distilled models undergo only <strong>SFT</strong><d-footnote>Supervised Fine Tuning : ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë‹µì•ˆì„ ì œì‹œí•˜ê³  ì´ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì§€ë„í•™ìŠµ ë°©ë²•ë¡ . í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ê³ , ê³„ì‚°ëŸ‰ì´ ìƒëŒ€ì ìœ¼ë¡œ ì ë‹¤. </d-footnote> and RL methodologies are not applied.</p> <h2 id="4-experiment">4. Experiment</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/35-480.webp 480w,/assets/img/35-800.webp 800w,/assets/img/35-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The â€œflagship modelâ€ DeepSeek-R1 was evaluated using a range of benchmarks covering math, coding, factual knowledge, and reasoning, including:</p> <ul> <li>Math &amp; Reasoning: AIME 2024, MATH-500, GPQA Diamond, CNMO 2024</li> <li>Coding: LiveCodeBench, Codeforces, SWE-Bench Verified, Aider 1</li> <li>General Knowledge &amp; QA: MMLU, MMLU-Redux, MMLU-Pro, C-Eval, CMMLU, SimpleQA, C-SimpleQA, IFEval, FRAMES</li> <li>LLM-based Open-Ended Generation: AlpacaEval 2.0, Arena-Hard</li> </ul> <p>For distilled R1 models, the results was reported on the AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.</p> <h3 id="evaluation-prompts--setup">Evaluation Prompts &amp; Setup</h3> <ul> <li>Benchmarks use standard prompt formats (e.g., simple-evals for MMLU, Zero-Eval for MMLU-Redux).</li> <li>Few-shot CoT prompts are modified to zero-shot, as CoT in few-shot settings may hurt DeepSeek-R1 performance.</li> <li>LiveCodeBench uses CoT-based evaluation on data collected between Aug 2024 â€“ Jan 2025.</li> <li>Codeforces evaluation includes 10 Div.2 contests with expert-crafted test cases.</li> </ul> <p>For long-output reasoning models, we use pass@k evaluation to reduce variability, with pass@1 calculated using temperature 0.6 and top-p 0.95. AIME 2024 additionally reports cons@64 (majority vote results).</p> <h3 id="baselines">Baselines</h3> <p>DeepSeek-R1 is compared against strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, OpenAI-o1-1217, and for distilled models, QwQ-32B-Preview.</p> <h3 id="generation-constraints">Generation Constraints</h3> <ul> <li>Maximum token length: 32,768</li> <li>Pass@k evaluation with 4 to 64 samples per query to ensure stable performance estimates.</li> </ul> <h2 id="5-discussion-unsuccessful-attepts">5. Discussion (Unsuccessful Attepts)</h2> <p>During the development of DeepSeek-R1, Process Reward Models (PRM) and Monte Carlo Tree Search (MCTS) were unsuccessful in training.</p> <h4 id="process-reward-model-prm">Process Reward Model (PRM)</h4> <p>PRM aims to guide the model toward better reasoning by rewarding correct intermediate steps. However, it has three major limitations:</p> <ol> <li>Defining fine-grained<d-footnote>Fine-Grained Approach : ì‘ì—…ì„ í° ë‹¨ìœ„ ì•ˆì—ì„œë„ ì„¸ë¶„í™”ëœ ì‘ì—… í”„ë¡œì„¸ìŠ¤ë¥¼ ë§Œë“¤ì–´ì„œ ì‹¤í–‰í•˜ëŠ” ë°©ì‹</d-footnote> reasoning steps is difficult.</li> <li>Determining the correctness of an intermediate step is challenging, as automated annotations are unreliable and manual annotation does not scale.</li> <li>PRM introduces reward hacking, requiring frequent retraining and additional resources.</li> </ol> <p>While PRM can be effective in reranking responses or guided searches, its computational overhead outweighs its benefits in large-scale reinforcement learning.</p> <h4 id="monte-carlo-tree-search-mcts">Monte Carlo Tree Search (MCTS)</h4> <p>Inspired by AlphaGo and AlphaZero, MCTS was tested as a method to enhance reasoning by systematically exploring solution paths. The process involved tagging reasoning steps, using a value model to guide searches, and iteratively refining the training process. However, two major challenges emerged:</p> <ol> <li>The search space for token generation is exponentially larger than structured games like chess, making it difficult to scale effectively.</li> <li>The value model plays a crucial role in search quality, but training a reliable fine-grained value model is inherently difficult.</li> </ol> <p>While MCTS can improve inference when used with a pre-trained value model, iteratively boosting model performance through self-search remains a challenge.</p> <h2 id="6-ì„œìš¸ëŒ€í•™êµ-ìì—°ì–´ì²˜ë¦¬-ë©ì‹¤-ë©ë¯¸íŒ…-ë‚´ìš©">6. ì„œìš¸ëŒ€í•™êµ ìì—°ì–´ì²˜ë¦¬ ë©ì‹¤ ë©ë¯¸íŒ… ë‚´ìš©</h2> <h3 id="í˜ì´í¼-ë¦¬ë·°-ìš”ì•½">í˜ì´í¼ ë¦¬ë·° ìš”ì•½</h3> <ul> <li>ë”¥ì‹œí¬ ëª¨ë¸ì€ ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¡œ íŠ¸ë ˆì´ë‹ì´ ë˜ì—ˆë‹¤.</li> <li>í•œêµ­ì–´ ì„±ëŠ¥ì€? <ul> <li>Inference Test : llama 8b, qwen2ì˜ ê²½ìš° í•œêµ­ì–´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ (í† í° ë‹¨ìˆœë°˜ë³µ, ë“±ì˜ ì´ìŠˆ ë°œìƒ)</li> <li>ì¤‘ì˜ì ì¸ì§€ ì•Œë ¤ì¤˜ ë“±, í•œêµ­ì–´ì˜ ìœ í˜•ì„ íŒë³„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì—ì„œë„ ì˜ëª»ëœ approachë¥¼ ë³´ì—¬ì¤Œ</li> <li>ë‹µë³€ì„ ì˜ì–´ë¡œ ì¶œë ¥í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤.</li> </ul> </li> <li>ì—°êµ¬ì‹¤ ì‘ì—…ì— ì‚¬ìš© ê°€ëŠ¥ì„± <ul> <li>ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ë©´, APIë¡œ ëª¨ë¸ outputì„ ìˆ˜ì§‘í•˜ì—¬ ë©ì‹¤ LLM ì„±ëŠ¥ ì¦ê°• (í˜„ì¬ OpenAIê°€ DeepSeekê°€ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí–ˆë‹¤ê³  í•˜ëŠ” ê²ƒ ì•„ë‹Œê°€â€¦?)</li> </ul> </li> </ul> <h3 id="training-pipeline-of-deepseek-r1">Training Pipeline of DeepSeek-R1</h3> <ol> <li>initial SFT w/ high quality examples â†’ collected from DeepSeek-R1 Zero</li> <li>RL focusing on reasoning tasks (coding, math, â€¦)</li> <li>collection of new training data through rejection sampling &amp; SFT correct and readable samples in more general domains</li> <li>final RL across all task types: Rule-based reward or LLM feedback</li> </ol> <p>SFT ì—†ì´ ê°•í™”í•™ìŠµë§Œ í•´ë„ ì¶”ë¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì´ë²ˆ í˜ì´í¼ì˜ key finding.</p> <h3 id="limitations">limitations</h3> <ul> <li>general capability</li> <li>SE tasks</li> <li>Language Mixing in multilingual context</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 JooHun Hyun. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-projects",title:"projects",description:"An archive of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-kor-github-al-folio-\uc124\uce58\ubd80\ud130-\ube14\ub85c\uadf8-\ud14c\ub9c8-\ubcc0\uacbd\uae4c\uc9c0",title:"(KOR) Github al-folio \uc124\uce58\ubd80\ud130 \ube14\ub85c\uadf8 \ud14c\ub9c8 \ubcc0\uacbd\uae4c\uc9c0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Al-Folio-%EC%84%A4%EC%B9%98-%EB%B0%A9%EB%B2%95,-%EC%82%AC%EC%9A%A9%EB%B2%95,-%EC%BB%A4%EC%8A%A4%ED%85%80-%EC%82%AC%EC%9A%A9/"}},{id:"post-eng-deepseek-r1-paper-review",title:"(ENG) DeepSeek-R1 Paper Review",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/DeepSeek/"}},{id:"post-eng-autogpt-installation",title:"(ENG) AutoGPT Installation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/AutoGPT/"}},{id:"post-kor-ai-agents-\uc2a4\ud130\ub514-\uc790\ub8cc-snu-cl-lab",title:"(KOR) AI Agents \uc2a4\ud130\ub514 \uc790\ub8cc[SNU CL LAB]",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/AI-Agents/"}},{id:"post-eng-cs224n-lecture-1",title:"(ENG) CS224n - Lecture 1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cs224n-1%EA%B0%95/"}},{id:"post-kor-kr-bert-a-small-scale-korean-specific-language-model",title:"(KOR) KR-BERT:A Small-Scale Korean-Specific Language Model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/KR-BERT-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"}},{id:"post-eng-feature-selection-extraction-and-ensamble-methods",title:"(ENG) Feature Selection, Extraction, and Ensamble Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/feature/"}},{id:"post-eng-training-and-evaluating-deep-networks",title:"(ENG) Training and Evaluating Deep Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"post-eng-rnn-and-lstm",title:"(ENG) RNN and LSTM",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/RNN/"}},{id:"post-eng-mlp-and-dnn",title:"(ENG) MLP and DNN",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/MLP-DNN/"}},{id:"post-kor-\uc5f0\uad6c\uc2e4-\uc548\uc804\uad50\uc721-\ubc30\uc18d\ud558\ub294-\ubc29\ubc95",title:"(KOR) \uc5f0\uad6c\uc2e4 \uc548\uc804\uad50\uc721 \ubc30\uc18d\ud558\ub294 \ubc29\ubc95",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/skip-safety-skip-copy/"}},{id:"post-eng-svm-svc",title:"(ENG) SVM, SVC",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/SVM-SVC/"}},{id:"post-kor-vscode-shortcut-keys-macos",title:"(KOR) VsCode Shortcut Keys[MacOS]",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/VsCode-Shortcut-Keys/"}},{id:"post-eng-development-of-artificial-intelligence",title:"(ENG) Development of Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/history-of-ML/"}},{id:"post-kor-\ub9c8\ud06c\ub2e4\uc6b4-\ubb38\ubc95-\uc815\ub9ac",title:"(KOR) \ub9c8\ud06c\ub2e4\uc6b4 \ubb38\ubc95 \uc815\ub9ac",description:"\ub9c8\ud06c\ub2e4\uc6b4 \ubb38\ubc95",section:"Posts",handler:()=>{window.location.href="/blog/2024/markdown-syntax/"}},{id:"post-eng-backpropagation",title:"(ENG) Backpropagation",description:"backpropagation",section:"Posts",handler:()=>{window.location.href="/blog/2024/backpropagation/"}},{id:"post-eng-github-commit-message-conventions",title:"(ENG) Github Commit Message Conventions",description:"gitub commmit message convetions",section:"Posts",handler:()=>{window.location.href="/blog/2024/github-conventions-copy/"}},{id:"post-kor-\ub17c\ubb38-\ub9ac\ubdf0-\ubc29\ubc95\ub860",title:"(KOR) \ub17c\ubb38 \ub9ac\ubdf0 \ubc29\ubc95\ub860",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/journal-reading-methodology/"}},{id:"post-kor-\uc544\ub098\ucf58\ub2e4-venv-\uc124\uc815",title:"(KOR) \uc544\ub098\ucf58\ub2e4 venv \uc124\uc815",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/conda-venv/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-2025-snu-nlp-lab-winter-internship",title:"2025 SNU NLP Lab Winter Internship",description:"2025 \uc11c\uc6b8\ub300\ud559\uad50 \uc790\uc5f0\uc5b4\ucc98\ub9ac \uc5f0\uad6c\uc2e4 \ud65c\ub3d9 \uc815\ub9ac",section:"Projects",handler:()=>{window.location.href="/projects/project1/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>